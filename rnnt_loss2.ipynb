{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "logits1 = tf.random.uniform((1, 4, 5, 3), dtype=tf.float32)\n",
    "labels1 = tf.concat([\n",
    "  tf.zeros((1, 1), dtype=tf.int32),\n",
    "  tf.random.uniform((1, 4), 1, 3, dtype=tf.int32)\n",
    "], axis=-1)\n",
    "\n",
    "logits2 = tf.random.uniform((1, 3, 4, 3), dtype=tf.float32)\n",
    "labels2 = tf.random.uniform((1, 4), 1, 3, dtype=tf.int32)\n",
    "\n",
    "logits2_padded = tf.pad(logits2, [[0, 0], [0, tf.shape(logits1)[1] - tf.shape(logits2)[1]], [0, tf.shape(logits1)[2] - tf.shape(logits2)[2]], [0, 0]], constant_values=0)\n",
    "labels2_padded = tf.pad(labels2, [[0, 0], [0, tf.shape(labels1)[1] - tf.shape(labels2)[1]]], constant_values=0)\n",
    "\n",
    "logits = tf.concat([logits1, logits2_padded], axis=0)\n",
    "labels = tf.concat([labels1, labels2_padded], axis=0)\n",
    "\n",
    "logits_rev = tf.reverse(logits, axis=[0])\n",
    "labels_rev = tf.reverse(labels, axis=[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnnt_loss (logits, labels, time_lengths, label_lengths):\n",
    "  log_pr = tf.math.log_softmax(logits, axis=-1)\n",
    "  pr = pr_loss(log_pr, labels, time_lengths, label_lengths)\n",
    "  ret = tf.reduce_sum(pr)\n",
    "  return ret\n",
    "\n",
    "@tf.custom_gradient\n",
    "def pr_loss (log_pr, labels, time_lengths, label_lengths):\n",
    "  LOG_0 = float('-inf')\n",
    "  batch_size = log_pr.shape[0]\n",
    "  max_time_lengths = log_pr.shape[1]\n",
    "  max_label_lengths = log_pr.shape[2]\n",
    "\n",
    "  def get_truth_log_pr (log_pr, labels):\n",
    "    labels_one_hot = tf.one_hot(labels, tf.shape(log_pr)[-1], axis=-1, dtype=tf.float32)\n",
    "    labels_one_hot = tf.expand_dims(labels_one_hot, axis=1)\n",
    "    labels_one_hot = tf.repeat(labels_one_hot, tf.shape(log_pr)[1], axis=1)\n",
    "    ret = tf.reduce_sum(log_pr * labels_one_hot, axis=-1)\n",
    "    return ret\n",
    "  \n",
    "  def get_blank_log_pr (log_pr):\n",
    "    return log_pr[:, :, :, 0]\n",
    "\n",
    "  truth_log_pr = get_truth_log_pr(log_pr, labels)\n",
    "  blank_log_pr = get_blank_log_pr(log_pr)\n",
    "\n",
    "  def get_alpha (log_pr, labels, time_lengths, label_lengths):\n",
    "    alpha = LOG_0 * np.ones((batch_size, max_time_lengths, max_label_lengths + 1), dtype=np.float32)\n",
    "    log_pr = tf.concat([\n",
    "      LOG_0 * tf.ones((batch_size, max_time_lengths, 1, tf.shape(log_pr)[-1]), tf.float32),\n",
    "      log_pr\n",
    "    ], axis=-2)\n",
    "    labels = tf.concat([\n",
    "      tf.zeros((batch_size, 1), tf.int32),\n",
    "      labels\n",
    "    ], axis=-1)\n",
    "    for b in range(batch_size):\n",
    "      alpha[b][0][1] = 0\n",
    "      for u in tf.range(2, label_lengths[b] + 1):\n",
    "        alpha[b][0][u] = alpha[b][0][u - 1] + log_pr[b][0][u - 1][labels[b][u]]\n",
    "\n",
    "      for t in tf.range(1, time_lengths[b]):\n",
    "        for u in tf.range(1, label_lengths[b] + 1):\n",
    "          alpha[b][t][u] = tf.reduce_logsumexp(\n",
    "            tf.stack([\n",
    "              alpha[b][t - 1][u] + log_pr[b][t - 1][u][0],\n",
    "              alpha[b][t][u - 1] + log_pr[b][t][u - 1][labels[b][u]],\n",
    "            ], axis=0),\n",
    "            axis=0\n",
    "          )\n",
    "\n",
    "    return alpha[:, :, 1: ]\n",
    "  \n",
    "  def get_beta (log_pr, labels, time_lengths, label_lengths):\n",
    "    beta = LOG_0 * np.ones((batch_size, max_time_lengths, max_label_lengths + 1), dtype=np.float32)\n",
    "    log_pr = tf.concat([log_pr, LOG_0 * tf.ones((batch_size, max_time_lengths, 1, tf.shape(log_pr)[-1]), tf.float32)], axis=-2)\n",
    "    labels = tf.concat([labels, tf.zeros((batch_size, 1), tf.int32)], axis=-1)\n",
    "\n",
    "    for b in range(batch_size):\n",
    "      beta[b][time_lengths[b] - 1][label_lengths[b] - 1] = log_pr[b][time_lengths[b] - 1][label_lengths[b] - 1][0]\n",
    "      for u in tf.reverse(tf.range(label_lengths[b] + 1 - 2), axis=[-1]):\n",
    "        beta[b][time_lengths[b] - 1][u] = beta[b][time_lengths[b] - 1][u + 1] + log_pr[b][time_lengths[b] - 1][u][labels[b][u + 1]]\n",
    "      for t in tf.reverse(tf.range(time_lengths[b] - 1), axis=[-1]):\n",
    "        for u in tf.reverse(tf.range(label_lengths[b] + 1 - 1), axis=[-1]):\n",
    "          beta[b][t][u] = tf.reduce_logsumexp(tf.stack([\n",
    "              beta[b][t + 1][u] + log_pr[b][t][u][0],\n",
    "              beta[b][t][u + 1] + log_pr[b][t][u][labels[b][u + 1]]\n",
    "            ]), axis=-1\n",
    "          )\n",
    "\n",
    "    return beta[:, :, : -1]\n",
    "\n",
    "  alpha = get_alpha(log_pr, labels, time_lengths, label_lengths)\n",
    "  beta = get_beta(log_pr, labels, time_lengths, label_lengths)\n",
    "\n",
    "  print('alpha, beta: ', alpha, beta)\n",
    "  \n",
    "  total_log_pr = beta[:, 0, 0]\n",
    "\n",
    "  vocab_size = log_pr.shape[-1]\n",
    "  def grad (upstream):\n",
    "    ret = np.zeros((batch_size, max_time_lengths, max_label_lengths, vocab_size), dtype=np.float32)\n",
    "    for b in range(batch_size):\n",
    "      for t in range(time_lengths[b]):\n",
    "        for u in range(label_lengths[b]):\n",
    "          if u + 1 < label_lengths[b]:\n",
    "            ret[b][t][u][labels[b][u + 1]] = -upstream[b] * tf.math.exp(alpha[b][t][u] + beta[b][t][u + 1] - total_log_pr[b])\n",
    "          if t + 1 < time_lengths[b]:\n",
    "            ret[b][t][u][0] = -upstream[b] * tf.math.exp(alpha[b][t][u] + beta[b][t + 1][u] - total_log_pr[b])\n",
    "          else:\n",
    "            if u < label_lengths[b] - 1:\n",
    "              # ret[b][t][u][0] = -upstream[b] * tf.math.exp(alpha[b][t][u] + LOG_0 - total_log_pr[b])\n",
    "              ret[b][t][u][0] = -upstream[b] * 0\n",
    "            else:\n",
    "              ret[b][t][u][0] = -upstream[b] * tf.math.exp(alpha[b][t][u] - total_log_pr[b])\n",
    "\n",
    "    return [tf.convert_to_tensor(ret) * tf.exp(log_pr)] + [None] * 3\n",
    "\n",
    "  return -total_log_pr, grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha, beta:  [[[ 0.         -0.92599946 -2.3156157  -3.055272   -4.2102556 ]\n",
      "  [-1.2887211  -1.6292272  -2.5907452  -3.0962415  -4.017257  ]\n",
      "  [-2.0283346  -2.1455922  -2.7752166  -3.330913   -4.218552  ]\n",
      "  [-2.9248867  -2.9254246  -3.0963216  -3.4934251  -4.2247667 ]]\n",
      "\n",
      " [[ 0.         -0.90289307 -1.5848131  -2.5483189         -inf]\n",
      "  [-1.4627242  -1.6832632  -2.0035896  -2.5004194         -inf]\n",
      "  [-2.5198498  -2.1539145  -2.2927456  -2.5829546         -inf]\n",
      "  [       -inf        -inf        -inf        -inf        -inf]]] [[[-5.392599  -4.8158193 -3.948039  -3.6297054 -3.8899534]\n",
      "  [-5.325465  -4.5423517 -3.7696152 -3.1361618 -2.853404 ]\n",
      "  [-5.462956  -4.405232  -3.5552168 -2.658824  -2.0091853]\n",
      "  [-5.9284177 -4.674135  -3.803445  -2.4680066 -1.1678319]]\n",
      "\n",
      " [[-3.8387084 -3.2207396 -2.971735  -3.2371302       -inf]\n",
      "  [-3.770599  -3.0109746 -2.4250593 -2.1882272       -inf]\n",
      "  [-3.7591357 -2.8791437 -2.1036036 -1.2557541       -inf]\n",
      "  [      -inf       -inf       -inf       -inf       -inf]]]\n",
      "tf.Tensor(9.231308, shape=(), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[[-0.01913884 -0.30910283  0.3282418 ]\n",
      "   [-0.06860353 -0.2427846   0.31138816]\n",
      "   [-0.02351383  0.09838533 -0.07487151]\n",
      "   [-0.08099362  0.0611681   0.01982553]\n",
      "   [-0.04304051  0.02878934  0.01425117]]\n",
      "\n",
      "  [[ 0.01807259 -0.09347808  0.07540552]\n",
      "   [-0.02855267 -0.12704903  0.1556017 ]\n",
      "   [-0.0301151   0.1351668  -0.10505171]\n",
      "   [-0.10263944  0.14238776 -0.03974833]\n",
      "   [-0.13002956  0.06003369  0.06999588]]\n",
      "\n",
      "  [[ 0.01861683 -0.05953721  0.04092037]\n",
      "   [ 0.02425868 -0.13479732  0.11053863]\n",
      "   [ 0.031416    0.13431704 -0.16573304]\n",
      "   [-0.05987129  0.15816449 -0.09829324]\n",
      "   [-0.24678552  0.11987937  0.12690614]]\n",
      "\n",
      "  [[ 0.01138065 -0.02244763  0.01106698]\n",
      "   [ 0.03280412 -0.06396724  0.03116312]\n",
      "   [ 0.0459091   0.11735387 -0.16326296]\n",
      "   [ 0.20161878  0.21029055 -0.41190934]\n",
      "   [-0.6889597   0.30324212  0.38571763]]]\n",
      "\n",
      "\n",
      " [[[-0.01632412  0.36300027 -0.3466762 ]\n",
      "   [-0.05000582  0.15752953 -0.10752374]\n",
      "   [-0.14531745  0.10193256  0.04338485]\n",
      "   [-0.09273411  0.0471329   0.0456012 ]\n",
      "   [ 0.          0.          0.        ]]\n",
      "\n",
      "  [[-0.00099314  0.08655982 -0.08556666]\n",
      "   [-0.02664919  0.11945097 -0.09280173]\n",
      "   [-0.07413082  0.1341679  -0.060037  ]\n",
      "   [-0.25920933  0.1878918   0.07131751]\n",
      "   [ 0.          0.          0.        ]]\n",
      "\n",
      "  [[ 0.02415309  0.02684051 -0.05099359]\n",
      "   [ 0.0759039   0.08752476 -0.16342868]\n",
      "   [ 0.18218987  0.14512162 -0.32731152]\n",
      "   [-0.7151388   0.34648094  0.36865783]\n",
      "   [ 0.          0.          0.        ]]\n",
      "\n",
      "  [[ 0.          0.          0.        ]\n",
      "   [ 0.          0.          0.        ]\n",
      "   [ 0.          0.          0.        ]\n",
      "   [ 0.          0.          0.        ]\n",
      "   [ 0.          0.          0.        ]]]], shape=(2, 4, 5, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "  tape.watch(logits)\n",
    "  loss = rnnt_loss(\n",
    "    logits,\n",
    "    labels,\n",
    "    tf.convert_to_tensor([\n",
    "      logits1.shape[1],\n",
    "      logits2.shape[1]\n",
    "    ], dtype=tf.int32),\n",
    "    tf.convert_to_tensor([\n",
    "      logits1.shape[2],\n",
    "      logits2.shape[2]\n",
    "    ], dtype=tf.int32)\n",
    "  )\n",
    "\n",
    "grads = tape.gradient(loss, logits)\n",
    "print(loss)\n",
    "print(grads)\n",
    "\n",
    "########################################\n",
    "\n",
    "# with tf.GradientTape() as tape:\n",
    "#   tape.watch(logits1)\n",
    "#   loss = rnnt_loss(logits1, labels1,\n",
    "#     tf.convert_to_tensor([\n",
    "#       logits1.shape[1]\n",
    "#     ], dtype=tf.int32),\n",
    "#     tf.convert_to_tensor([\n",
    "#       logits1.shape[2]\n",
    "#     ], dtype=tf.int32)\n",
    "#   )\n",
    "\n",
    "# grads = tape.gradient(loss, logits1)\n",
    "# print(loss)\n",
    "# print(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_to_zero (tensor):\n",
    "  return tf.where(tf.math.is_nan(tensor), tf.zeros_like(tensor), tensor)\n",
    "\n",
    "def rnnt_loss (logits, labels, time_lengths, label_lengths):\n",
    "  log_pr = tf.math.log_softmax(logits, axis=-1)\n",
    "  pr = pr_loss(log_pr, labels, time_lengths, label_lengths)\n",
    "  ret = tf.reduce_sum(pr)\n",
    "  return ret\n",
    "\n",
    "@tf.custom_gradient\n",
    "def pr_loss (log_pr, labels, time_lengths, label_lengths):\n",
    "  LOG_0 = float('-inf')\n",
    "  batch_size = tf.shape(log_pr)[0]\n",
    "  max_time_lengths = tf.shape(log_pr)[1]\n",
    "  max_label_lengths = tf.shape(log_pr)[2]\n",
    "\n",
    "  def get_truth_log_pr (log_pr, labels):\n",
    "    labels_one_hot = tf.one_hot(labels, tf.shape(log_pr)[-1], axis=-1, dtype=tf.float32)\n",
    "    labels_one_hot = labels_one_hot[:, 1: ]\n",
    "    labels_one_hot = tf.expand_dims(labels_one_hot, axis=1)\n",
    "    labels_one_hot = tf.repeat(labels_one_hot, tf.shape(log_pr)[1], axis=1)\n",
    "    ret = tf.reduce_sum(log_pr[:, :, : -1, :] * labels_one_hot, axis=-1)\n",
    "    ret = tf.concat([\n",
    "      ret,\n",
    "      LOG_0 * tf.ones((tf.shape(log_pr)[0], tf.shape(log_pr)[1], 1), dtype=tf.float32)\n",
    "    ], axis=-1)\n",
    "    return ret\n",
    "  \n",
    "  def get_blank_log_pr (log_pr):\n",
    "    return log_pr[:, :, :, 0]\n",
    "\n",
    "  truth_log_pr = get_truth_log_pr(log_pr, labels)\n",
    "  blank_log_pr = get_blank_log_pr(log_pr)\n",
    "\n",
    "  def get_alpha (truth_log_pr, blank_log_pr):\n",
    "    reversed_truth_log_pr = tf.reverse(truth_log_pr, axis=[-1])\n",
    "    padded_truth_log_pr = tf.pad(\n",
    "      reversed_truth_log_pr,\n",
    "      [[0, 0], [0, 0], [tf.shape(reversed_truth_log_pr)[-2] - 1, 0]],\n",
    "      constant_values=LOG_0\n",
    "    )\n",
    "    truth_diag = tf.linalg.diag_part(\n",
    "      padded_truth_log_pr,\n",
    "      k=(0, tf.shape(padded_truth_log_pr)[-1] - 1),\n",
    "      padding_value=LOG_0,\n",
    "      align='LEFT_RIGHT'\n",
    "    )\n",
    "    truth_diag = tf.transpose(truth_diag, perm=[1, 0, 2])\n",
    "\n",
    "    reversed_blank_log_pr = tf.reverse(blank_log_pr, axis=[-1])\n",
    "    padded_blank_log_pr = tf.pad(\n",
    "      reversed_blank_log_pr,\n",
    "      [[0, 0], [0, 0], [tf.shape(reversed_blank_log_pr)[-2] - 1, 0]],\n",
    "      constant_values=LOG_0\n",
    "    )\n",
    "    blank_diag = tf.linalg.diag_part(\n",
    "      padded_blank_log_pr,\n",
    "      k=(0, tf.shape(padded_blank_log_pr)[-1] - 1),\n",
    "      padding_value=LOG_0,\n",
    "      align='LEFT_RIGHT'\n",
    "    )\n",
    "    blank_diag = tf.concat([\n",
    "      LOG_0 * tf.ones((tf.shape(blank_diag)[0], tf.shape(blank_diag)[1], 1), dtype=tf.float32),\n",
    "      blank_diag[:, :, : -1]\n",
    "    ], axis=-1)\n",
    "    blank_diag = tf.transpose(blank_diag, perm=[1, 0, 2])\n",
    "\n",
    "    initial_diag = tf.concat([\n",
    "      tf.zeros((tf.shape(blank_diag)[1], 1), dtype=tf.float32),\n",
    "      LOG_0 * tf.ones((tf.shape(blank_diag)[1], tf.shape(blank_diag)[-1] - 1), dtype=tf.float32)\n",
    "    ], axis=-1)\n",
    "\n",
    "    def step (a, x):\n",
    "      t, b = x\n",
    "      return (\n",
    "        tf.reduce_logsumexp(\n",
    "          tf.stack([\n",
    "              a + t,\n",
    "              tf.concat([\n",
    "                LOG_0 * tf.ones((tf.shape(a)[0], 1), dtype=tf.float32),\n",
    "                a[:, : -1]\n",
    "              ], axis=-1) + b\n",
    "            ],\n",
    "            axis=0\n",
    "          ), axis=0\n",
    "        )\n",
    "      )\n",
    "    alpha_diag =  tf.concat([\n",
    "      tf.expand_dims(initial_diag, axis=0),\n",
    "      tf.scan(step, (truth_diag, blank_diag), initial_diag)\n",
    "    ], axis=0)\n",
    "    alpha_diag = tf.transpose(alpha_diag, perm=[1, 2, 0])\n",
    "    alpha = tf.linalg.diag_part(alpha_diag, k=(0, max_label_lengths - 1))\n",
    "    alpha = tf.reverse(alpha, axis=[-2])\n",
    "    alpha = tf.transpose(alpha, perm=[0, 2, 1])\n",
    "    return alpha\n",
    "\n",
    "  def get_beta (truth_log_pr, blank_log_pr):\n",
    "    reversed_truth_log_pr = tf.reverse(truth_log_pr, axis=[-1])\n",
    "    reversed_truth_log_pr = tf.concat([\n",
    "      LOG_0 * tf.ones((tf.shape(reversed_truth_log_pr)[0], tf.shape(reversed_truth_log_pr)[1], 1), dtype=tf.float32),\n",
    "      reversed_truth_log_pr[:, :, 1: ]\n",
    "    ], axis=-1)\n",
    "    padded_truth_log_pr = tf.pad(\n",
    "      reversed_truth_log_pr,\n",
    "      [[0, 0], [0, 0], [tf.shape(reversed_truth_log_pr)[-2] - 1, 0]],\n",
    "      constant_values=LOG_0\n",
    "    )\n",
    "    truth_diag = tf.linalg.diag_part(\n",
    "      padded_truth_log_pr,\n",
    "      k=(0, tf.shape(padded_truth_log_pr)[-1] - 1),\n",
    "      padding_value=LOG_0,\n",
    "      align='LEFT_RIGHT'\n",
    "    )\n",
    "    truth_diag = tf.transpose(truth_diag, perm=[1, 0, 2])\n",
    "    truth_diag = truth_diag[: -1]\n",
    "\n",
    "    reversed_blank_log_pr = tf.reverse(blank_log_pr, axis=[-1])\n",
    "    reversed_blank_log_pr = tf.concat([\n",
    "      reversed_blank_log_pr[:, : -1, :],\n",
    "      LOG_0 * tf.ones((tf.shape(reversed_blank_log_pr)[0], 1, tf.shape(reversed_blank_log_pr)[-1]), dtype=tf.float32)\n",
    "    ], axis=-2)\n",
    "    padded_blank_log_pr = tf.pad(\n",
    "      reversed_blank_log_pr,\n",
    "      [[0, 0], [0, 0], [tf.shape(reversed_blank_log_pr)[-2] - 1, 0]],\n",
    "      constant_values=LOG_0\n",
    "    )\n",
    "    blank_diag = tf.linalg.diag_part(\n",
    "      padded_blank_log_pr,\n",
    "      k=(0, tf.shape(padded_blank_log_pr)[-1] - 1),\n",
    "      padding_value=LOG_0,\n",
    "      align='LEFT_RIGHT'\n",
    "    )\n",
    "    blank_diag = tf.transpose(blank_diag, perm=[1, 0, 2])\n",
    "    blank_diag = blank_diag[: -1]\n",
    "\n",
    "    mask = tf.sequence_mask(\n",
    "      time_lengths + label_lengths - 2,\n",
    "      tf.shape(log_pr)[1] + tf.shape(log_pr)[2] - 2,\n",
    "      dtype=tf.float32\n",
    "    )\n",
    "    mask = tf.transpose(mask, perm=[1, 0])\n",
    "\n",
    "    dp_start_value = tf.gather_nd(\n",
    "      blank_log_pr,\n",
    "      indices=tf.stack([time_lengths, label_lengths], axis=-1) - 1,\n",
    "      batch_dims=1\n",
    "    )\n",
    "\n",
    "    initial_diag_mask = tf.one_hot(time_lengths - 1, depth=tf.shape(log_pr)[1])\n",
    "    initial_diag = tf.expand_dims(dp_start_value, axis=1) * initial_diag_mask + nan_to_zero(LOG_0 * (1.0 - initial_diag_mask))\n",
    "\n",
    "    def step (a, x):\n",
    "      m, t, b = x\n",
    "      a_next = tf.reduce_logsumexp(\n",
    "        tf.stack([\n",
    "          a + t,\n",
    "          tf.concat([\n",
    "            a[:, 1: ],\n",
    "            LOG_0 * tf.ones((tf.shape(a)[0], 1), dtype=tf.float32)\n",
    "          ], axis=-1) + b\n",
    "        ], axis=0),\n",
    "        axis=0\n",
    "      )\n",
    "      masked_a_next = nan_to_zero(a_next * tf.expand_dims(m, axis=1)) + nan_to_zero(a * tf.expand_dims(1.0 - m, axis=1))\n",
    "      return masked_a_next\n",
    "\n",
    "    beta_diag = tf.concat([\n",
    "      tf.scan(step, (mask, truth_diag, blank_diag), initial_diag, reverse=True),\n",
    "      tf.expand_dims(initial_diag, axis=0)\n",
    "    ], axis=0)\n",
    "\n",
    "    beta_diag = tf.transpose(beta_diag, perm=[1, 2, 0])\n",
    "    beta = tf.linalg.diag_part(beta_diag, k=(0, tf.shape(log_pr)[2] - 1), padding_value=LOG_0)\n",
    "    beta = tf.transpose(beta, perm=[0, 2, 1])\n",
    "    beta = tf.reverse(beta, axis=[-1])\n",
    "\n",
    "    return beta\n",
    "  \n",
    "  time_mask = tf.sequence_mask(time_lengths, tf.shape(log_pr)[1], dtype=tf.float32)\n",
    "  label_mask = tf.sequence_mask(label_lengths, tf.shape(log_pr)[2], dtype=tf.float32)\n",
    "  total_mask = tf.expand_dims(time_mask, axis=2) * tf.expand_dims(label_mask, axis=1)\n",
    "\n",
    "  alpha = get_alpha(truth_log_pr, blank_log_pr)\n",
    "  alpha = alpha + nan_to_zero((1.0 - total_mask) * LOG_0)\n",
    "  beta = get_beta(truth_log_pr, blank_log_pr)\n",
    "  beta = beta + nan_to_zero((1.0 - total_mask) * LOG_0)\n",
    "\n",
    "  indices = tf.concat([\n",
    "    tf.expand_dims(tf.range(0, tf.shape(log_pr)[0]), axis=1),\n",
    "    tf.stack([\n",
    "      time_lengths,\n",
    "      label_lengths - 1\n",
    "    ], axis=-1),\n",
    "  ], axis=-1)\n",
    "  \n",
    "  beta_mask = tf.scatter_nd(\n",
    "    indices,\n",
    "    tf.ones(tf.shape(indices)[0], tf.float32),\n",
    "    (tf.shape(log_pr)[0], tf.shape(log_pr)[1] + 1, tf.shape(log_pr)[2])\n",
    "  )\n",
    "  beta_mask = 1.0 - beta_mask\n",
    "  beta = nan_to_zero(tf.pad(beta, [[0, 0], [0, 1], [0, 0]], constant_values=LOG_0) * beta_mask)\n",
    "\n",
    "  total_mask = tf.expand_dims(total_mask, axis=-1)\n",
    "\n",
    "  total_log_pr = beta[:, 0, 0]\n",
    "\n",
    "  def grad (upstream):\n",
    "    blank_grads = \\\n",
    "      alpha + beta[:, 1: , :] \\\n",
    "      - tf.reshape(total_log_pr, shape=(tf.shape(total_log_pr)[0], 1, 1))\n",
    "    truth_grads = \\\n",
    "      alpha + tf.pad(\n",
    "        beta[:, : -1, 1: ],\n",
    "        [[0, 0], [0, 0], [0, 1]],\n",
    "        constant_values=LOG_0\n",
    "      ) \\\n",
    "      - tf.reshape(total_log_pr, shape=(tf.shape(total_log_pr)[0], 1, 1))\n",
    "    blank_one_hot = tf.one_hot(tf.zeros_like(labels, dtype=tf.int32), tf.shape(log_pr)[-1], dtype=tf.float32)\n",
    "    blank_one_hot = tf.expand_dims(blank_one_hot, axis=1)\n",
    "    blank_one_hot = tf.repeat(blank_one_hot, tf.shape(log_pr)[1], axis=1)\n",
    "    blank_grads = tf.exp(tf.expand_dims(blank_grads, axis=-1) + log_pr) * blank_one_hot\n",
    "    truth_one_hot = tf.one_hot(labels[:, 1: ], tf.shape(log_pr)[-1], dtype=tf.float32)\n",
    "    truth_one_hot = tf.concat([truth_one_hot, tf.zeros((tf.shape(log_pr)[0], 1, tf.shape(log_pr)[-1]), dtype=tf.float32)], axis=-2)\n",
    "    truth_one_hot = tf.expand_dims(truth_one_hot, axis=1)\n",
    "    truth_one_hot = tf.repeat(truth_one_hot, tf.shape(log_pr)[1], axis=1)\n",
    "    truth_grads = tf.exp(tf.expand_dims(truth_grads, axis=-1) + log_pr) * truth_one_hot\n",
    "\n",
    "    grads = blank_grads + truth_grads\n",
    "\n",
    "    return (\n",
    "      [\n",
    "        tf.reshape(-upstream, shape=(tf.shape(upstream)[0], 1, 1, 1))\n",
    "        * grads * total_mask\n",
    "      ] +\n",
    "      [None] * 3\n",
    "    )\n",
    "\n",
    "  return -total_log_pr, grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(9.231308, shape=(), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[[-0.01913884 -0.30910283  0.3282418 ]\n",
      "   [-0.06860353 -0.2427846   0.31138816]\n",
      "   [-0.02351385  0.09838533 -0.07487151]\n",
      "   [-0.08099362  0.06116809  0.01982552]\n",
      "   [-0.04304052  0.02878935  0.01425118]]\n",
      "\n",
      "  [[ 0.01807259 -0.09347808  0.07540552]\n",
      "   [-0.02855265 -0.12704903  0.15560171]\n",
      "   [-0.0301151   0.1351668  -0.10505171]\n",
      "   [-0.10263944  0.14238776 -0.03974833]\n",
      "   [-0.13002956  0.06003369  0.06999588]]\n",
      "\n",
      "  [[ 0.01861684 -0.05953721  0.04092037]\n",
      "   [ 0.02425869 -0.1347973   0.11053863]\n",
      "   [ 0.03141599  0.13431704 -0.16573304]\n",
      "   [-0.05987129  0.15816449 -0.09829323]\n",
      "   [-0.24678552  0.11987937  0.12690614]]\n",
      "\n",
      "  [[ 0.01138065 -0.02244763  0.01106698]\n",
      "   [ 0.03280412 -0.06396724  0.03116312]\n",
      "   [ 0.0459091   0.11735388 -0.16326298]\n",
      "   [ 0.20161878  0.21029055 -0.41190934]\n",
      "   [-0.6889597   0.30324212  0.38571763]]]\n",
      "\n",
      "\n",
      " [[[-0.01632415  0.36300027 -0.34667614]\n",
      "   [-0.05000587  0.1575295  -0.10752374]\n",
      "   [-0.14531748  0.10193256  0.04338486]\n",
      "   [-0.09273412  0.0471329   0.04560121]\n",
      "   [ 0.          0.          0.        ]]\n",
      "\n",
      "  [[-0.00099314  0.08655982 -0.08556665]\n",
      "   [-0.02664921  0.11945097 -0.09280172]\n",
      "   [-0.07413082  0.1341679  -0.060037  ]\n",
      "   [-0.25920933  0.18789181  0.07131752]\n",
      "   [ 0.          0.          0.        ]]\n",
      "\n",
      "  [[ 0.02415309  0.02684051 -0.0509936 ]\n",
      "   [ 0.0759039   0.08752476 -0.16342868]\n",
      "   [ 0.18218987  0.14512162 -0.32731152]\n",
      "   [-0.7151388   0.34648094  0.36865783]\n",
      "   [ 0.          0.          0.        ]]\n",
      "\n",
      "  [[ 0.          0.          0.        ]\n",
      "   [ 0.          0.          0.        ]\n",
      "   [ 0.          0.          0.        ]\n",
      "   [ 0.          0.          0.        ]\n",
      "   [ 0.          0.          0.        ]]]], shape=(2, 4, 5, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "  tape.watch(logits)\n",
    "  loss = rnnt_loss(\n",
    "    logits,\n",
    "    labels,\n",
    "    tf.convert_to_tensor([\n",
    "      logits1.shape[1],\n",
    "      logits2.shape[1]\n",
    "    ], dtype=tf.int32),\n",
    "    tf.convert_to_tensor([\n",
    "      logits1.shape[2],\n",
    "      logits2.shape[2]\n",
    "    ], dtype=tf.int32)\n",
    "  )\n",
    "\n",
    "grads = tape.gradient(loss, logits)\n",
    "print(loss)\n",
    "print(grads)\n",
    "\n",
    "##########################################\n",
    "\n",
    "# with tf.GradientTape() as tape:\n",
    "#   tape.watch(logits1)\n",
    "#   loss = rnnt_loss(\n",
    "#     logits1,\n",
    "#     labels1,\n",
    "#     tf.convert_to_tensor([\n",
    "#       logits1.shape[1],\n",
    "#     ], dtype=tf.int32),\n",
    "#     tf.convert_to_tensor([\n",
    "#       logits1.shape[2],\n",
    "#     ], dtype=tf.int32)\n",
    "#   )\n",
    "\n",
    "# grads = tape.gradient(loss, logits1)\n",
    "# print(loss)\n",
    "# print(grads)\n",
    "\n",
    "##########################################\n",
    "\n",
    "# with tf.GradientTape() as tape:\n",
    "#   tape.watch(logits2)\n",
    "#   loss = rnnt_loss(\n",
    "#     logits2,\n",
    "#     labels2,\n",
    "#     tf.convert_to_tensor([\n",
    "#       logits2.shape[1],\n",
    "#     ], dtype=tf.int32),\n",
    "#     tf.convert_to_tensor([\n",
    "#       logits2.shape[2],\n",
    "#     ], dtype=tf.int32)\n",
    "#   )\n",
    "\n",
    "# grads = tape.gradient(loss, logits2)\n",
    "# print(loss)\n",
    "# print(grads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[-0.01950178 -0.30899599  0.3284134 ]\n",
      "   [-0.06956212 -0.24224073  0.31165951]\n",
      "   [-0.02381245  0.0965459  -0.07500952]\n",
      "   [-0.08121312  0.06100342  0.01958293]\n",
      "   [-0.04330702  0.0284321   0.01583567]]\n",
      "\n",
      "  [[ 0.01805459 -0.09361105  0.07520283]\n",
      "   [-0.02879723 -0.12782161  0.15542826]\n",
      "   [-0.03050284  0.13506851 -0.10523877]\n",
      "   [-0.1027229   0.1424693  -0.03993363]\n",
      "   [-0.13012888  0.05957048  0.07005575]]\n",
      "\n",
      "  [[ 0.01833876 -0.05947057  0.04068129]\n",
      "   [ 0.02410301 -0.13517386  0.11060436]\n",
      "   [ 0.03130596  0.13415524 -0.1659625 ]\n",
      "   [-0.06003268  0.15764309 -0.11254264]\n",
      "   [-0.24703215  0.1189993   0.12697737]]\n",
      "\n",
      "  [[ 0.01088513 -0.02291232  0.0107045 ]\n",
      "   [ 0.03228918 -0.06428875  0.03048189]\n",
      "   [ 0.04443169  0.11729471 -0.16313082]\n",
      "   [ 0.20091861  0.20994577 -0.41478291]\n",
      "   [-0.68885601  0.30341071  0.38600522]]]\n",
      "\n",
      "\n",
      " [[[-0.01673768  0.36341387 -0.34660751]\n",
      "   [-0.04998233  0.15688762 -0.10757051]\n",
      "   [-0.14544274  0.10301723  0.04330653]\n",
      "   [-0.09286078  0.04694804  0.04555332]\n",
      "   [        nan         nan         nan]]\n",
      "\n",
      "  [[-0.0011581   0.08662809 -0.08567602]\n",
      "   [-0.02670603  0.12010252 -0.09227221]\n",
      "   [-0.07443059  0.13389859 -0.0601127 ]\n",
      "   [-0.25922212  0.18786886  0.07272761]\n",
      "   [        nan         nan         nan]]\n",
      "\n",
      "  [[ 0.0224406   0.02633832 -0.05114836]\n",
      "   [ 0.07369649  0.08761077 -0.16344416]\n",
      "   [ 0.182429    0.14538939 -0.32714859]\n",
      "   [-0.71473545  0.3468841   0.36885935]\n",
      "   [        nan         nan         nan]]\n",
      "\n",
      "  [[        nan         nan         nan]\n",
      "   [        nan         nan         nan]\n",
      "   [        nan         nan         nan]\n",
      "   [        nan         nan         nan]\n",
      "   [        nan         nan         nan]]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "rst = np.zeros_like(logits, dtype=np.float64)\n",
    "loss_before = rnnt_loss(\n",
    "  logits,\n",
    "  labels,\n",
    "  tf.convert_to_tensor([\n",
    "    logits1.shape[1],\n",
    "    logits2.shape[1]\n",
    "  ], dtype=tf.int32),\n",
    "  tf.convert_to_tensor([\n",
    "    logits1.shape[2],\n",
    "    logits2.shape[2]\n",
    "  ], dtype=tf.int32)\n",
    ")\n",
    "for i in range(logits.shape[0]):\n",
    "  for j in range(logits.shape[1]):\n",
    "    for k in range(logits.shape[2]):\n",
    "      for w in range(logits.shape[3]):\n",
    "        h = logits[i][j][k][w] * 0.005\n",
    "        a = np.zeros_like(logits, dtype=np.float64)\n",
    "        a[i][j][k][w] += h\n",
    "        loss_after = rnnt_loss(\n",
    "          logits + a,\n",
    "          labels,\n",
    "          tf.convert_to_tensor([\n",
    "            logits1.shape[1],\n",
    "            logits2.shape[1]\n",
    "          ], dtype=tf.int32),\n",
    "          tf.convert_to_tensor([\n",
    "            logits1.shape[2],\n",
    "            logits2.shape[2]\n",
    "          ], dtype=tf.int32)\n",
    "        )\n",
    "        rst[i][j][k][w] = (loss_after - loss_before) / h\n",
    "\n",
    "print(rst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec62f2ec7c7d49224ec335e3ef472b8ef537dc7949933ed74784e8710c2a92fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
