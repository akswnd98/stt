{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o/ 근데 칠십 퍼센트가 커 보이긴 하는데 이백 벌다 백 사십 벌면 빡셀걸? b/\n",
      "근데 삼 학년 때 까지는 국가장학금 바+ 받으면서 다녔던 건가?\n"
     ]
    }
   ],
   "source": [
    "test1 = \"o/ 근데 (70%)/(칠십 퍼센트)가 커 보이긴 하는데 (200)/(이백) 벌다 (140)/(백 사십) 벌면 빡셀걸? b/\"\n",
    "test2 = \"근데 (3학년)/(삼 학년) 때 까지는 국가장학금 바+ 받으면서 다녔던 건가?\"\n",
    "\n",
    "def bracket_filter(sentence):\n",
    "  new_sentence = str()\n",
    "  flag = False\n",
    "  \n",
    "  for ch in sentence:\n",
    "    if ch == '(' and flag == False:\n",
    "      flag = True\n",
    "      continue\n",
    "    if ch == '(' and flag == True:\n",
    "      flag = False\n",
    "      continue\n",
    "    if ch != ')' and flag == False:\n",
    "      new_sentence += ch\n",
    "  return new_sentence\n",
    "\n",
    "print(bracket_filter(test1))\n",
    "print(bracket_filter(test2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "근데 칠십 퍼센트가 커 보이긴 하는데 이백 벌다 백 사십 벌면 빡셀걸?\n",
      "c샾 배워봤어?\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "test1 = \"o/ 근데 칠십 퍼센트가 커 보이긴 하는데 이백 벌다 백 사십 벌면 빡셀걸? b/\"\n",
    "test2 = \"c# 배워봤어?\"\n",
    "\n",
    "def special_filter(sentence):\n",
    "  SENTENCE_MARK = ['?', '!']\n",
    "  NOISE = ['o', 'n', 'u', 'b', 'l']\n",
    "  EXCEPT = ['/', '+', '*', '-', '@', '$', '^', '&', '[', ']', '=', ':', ';', '.', ',']\n",
    "  \n",
    "  new_sentence = str()\n",
    "  for idx, ch in enumerate(sentence):\n",
    "    if ch not in SENTENCE_MARK:\n",
    "      # o/, n/ 등 처리\n",
    "      if idx + 1 < len(sentence) and ch in NOISE and sentence[idx+1] == '/': \n",
    "        continue \n",
    "\n",
    "    if ch == '#': \n",
    "      new_sentence += '샾'\n",
    "\n",
    "    elif ch not in EXCEPT: \n",
    "      new_sentence += ch\n",
    "\n",
    "  pattern = re.compile(r'\\s\\s+')\n",
    "  new_sentence = re.sub(pattern, ' ', new_sentence.strip())\n",
    "  return new_sentence\n",
    "\n",
    "print(special_filter(test1))\n",
    "print(special_filter(test2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아 나 쓰리 DS 갖고 싶다 쓰리 DS\n"
     ]
    }
   ],
   "source": [
    "# test = \"o/ 근데 (70%)/(칠십 퍼센트)가 커 보이긴 하는데 (200)/(이백) 벌다 (140)/(백 사십) 벌면 빡셀걸? b/\"\n",
    "test = 'b/ 아 나 (3DS)/(쓰리 DS) 갖고 싶다. (3DS)/(쓰리 DS)'\n",
    "\n",
    "def sentence_filter(raw_sentence):\n",
    "  return special_filter(bracket_filter(raw_sentence))\n",
    "\n",
    "print(sentence_filter(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosung = (\"ㄱ\", \"ㄲ\", \"ㄴ\", \"ㄷ\", \"ㄸ\", \"ㄹ\", \"ㅁ\", \"ㅂ\", \"ㅃ\", \"ㅅ\", \"ㅆ\", \"ㅇ\", \"ㅈ\", \"ㅉ\", \"ㅊ\", \"ㅋ\", \"ㅌ\", \"ㅍ\", \"ㅎ\")\n",
    "\n",
    "jungsung = (\"ㅏ\", \"ㅐ\", \"ㅑ\", \"ㅒ\", \"ㅓ\", \"ㅔ\", \"ㅕ\", \"ㅖ\", \"ㅗ\", \"ㅘ\", \"ㅙ\", \"ㅚ\", \"ㅛ\", \"ㅜ\", \"ㅝ\", \"ㅞ\", \"ㅟ\", \"ㅠ\", \"ㅡ\", \"ㅢ\", \"ㅣ\")\n",
    "\n",
    "jongsung = (\"\", \"ㄱ\", \"ㄲ\", \"ㄳ\", \"ㄴ\", \"ㄵ\", \"ㄶ\", \"ㄷ\", \"ㄹ\", \"ㄺ\", \"ㄻ\", \"ㄼ\", \"ㄽ\", \"ㄾ\", \"ㄿ\", \"ㅀ\", \"ㅁ\", \"ㅂ\", \"ㅄ\", \"ㅅ\", \"ㅆ\", \"ㅇ\", \"ㅈ\", \"ㅊ\", \"ㅋ\", \"ㅌ\", \"ㅍ\", \"ㅎ\")\n",
    "\n",
    "def isHangeul(one_character):\n",
    "    return 0xAC00 <= ord(one_character[:1]) <= 0xD7A3\n",
    "\n",
    "def hangeulExplode(one_hangeul):\n",
    "    a = one_hangeul[:1]\n",
    "    if isHangeul(a) != True:\n",
    "        return False\n",
    "    b = ord(a) - 0xAC00\n",
    "    cho = b // (21*28)\n",
    "    jung = b % (21*28) // 28\n",
    "    jong = b % 28\n",
    "    if jong == 0:\n",
    "        return (chosung[cho], jungsung[jung])\n",
    "    else:\n",
    "        return (chosung[cho], jungsung[jung], jongsung[jong])\n",
    "\n",
    "def hangeulJoin(inputlist):\n",
    "    result = \"\"\n",
    "    cho, jung, jong = 0, 0, 0\n",
    "    inputlist.insert(0, \"\")\n",
    "    while len(inputlist) > 1:\n",
    "        if inputlist[-1] in jongsung:\n",
    "            if inputlist[-2] in jungsung:\n",
    "                jong = jongsung.index(inputlist.pop())\n",
    "            \n",
    "            else:\n",
    "                result += inputlist.pop()\n",
    "        elif inputlist[-1] in jungsung:\n",
    "            if inputlist[-2] in chosung:\n",
    "                jung = jungsung.index(inputlist.pop())\n",
    "                cho = chosung.index(inputlist.pop())\n",
    "                result += chr(0xAC00 + ((cho*21)+jung)*28+jong)\n",
    "                cho, jung, jong = 0, 0, 0\n",
    "            else:\n",
    "                result += inputlist.pop()\n",
    "\n",
    "        else:\n",
    "            result += inputlist.pop()\n",
    "    else:\n",
    "        return result[::-1]\n",
    "\n",
    "def pureosseugi(inputtext):\n",
    "    result = \"\"\n",
    "    for i in inputtext:\n",
    "        if isHangeul(i) == True:\n",
    "            for j in hangeulExplode(i):\n",
    "                result += j\n",
    "        else:\n",
    "            result += i\n",
    "    \n",
    "    return result\n",
    "\n",
    "def moasseugi(inputtext):\n",
    "    t1 = []\n",
    "    for i in inputtext:\n",
    "        t1.append(i)\n",
    "\n",
    "    return hangeulJoin(t1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "\n",
    "class Phoneme:\n",
    "  def __init__ (self):\n",
    "    self.BLANK_INDEX = 0\n",
    "    self.START_INDEX = 1\n",
    "    self.END_INDEX = 2\n",
    "    self.SPACE_INDEX = 3\n",
    "    self.OOV_INDEX = 4\n",
    "\n",
    "class PhonemeGenerator (Phoneme):\n",
    "  def __init__ (self, labels):\n",
    "    super(PhonemeGenerator, self).__init__()\n",
    "    self.phonemes = ['<BLANK>', '<START>', '<END>', ' ']\n",
    "\n",
    "    texts = []\n",
    "    for label in labels:\n",
    "      for phoneme in label:\n",
    "        if not isHangeul(phoneme) and not phoneme == ' ' and not phoneme == '?':\n",
    "          break\n",
    "        for pure in pureosseugi(phoneme):\n",
    "          texts.append(pure)\n",
    "\n",
    "    tokenizer = Tokenizer(filters='', lower=False, oov_token='<OOV>')\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "\n",
    "    for i in range(1, len(tokenizer.index_word) + 1):\n",
    "      self.phonemes.append(tokenizer.index_word[i])\n",
    "\n",
    "    self.phoneme_index = {}\n",
    "    for i, phoneme in enumerate(self.phonemes):\n",
    "      self.phoneme_index[phoneme] = i\n",
    "    \n",
    "    self.VOCAB_SIZE = len(self.phonemes)\n",
    "  \n",
    "  def save (self, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "      pickle.dump((self.phonemes, self.phoneme_index), f)\n",
    "\n",
    "\n",
    "class PhonemeLoader (Phoneme):\n",
    "  def __init__ (self, filename):\n",
    "    super(PhonemeLoader, self).__init__()\n",
    "    with open(filename, 'rb') as f:\n",
    "      self.phonemes, self.phoneme_index = pickle.load(f)\n",
    "\n",
    "    self.VOCAB_SIZE = len(self.phonemes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import copy\n",
    "\n",
    "class Dataset:\n",
    "  def get_data (self, idx):\n",
    "    return (\n",
    "      self.extract_spectrogram(self.file_phoneme_vector_pairs[idx][0]),\n",
    "      [0] + self.file_phoneme_vector_pairs[idx][1]\n",
    "    )\n",
    "  \n",
    "  def get_len (self):\n",
    "    return len(self.file_phoneme_vector_pairs)\n",
    "\n",
    "  def extract_spectrogram (self, filename):\n",
    "    waveform = np.memmap(filename, dtype='h', mode='r')\n",
    "\n",
    "    sr = 16000\n",
    "    st = 1 / sr\n",
    "\n",
    "    t = np.linspace(0, waveform.shape[0] * st, waveform.shape[0])\n",
    "\n",
    "    S = librosa.feature.melspectrogram(y=np.float32(waveform), n_fft=512, hop_length=256, window='hamming', sr=sr, n_mels=128)\n",
    "    log_S = librosa.power_to_db(S, ref=np.max)\n",
    "    log_S = np.transpose(log_S)\n",
    "\n",
    "    return log_S\n",
    "\n",
    "class ShufflableDataset (Dataset):\n",
    "  def __init__ (self, phoneme, file_phoneme_vector_pairs, original_file_phoneme_vector_pairs):\n",
    "    super(ShufflableDataset, self).__init__()\n",
    "    self.phoneme = phoneme\n",
    "    self.original_file_phoneme_vector_pairs = original_file_phoneme_vector_pairs\n",
    "    self.file_phoneme_vector_pairs = file_phoneme_vector_pairs\n",
    "  \n",
    "  def shuffle (self):\n",
    "    self.file_phoneme_vector_pairs = copy.deepcopy(self.original_file_phoneme_vector_pairs)\n",
    "    random.shuffle(self.file_phoneme_vector_pairs)\n",
    "  \n",
    "  def save (self, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "      pickle.dump((self.phoneme, self.file_phoneme_vector_pairs, self.original_file_phoneme_vector_pairs), f)\n",
    "\n",
    "class ShufflableDatasetGenerator:\n",
    "  @staticmethod\n",
    "  def generate ():\n",
    "    with open('./ko-audio-dataset/sentence-script/KsponSpeech_scripts/train.trn', 'r', encoding='utf-8') as file:\n",
    "      lines = file.readlines()\n",
    "    def process_line (line):\n",
    "      line = line.split(' :: ')\n",
    "      line[0] = os.path.join('./ko-audio-dataset/audio/', line[0])\n",
    "      line[1] = sentence_filter(line[1])\n",
    "      return line\n",
    "    file_label_pairs = list(map(process_line, lines))\n",
    "    file_label_pairs = list(filter(lambda pair: not ShufflableDatasetGenerator.check_skip_label(pair[1]), file_label_pairs))\n",
    "    phoneme_generator = PhonemeGenerator([label for file, label in file_label_pairs])\n",
    "    file_phoneme_pairs = list(map(lambda pair: (pair[0], pureosseugi(pair[1])), file_label_pairs))\n",
    "    original_file_phoneme_vector_pairs = [[pair[0], [phoneme_generator.phoneme_index[phoneme] for phoneme in pair[1]]] for pair in file_phoneme_pairs]\n",
    "    file_phoneme_vector_pairs = copy.deepcopy(original_file_phoneme_vector_pairs)\n",
    "    return ShufflableDataset(phoneme_generator, file_phoneme_vector_pairs, original_file_phoneme_vector_pairs)\n",
    "\n",
    "  @staticmethod\n",
    "  def check_skip_hangul (hangul):\n",
    "    return not isHangeul(hangul) and not hangul == ' ' and not hangul == '?'\n",
    "\n",
    "  @staticmethod\n",
    "  def check_skip_label (label):\n",
    "    for hangul in label:\n",
    "      if ShufflableDatasetGenerator.check_skip_hangul(hangul):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "class ShufflableDatasetLoader:\n",
    "  @staticmethod\n",
    "  def load (filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "      phoneme, file_phoneme_vector_pairs, original_file_phoneme_vector_pairs = pickle.load(f)\n",
    "    return ShufflableDataset(phoneme, file_phoneme_vector_pairs, original_file_phoneme_vector_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetSizeLimitingIterator:\n",
    "  def __init__ (self, dataset, time_limit, label_limit, idx):\n",
    "    self.dataset = dataset\n",
    "    self.time_limit = time_limit\n",
    "    self.label_limit = label_limit\n",
    "    self.idx = idx\n",
    "    while self.idx < self.dataset.get_len():\n",
    "      data = self.dataset.get_data(self.idx)\n",
    "      if data[0].shape[0] <= self.time_limit and len(data[1]) <= self.label_limit:\n",
    "        break\n",
    "      self.idx += 1\n",
    "\n",
    "  def next (self):\n",
    "    idx = self.idx + 1\n",
    "    while idx < self.dataset.get_len():\n",
    "      data = self.dataset.get_data(idx)\n",
    "      if data[0].shape[0] <= self.time_limit and len(data[1]) <= self.label_limit:\n",
    "        break\n",
    "      idx += 1\n",
    "    return DatasetSizeLimitingIterator(self.dataset, self.time_limit, self.label_limit, idx)\n",
    "  \n",
    "  def check_end (self):\n",
    "    return self.idx >= self.dataset.get_len()\n",
    "\n",
    "  def get_value (self):\n",
    "    return self.dataset.get_data(self.idx)\n",
    "\n",
    "  def save (self, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "      pickle.dump((self.time_limit, self.label_limit,self.idx), f)\n",
    "\n",
    "class IteratorLoader:\n",
    "  @staticmethod\n",
    "  def load (dataset, filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "      time_limit, label_limit, idx = pickle.load(f)\n",
    "    return DatasetSizeLimitingIterator(dataset, time_limit, label_limit, idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class DatasetStepExtractorLoader:\n",
    "  @staticmethod\n",
    "  def load (path):\n",
    "    dataset = ShufflableDatasetLoader.load(os.path.join(path, 'dataset.pickle'))\n",
    "    return SavableDatasetStepExtractor(\n",
    "      dataset,\n",
    "      pickle.load(open(os.path.join(path, 'step_size.pickle'), 'rb')),\n",
    "      IteratorLoader.load(dataset, os.path.join(path, 'iterator.pickle'))\n",
    "    )\n",
    "\n",
    "class DatasetStepExtractor:\n",
    "  def __init__ (self, dataset, step_size, iterator):\n",
    "    self.dataset = dataset\n",
    "    self.step_size = step_size\n",
    "    self.value = []\n",
    "    self.iterator = iterator\n",
    "    while not iterator.check_end() and len(self.value) < self.step_size:\n",
    "      self.value.append(iterator.get_value())\n",
    "      iterator = iterator.next()\n",
    "    self.is_end = len(self.value) < self.step_size\n",
    "    self.next_iterator = iterator\n",
    "  \n",
    "  def check_end (self):\n",
    "    return self.is_end\n",
    "  \n",
    "  def get_value (self):\n",
    "    return self.value\n",
    "  \n",
    "  def get_next_extractor (self):\n",
    "    if self.is_end:\n",
    "      return None\n",
    "    else:\n",
    "      return SavableDatasetStepExtractor(self.dataset, self.step_size, self.next_iterator)\n",
    "\n",
    "class SavableDatasetStepExtractor (DatasetStepExtractor):\n",
    "  def save (self, saver):\n",
    "    saver.save(self.dataset, self.step_size, self.iterator)\n",
    "\n",
    "class DatasetStepExtractorSaver:\n",
    "  def __init__ (self, path):\n",
    "    self.path = path\n",
    "  \n",
    "  def save (self, dataset, step_size, iterator):\n",
    "    os.makedirs(self.path, exist_ok=True)\n",
    "\n",
    "class IteratorSaver (DatasetStepExtractorSaver):\n",
    "  def save (self, dataset, step_size, iterator):\n",
    "    super().save(dataset, step_size, iterator)\n",
    "    iterator.save(os.path.join(self.path, 'iterator.pickle'))\n",
    "  \n",
    "class DatasetSaver (DatasetStepExtractorSaver):\n",
    "  def save (self, dataset, step_size, iterator):\n",
    "    super().save(dataset, step_size, iterator)\n",
    "    dataset.save(os.path.join(self.path, 'dataset.pickle'))\n",
    "\n",
    "class StepSizeSaver (DatasetStepExtractorSaver):\n",
    "  def save (self, dataset, step_size, iterator):\n",
    "    super().save(dataset, step_size, iterator)\n",
    "    with open(os.path.join(self.path, 'step_size.pickle'), 'wb') as f:\n",
    "      pickle.dump(step_size, f)\n",
    "\n",
    "class ExtractorSaver (DatasetStepExtractorSaver):\n",
    "  def save (self, dataset, step_size, iterator):\n",
    "    super().save(dataset, step_size, iterator)\n",
    "    iterator.save(os.path.join(self.path, 'iterator.pickle'))\n",
    "    dataset.save(os.path.join(self.path, 'dataset.pickle'))\n",
    "    with open(os.path.join(self.path, 'step_size.pickle'), 'wb') as f:\n",
    "      pickle.dump(step_size, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ 여기 까지 dataset 부분 ###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_to_zero (tensor):\n",
    "  return tf.where(tf.math.is_nan(tensor), tf.zeros_like(tensor), tensor)\n",
    "\n",
    "def rnnt_loss (logits, labels, time_lengths, label_lengths):\n",
    "  log_pr = tf.math.log_softmax(logits, axis=-1)\n",
    "  pr = pr_loss(log_pr, labels, time_lengths, label_lengths)\n",
    "  ret = tf.reduce_sum(pr)\n",
    "  return ret\n",
    "\n",
    "@tf.custom_gradient\n",
    "def pr_loss (log_pr, labels, time_lengths, label_lengths):\n",
    "  LOG_0 = float('-inf')\n",
    "  batch_size = tf.shape(log_pr)[0]\n",
    "  max_time_lengths = tf.shape(log_pr)[1]\n",
    "  max_label_lengths = tf.shape(log_pr)[2]\n",
    "\n",
    "  def get_truth_log_pr (log_pr, labels):\n",
    "    labels_one_hot = tf.one_hot(labels, tf.shape(log_pr)[-1], axis=-1, dtype=tf.float32)\n",
    "    labels_one_hot = labels_one_hot[:, 1: ]\n",
    "    labels_one_hot = tf.expand_dims(labels_one_hot, axis=1)\n",
    "    labels_one_hot = tf.repeat(labels_one_hot, tf.shape(log_pr)[1], axis=1)\n",
    "    ret = tf.reduce_sum(log_pr[:, :, : -1, :] * labels_one_hot, axis=-1)\n",
    "    ret = tf.concat([\n",
    "      ret,\n",
    "      LOG_0 * tf.ones((tf.shape(log_pr)[0], tf.shape(log_pr)[1], 1), dtype=tf.float32)\n",
    "    ], axis=-1)\n",
    "    return ret\n",
    "  \n",
    "  def get_blank_log_pr (log_pr):\n",
    "    return log_pr[:, :, :, 0]\n",
    "\n",
    "  truth_log_pr = get_truth_log_pr(log_pr, labels)\n",
    "  blank_log_pr = get_blank_log_pr(log_pr)\n",
    "\n",
    "  def get_alpha (truth_log_pr, blank_log_pr):\n",
    "    reversed_truth_log_pr = tf.reverse(truth_log_pr, axis=[-1])\n",
    "    padded_truth_log_pr = tf.pad(\n",
    "      reversed_truth_log_pr,\n",
    "      [[0, 0], [0, 0], [tf.shape(reversed_truth_log_pr)[-2] - 1, 0]],\n",
    "      constant_values=LOG_0\n",
    "    )\n",
    "    truth_diag = tf.linalg.diag_part(\n",
    "      padded_truth_log_pr,\n",
    "      k=(0, tf.shape(padded_truth_log_pr)[-1] - 1),\n",
    "      padding_value=LOG_0,\n",
    "      align='LEFT_RIGHT'\n",
    "    )\n",
    "    truth_diag = tf.transpose(truth_diag, perm=[1, 0, 2])\n",
    "\n",
    "    reversed_blank_log_pr = tf.reverse(blank_log_pr, axis=[-1])\n",
    "    padded_blank_log_pr = tf.pad(\n",
    "      reversed_blank_log_pr,\n",
    "      [[0, 0], [0, 0], [tf.shape(reversed_blank_log_pr)[-2] - 1, 0]],\n",
    "      constant_values=LOG_0\n",
    "    )\n",
    "    blank_diag = tf.linalg.diag_part(\n",
    "      padded_blank_log_pr,\n",
    "      k=(0, tf.shape(padded_blank_log_pr)[-1] - 1),\n",
    "      padding_value=LOG_0,\n",
    "      align='LEFT_RIGHT'\n",
    "    )\n",
    "    blank_diag = tf.concat([\n",
    "      LOG_0 * tf.ones((tf.shape(blank_diag)[0], tf.shape(blank_diag)[1], 1), dtype=tf.float32),\n",
    "      blank_diag[:, :, : -1]\n",
    "    ], axis=-1)\n",
    "    blank_diag = tf.transpose(blank_diag, perm=[1, 0, 2])\n",
    "\n",
    "    initial_diag = tf.concat([\n",
    "      tf.zeros((tf.shape(blank_diag)[1], 1), dtype=tf.float32),\n",
    "      LOG_0 * tf.ones((tf.shape(blank_diag)[1], tf.shape(blank_diag)[-1] - 1), dtype=tf.float32)\n",
    "    ], axis=-1)\n",
    "\n",
    "    def step (a, x):\n",
    "      t, b = x\n",
    "      return (\n",
    "        tf.reduce_logsumexp(\n",
    "          tf.stack([\n",
    "              a + t,\n",
    "              tf.concat([\n",
    "                LOG_0 * tf.ones((tf.shape(a)[0], 1), dtype=tf.float32),\n",
    "                a[:, : -1]\n",
    "              ], axis=-1) + b\n",
    "            ],\n",
    "            axis=0\n",
    "          ), axis=0\n",
    "        )\n",
    "      )\n",
    "    alpha_diag =  tf.concat([\n",
    "      tf.expand_dims(initial_diag, axis=0),\n",
    "      tf.scan(step, (truth_diag, blank_diag), initial_diag)\n",
    "    ], axis=0)\n",
    "    alpha_diag = tf.transpose(alpha_diag, perm=[1, 2, 0])\n",
    "    alpha = tf.linalg.diag_part(alpha_diag, k=(0, max_label_lengths - 1))\n",
    "    alpha = tf.reverse(alpha, axis=[-2])\n",
    "    alpha = tf.transpose(alpha, perm=[0, 2, 1])\n",
    "    return alpha\n",
    "\n",
    "  def get_beta (truth_log_pr, blank_log_pr):\n",
    "    reversed_truth_log_pr = tf.reverse(truth_log_pr, axis=[-1])\n",
    "    reversed_truth_log_pr = tf.concat([\n",
    "      LOG_0 * tf.ones((tf.shape(reversed_truth_log_pr)[0], tf.shape(reversed_truth_log_pr)[1], 1), dtype=tf.float32),\n",
    "      reversed_truth_log_pr[:, :, 1: ]\n",
    "    ], axis=-1)\n",
    "    padded_truth_log_pr = tf.pad(\n",
    "      reversed_truth_log_pr,\n",
    "      [[0, 0], [0, 0], [tf.shape(reversed_truth_log_pr)[-2] - 1, 0]],\n",
    "      constant_values=LOG_0\n",
    "    )\n",
    "    truth_diag = tf.linalg.diag_part(\n",
    "      padded_truth_log_pr,\n",
    "      k=(0, tf.shape(padded_truth_log_pr)[-1] - 1),\n",
    "      padding_value=LOG_0,\n",
    "      align='LEFT_RIGHT'\n",
    "    )\n",
    "    truth_diag = tf.transpose(truth_diag, perm=[1, 0, 2])\n",
    "    truth_diag = truth_diag[: -1]\n",
    "\n",
    "    reversed_blank_log_pr = tf.reverse(blank_log_pr, axis=[-1])\n",
    "    reversed_blank_log_pr = tf.concat([\n",
    "      reversed_blank_log_pr[:, : -1, :],\n",
    "      LOG_0 * tf.ones((tf.shape(reversed_blank_log_pr)[0], 1, tf.shape(reversed_blank_log_pr)[-1]), dtype=tf.float32)\n",
    "    ], axis=-2)\n",
    "    padded_blank_log_pr = tf.pad(\n",
    "      reversed_blank_log_pr,\n",
    "      [[0, 0], [0, 0], [tf.shape(reversed_blank_log_pr)[-2] - 1, 0]],\n",
    "      constant_values=LOG_0\n",
    "    )\n",
    "    blank_diag = tf.linalg.diag_part(\n",
    "      padded_blank_log_pr,\n",
    "      k=(0, tf.shape(padded_blank_log_pr)[-1] - 1),\n",
    "      padding_value=LOG_0,\n",
    "      align='LEFT_RIGHT'\n",
    "    )\n",
    "    blank_diag = tf.transpose(blank_diag, perm=[1, 0, 2])\n",
    "    blank_diag = blank_diag[: -1]\n",
    "\n",
    "    mask = tf.sequence_mask(\n",
    "      time_lengths + label_lengths - 2,\n",
    "      tf.shape(log_pr)[1] + tf.shape(log_pr)[2] - 2,\n",
    "      dtype=tf.float32\n",
    "    )\n",
    "    mask = tf.transpose(mask, perm=[1, 0])\n",
    "\n",
    "    dp_start_value = tf.gather_nd(\n",
    "      blank_log_pr,\n",
    "      indices=tf.stack([time_lengths, label_lengths], axis=-1) - 1,\n",
    "      batch_dims=1\n",
    "    )\n",
    "\n",
    "    initial_diag_mask = tf.one_hot(time_lengths - 1, depth=tf.shape(log_pr)[1])\n",
    "    initial_diag = tf.expand_dims(dp_start_value, axis=1) * initial_diag_mask + nan_to_zero(LOG_0 * (1.0 - initial_diag_mask))\n",
    "\n",
    "    def step (a, x):\n",
    "      m, t, b = x\n",
    "      a_next = tf.reduce_logsumexp(\n",
    "        tf.stack([\n",
    "          a + t,\n",
    "          tf.concat([\n",
    "            a[:, 1: ],\n",
    "            LOG_0 * tf.ones((tf.shape(a)[0], 1), dtype=tf.float32)\n",
    "          ], axis=-1) + b\n",
    "        ], axis=0),\n",
    "        axis=0\n",
    "      )\n",
    "      masked_a_next = nan_to_zero(a_next * tf.expand_dims(m, axis=1)) + nan_to_zero(a * tf.expand_dims(1.0 - m, axis=1))\n",
    "      return masked_a_next\n",
    "\n",
    "    beta_diag = tf.concat([\n",
    "      tf.scan(step, (mask, truth_diag, blank_diag), initial_diag, reverse=True),\n",
    "      tf.expand_dims(initial_diag, axis=0)\n",
    "    ], axis=0)\n",
    "\n",
    "    beta_diag = tf.transpose(beta_diag, perm=[1, 2, 0])\n",
    "    beta = tf.linalg.diag_part(beta_diag, k=(0, tf.shape(log_pr)[2] - 1), padding_value=LOG_0)\n",
    "    beta = tf.transpose(beta, perm=[0, 2, 1])\n",
    "    beta = tf.reverse(beta, axis=[-1])\n",
    "\n",
    "    return beta\n",
    "  \n",
    "  time_mask = tf.sequence_mask(time_lengths, tf.shape(log_pr)[1], dtype=tf.float32)\n",
    "  label_mask = tf.sequence_mask(label_lengths, tf.shape(log_pr)[2], dtype=tf.float32)\n",
    "  total_mask = tf.expand_dims(time_mask, axis=2) * tf.expand_dims(label_mask, axis=1)\n",
    "\n",
    "  alpha = get_alpha(truth_log_pr, blank_log_pr)\n",
    "  alpha = alpha + nan_to_zero((1.0 - total_mask) * LOG_0)\n",
    "  beta = get_beta(truth_log_pr, blank_log_pr)\n",
    "  beta = beta + nan_to_zero((1.0 - total_mask) * LOG_0)\n",
    "\n",
    "  indices = tf.concat([\n",
    "    tf.expand_dims(tf.range(0, tf.shape(log_pr)[0]), axis=1),\n",
    "    tf.stack([\n",
    "      time_lengths,\n",
    "      label_lengths - 1\n",
    "    ], axis=-1),\n",
    "  ], axis=-1)\n",
    "  \n",
    "  beta_mask = tf.scatter_nd(\n",
    "    indices,\n",
    "    tf.ones(tf.shape(indices)[0], tf.float32),\n",
    "    (tf.shape(log_pr)[0], tf.shape(log_pr)[1] + 1, tf.shape(log_pr)[2])\n",
    "  )\n",
    "  beta_mask = 1.0 - beta_mask\n",
    "  beta = nan_to_zero(tf.pad(beta, [[0, 0], [0, 1], [0, 0]], constant_values=LOG_0) * beta_mask)\n",
    "\n",
    "  total_mask = tf.expand_dims(total_mask, axis=-1)\n",
    "\n",
    "  total_log_pr = beta[:, 0, 0]\n",
    "\n",
    "  def grad (upstream):\n",
    "    blank_grads = \\\n",
    "      alpha + beta[:, 1: , :] \\\n",
    "      - tf.reshape(total_log_pr, shape=(tf.shape(total_log_pr)[0], 1, 1))\n",
    "    truth_grads = \\\n",
    "      alpha + tf.pad(\n",
    "        beta[:, : -1, 1: ],\n",
    "        [[0, 0], [0, 0], [0, 1]],\n",
    "        constant_values=LOG_0\n",
    "      ) \\\n",
    "      - tf.reshape(total_log_pr, shape=(tf.shape(total_log_pr)[0], 1, 1))\n",
    "    blank_one_hot = tf.one_hot(tf.zeros_like(labels, dtype=tf.int32), tf.shape(log_pr)[-1], dtype=tf.float32)\n",
    "    blank_one_hot = tf.expand_dims(blank_one_hot, axis=1)\n",
    "    blank_one_hot = tf.repeat(blank_one_hot, tf.shape(log_pr)[1], axis=1)\n",
    "    blank_grads = tf.exp(tf.expand_dims(blank_grads, axis=-1) + log_pr) * blank_one_hot\n",
    "    truth_one_hot = tf.one_hot(labels[:, 1: ], tf.shape(log_pr)[-1], dtype=tf.float32)\n",
    "    truth_one_hot = tf.concat([truth_one_hot, tf.zeros((tf.shape(log_pr)[0], 1, tf.shape(log_pr)[-1]), dtype=tf.float32)], axis=-2)\n",
    "    truth_one_hot = tf.expand_dims(truth_one_hot, axis=1)\n",
    "    truth_one_hot = tf.repeat(truth_one_hot, tf.shape(log_pr)[1], axis=1)\n",
    "    truth_grads = tf.exp(tf.expand_dims(truth_grads, axis=-1) + log_pr) * truth_one_hot\n",
    "\n",
    "    grads = blank_grads + truth_grads\n",
    "\n",
    "    return (\n",
    "      [\n",
    "        tf.reshape(-upstream, shape=(tf.shape(upstream)[0], 1, 1, 1))\n",
    "        * grads * total_mask\n",
    "      ] +\n",
    "      [None] * 3\n",
    "    )\n",
    "\n",
    "  return -total_log_pr, grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 256\n",
    "UNITS = 1024\n",
    "CODER_OUTPUT_DIM = 512\n",
    "JOINT_NET_INNER_DIM = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder (tf.keras.Model):\n",
    "  def __init__ (self, units, output_dim):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.gru = tf.keras.layers.GRU(units, recurrent_initializer='glorot_uniform', return_sequences=True, return_state=True)\n",
    "    self.fc = tf.keras.layers.Dense(output_dim)\n",
    "    \n",
    "  def call (self, inputs):\n",
    "    rnn_outputs, hidden_states = self.gru(inputs)\n",
    "    outputs = self.fc(rnn_outputs)\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder (tf.keras.Model):\n",
    "  def __init__ (self, vocab_size, embedding_dim, units, output_dim):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(units, recurrent_initializer='glorot_uniform', return_sequences=True, return_state=True)\n",
    "    self.fc = tf.keras.layers.Dense(output_dim)\n",
    "\n",
    "  def call (self, inputs):\n",
    "    embedded = self.embedding(inputs)\n",
    "    outputs, hidden_states = self.gru(embedded)\n",
    "    outputs = self.fc(outputs)\n",
    "    return outputs, hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointNet (tf.keras.Model):\n",
    "  def __init__ (self, inner_dim, vocab_size):\n",
    "    super(JointNet, self).__init__()\n",
    "    self.forward_layer = tf.keras.layers.Dense(inner_dim, activation='tanh')\n",
    "    self.project_layer = tf.keras.layers.Dense(vocab_size)\n",
    "  \n",
    "  def call (self, inputs):\n",
    "    enc_outputs, dec_outputs = inputs\n",
    "    joint_inputs = tf.expand_dims(enc_outputs, axis=2) + tf.expand_dims(dec_outputs, axis=1)\n",
    "    outputs = self.forward_layer(joint_inputs)\n",
    "    outputs = self.project_layer(outputs)\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transducer (tf.keras.Model):\n",
    "  def __init__ (self, embedding_dim, units, coder_output_dim, joint_net_inner_dim, vocab_size):\n",
    "    super(Transducer, self).__init__()\n",
    "    self.encoder = Encoder(units, coder_output_dim)\n",
    "    self.decoder = Decoder(vocab_size, embedding_dim, units, coder_output_dim)\n",
    "    self.joint_net = JointNet(joint_net_inner_dim, vocab_size)\n",
    "    self.vocab_size = vocab_size\n",
    "  \n",
    "  def call (self, inputs):\n",
    "    enc_inputs, dec_inputs = inputs\n",
    "    enc_outputs = self.encoder(enc_inputs)\n",
    "    dec_outputs, dec_states = self.decoder(dec_inputs)\n",
    "    outputs = self.joint_net([enc_outputs, dec_outputs])\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import librosa\n",
    "from functools import reduce\n",
    "\n",
    "class Saver:\n",
    "  def save (self, path, cur_epoch, cnt):\n",
    "    pass\n",
    "\n",
    "class InitialSaver:\n",
    "  def initial_save (self):\n",
    "    pass\n",
    "\n",
    "class TrainPolicy:\n",
    "  def train (self):\n",
    "    pass\n",
    "\n",
    "class StepPolicy (TrainPolicy):\n",
    "  def __init__ (\n",
    "    self,\n",
    "    optimizer,\n",
    "    epochs,\n",
    "    batch_size,\n",
    "    batch_num,\n",
    "    time_limit,\n",
    "    label_limit,\n",
    "    step_performer,\n",
    "    step_end_handlers,\n",
    "    epoch_end_handlers,\n",
    "    dataset,\n",
    "    dataset_extractor,\n",
    "    cur_epoch,\n",
    "    cnt\n",
    "  ):\n",
    "    self.optimizer = optimizer\n",
    "    self.epochs = epochs\n",
    "    self.batch_size = batch_size\n",
    "    self.batch_num = batch_num\n",
    "    self.time_limit = time_limit\n",
    "    self.label_limit = label_limit\n",
    "\n",
    "    self.step_performer = step_performer\n",
    "    self.step_end_handlers = step_end_handlers\n",
    "    self.epoch_end_handlers = epoch_end_handlers\n",
    "\n",
    "    self.dataset = dataset\n",
    "    self.dataset_extractor = dataset_extractor\n",
    "    self.cur_epoch = cur_epoch\n",
    "    self.cnt = cnt\n",
    "\n",
    "  def train (self):\n",
    "    for i in range(self.cur_epoch, self.epochs):\n",
    "      while not self.dataset_extractor.check_end():\n",
    "        loss = self.step_performer.step(self.optimizer, self.batch_size, self.batch_num, self.dataset_extractor.get_value())\n",
    "        self.cnt += self.batch_num * self.batch_size\n",
    "        for step_end_handler in self.step_end_handlers:\n",
    "          step_end_handler.handle(self.cur_epoch, self.cnt, loss, self.batch_num * self.batch_size)\n",
    "        self.dataset_extractor = self.dataset_extractor.get_next_extractor()\n",
    "      \n",
    "      self.cur_epoch += 1\n",
    "      self.cnt = 0\n",
    "      self.dataset.shuffle()\n",
    "      self.dataset_extractor = SavableDatasetStepExtractor(\n",
    "        self.dataset,\n",
    "        self.batch_size * self.batch_num,\n",
    "        DatasetSizeLimitingIterator(self.dataset, self.time_limit, self.label_limit, 0)\n",
    "      )\n",
    "      for epoch_end_handler in self.epoch_end_handlers:\n",
    "        epoch_end_handler.handle(self.epochs, self.cur_epoch, self.cnt)\n",
    "\n",
    "class SavableStepPolicy (StepPolicy, Saver, InitialSaver):\n",
    "  def __init__ (\n",
    "    self,\n",
    "    path,\n",
    "    savable_save_notifier,\n",
    "    optimizer,\n",
    "    epochs,\n",
    "    batch_size,\n",
    "    batch_num,\n",
    "    time_limit,\n",
    "    label_limit,\n",
    "    step_performer,\n",
    "    step_end_handlers,\n",
    "    epoch_end_handlers,\n",
    "    dataset,\n",
    "    dataset_extractor,\n",
    "    cur_epoch,\n",
    "    cnt,\n",
    "    prev_save_time\n",
    "  ):\n",
    "    StepPolicy.__init__(\n",
    "      self,\n",
    "      optimizer,\n",
    "      epochs,\n",
    "      batch_size,\n",
    "      batch_num,\n",
    "      time_limit,\n",
    "      label_limit,\n",
    "      step_performer,\n",
    "      step_end_handlers,\n",
    "      epoch_end_handlers,\n",
    "      dataset,\n",
    "      dataset_extractor,\n",
    "      cur_epoch,\n",
    "      cnt\n",
    "    )\n",
    "    savable_save_notifier.savers.append(self)\n",
    "    savable_save_notifier.savers.append(self.step_performer)\n",
    "    self.step_end_handlers.append(\n",
    "      savable_save_notifier\n",
    "    )\n",
    "    self.epoch_end_handlers.append(\n",
    "      SavableEpochSaveNotifier(\n",
    "        path,\n",
    "        [\n",
    "          self,\n",
    "          self.step_performer,\n",
    "        ],\n",
    "        prev_save_time\n",
    "      )\n",
    "    )\n",
    "  \n",
    "  def save (self, path, cur_epoch, cnt):\n",
    "    with open(os.path.join(path, 'progress.pickle'), 'wb') as f:\n",
    "      pickle.dump((self.cur_epoch, self.cnt), f)\n",
    "    self.dataset_extractor.save(IteratorSaver(os.path.join(path, 'dataset_extractor')))\n",
    "\n",
    "  def initial_save (self, path):\n",
    "    with open(os.path.join(path, 'progress.pickle'), 'wb') as f:\n",
    "      pickle.dump((self.cur_epoch, self.cnt), f)\n",
    "    self.dataset_extractor.save(ExtractorSaver(os.path.join(path, 'dataset_extractor')))\n",
    "    with open(os.path.join(path, 'policy_meta.pickle'), 'wb') as f:\n",
    "      pickle.dump((self.batch_num, self.batch_size, self.time_limit, self.label_limit), f)\n",
    "\n",
    "class TrainStepPerformer:\n",
    "  def __init__ (self, transducer):\n",
    "    self.transducer = transducer\n",
    "\n",
    "  def step (self, optimizer, batch_size, batch_num, datas):\n",
    "    acc_grads = [tf.zeros_like(var) for var in self.transducer.trainable_variables]\n",
    "    acc_loss = 0\n",
    "    for i in range(0, len(datas), batch_size):\n",
    "      batch_datas = datas[i: i + batch_size]\n",
    "      label_lengths = self.get_label_lengths(batch_datas)\n",
    "      label_lengths = tf.convert_to_tensor(label_lengths, dtype=tf.int32)\n",
    "      labels = pad_sequences(list(map(lambda x: x[1], batch_datas)), maxlen=self.get_labels_maxlen(batch_datas), truncating='post', padding='post')\n",
    "      labels = list(map(lambda x: x[1], batch_datas))\n",
    "      max_label_length = reduce(lambda a, x: max(a, x), [len(label) for label in labels])\n",
    "      labels = [label + [0] * (max_label_length - len(label)) for label in labels]\n",
    "      labels = tf.constant(labels, dtype=tf.int32)\n",
    "      spectrograms = [x[0] for x in batch_datas]\n",
    "      spectrogram_lengths = self.get_spectrogram_lengths(spectrograms)\n",
    "      spectrogram_lengths = tf.convert_to_tensor(spectrogram_lengths, dtype=tf.int32)\n",
    "      spectrograms = tf.cast(pad_sequences(spectrograms, maxlen=self.get_spectrograms_maxlen(spectrograms), truncating='post', padding='post'), dtype=tf.float32)\n",
    "      with tf.GradientTape() as tape:\n",
    "        logits = self.transducer([spectrograms, labels])\n",
    "        loss = rnnt_loss(\n",
    "          logits,\n",
    "          labels,\n",
    "          spectrogram_lengths,\n",
    "          label_lengths\n",
    "        )\n",
    "      acc_loss += loss.numpy()\n",
    "      grads = tape.gradient(loss, self.transducer.trainable_variables)\n",
    "      acc_grads = [acc_grad + grad for acc_grad, grad in zip(acc_grads, grads)]\n",
    "    acc_grads = [acc_grad / (batch_size * batch_num) for acc_grad in acc_grads]\n",
    "    optimizer.apply_gradients(zip(acc_grads, self.transducer.trainable_variables))\n",
    "    \n",
    "    return acc_loss / (batch_size * batch_num)\n",
    "  \n",
    "  def get_labels_maxlen (self, batch_datas):\n",
    "    ret = 0\n",
    "    for file_label_vector_pair in batch_datas:\n",
    "      ret = max(ret, len(file_label_vector_pair[1]))\n",
    "    return ret\n",
    "  \n",
    "  def get_label_lengths (self, batch_datas):\n",
    "    ret = list(map(lambda x: len(x[1]), batch_datas))\n",
    "    return ret\n",
    "  \n",
    "  def get_spectrograms_maxlen (self, spectrograms):\n",
    "    ret = 0\n",
    "    for spectrogram in spectrograms:\n",
    "      ret = max(ret, spectrogram.shape[0])\n",
    "    return ret\n",
    "\n",
    "  def get_spectrogram_lengths (self, spectrograms):\n",
    "    ret = list(map(lambda x: x.shape[0], spectrograms))\n",
    "    return ret\n",
    "\n",
    "class SavableTrainStepPerformer (TrainStepPerformer, Saver, InitialSaver):\n",
    "  def __init__ (self, transducer, embedding_dim, units, coder_output_dim, joint_net_inner_dim, vocab_size):\n",
    "    TrainStepPerformer.__init__(self, transducer)\n",
    "    self.embedding_dim = embedding_dim\n",
    "    self.units = units\n",
    "    self.coder_output_dim = coder_output_dim\n",
    "    self.joint_net_inner_dim = joint_net_inner_dim\n",
    "    self.vocab_size = vocab_size\n",
    "\n",
    "  def save (self, path, cur_epoch, cnt):\n",
    "    self.transducer.save_weights(os.path.join(path, 'checkpoints/ep{}_no{}/ckpt'.format(cur_epoch, cnt)))\n",
    "\n",
    "  def initial_save (self, path):\n",
    "    self.transducer.save_weights(os.path.join(path, 'checkpoints/ep{}_no{}/ckpt'.format(0, 0)))\n",
    "    with open(os.path.join(path, 'model_meta.pickle'), 'wb') as f:\n",
    "      pickle.dump((\n",
    "        self.embedding_dim,\n",
    "        self.units,\n",
    "        self.coder_output_dim,\n",
    "        self.joint_net_inner_dim,\n",
    "        self.vocab_size\n",
    "      ), f)\n",
    "\n",
    "class SavableTrainStepPerformerLoader:\n",
    "  @staticmethod\n",
    "  def load (path):\n",
    "    with open(os.path.join(path, 'model_meta.pickle'), 'rb') as f:\n",
    "      model_meta = pickle.load(f)\n",
    "    with open(os.path.join(path, 'progress.pickle'), 'rb') as f:\n",
    "      progress = pickle.load(f)\n",
    "    transducer = Transducer(*model_meta)\n",
    "    transducer([\n",
    "      tf.keras.Input(shape=(None, 128), dtype=tf.float32),\n",
    "      tf.keras.Input(shape=(None, ), dtype=tf.int32)\n",
    "    ])\n",
    "    transducer.load_weights(os.path.join(path, 'checkpoints/ep{}_no{}/ckpt'.format(progress[0], progress[1])))\n",
    "    return SavableTrainStepPerformer(\n",
    "      transducer,\n",
    "      *model_meta\n",
    "    )\n",
    "\n",
    "class PrevSaveTime:\n",
    "  def __init__ (self, prev_save_time):\n",
    "    self.prev_save_time = prev_save_time\n",
    "  \n",
    "  def update (self, prev_save_time):\n",
    "    self.prev_save_time = prev_save_time\n",
    "\n",
    "class EpochEndHandler:\n",
    "  def handle (self, epochs, cur_epoch, cnt):\n",
    "    pass\n",
    "\n",
    "class EpochPrinter (EpochEndHandler):\n",
    "  def handle (self, epochs, cur_epoch, cnt):\n",
    "    print('epoch: {} / {}'.format(cur_epoch, epochs))\n",
    "\n",
    "class EpochSaveNotifier (EpochEndHandler):\n",
    "  def __init__ (self, path, savers):\n",
    "    super(EpochSaveNotifier, self).__init__()\n",
    "    self.path = path\n",
    "    self.savers = savers\n",
    "\n",
    "  def handle (self, epochs, cur_epoch, cnt):\n",
    "    for saver in self.savers:\n",
    "      saver.save(self.path, cur_epoch, cnt)\n",
    "  \n",
    "class SavableEpochSaveNotifier (EpochSaveNotifier):\n",
    "  def __init__ (self, path, savers, prev_save_time):\n",
    "    super(SavableEpochSaveNotifier, self).__init__(path, savers)\n",
    "    self.prev_save_time = prev_save_time\n",
    "    self.savers.append(self)\n",
    "  \n",
    "  def save (self, path, cur_epoch, cnt):\n",
    "    with open(os.path.join(path, 'prev_save_time.pickle'), 'wb') as f:\n",
    "      pickle.dump(0, f)\n",
    "    self.prev_save_time.update(0)\n",
    "\n",
    "class StepEndHandler:\n",
    "  def handle (self, cur_epoch, cnt, loss, step_size):\n",
    "    pass\n",
    "\n",
    "class StepPrinter (StepEndHandler):\n",
    "  def handle (self, cur_epoch, cnt, loss, step_size):\n",
    "    print('epoch, cnt: {}, {}'.format(cur_epoch, cnt))\n",
    "    print('loss: {}'.format(loss / step_size))\n",
    "\n",
    "class SaveNotifier (StepEndHandler):\n",
    "  def __init__ (self, path, savers, save_time_delta, prev_save_time):\n",
    "    super(SaveNotifier, self).__init__()\n",
    "    self.path = path\n",
    "    self.savers = savers\n",
    "    self.save_time_delta = save_time_delta\n",
    "    self.prev_save_time = prev_save_time\n",
    "\n",
    "  def handle (self, cur_epoch, cnt, loss, step_size):\n",
    "    if self.prev_save_time.prev_save_time + self.save_time_delta <= cnt:\n",
    "      for saver in self.savers:\n",
    "        saver.save(self.path, cur_epoch, cnt)\n",
    "      self.prev_save_time.update(cnt)\n",
    "\n",
    "class SavableSaveNotifier (SaveNotifier, Saver, InitialSaver):\n",
    "  def __init__ (self, path, savers, save_time_delta, prev_save_time):\n",
    "    super(SavableSaveNotifier, self).__init__(path, savers, save_time_delta, prev_save_time)\n",
    "    self.savers.append(self)\n",
    "  \n",
    "  def save (self, path, cur_epoch, cnt):\n",
    "    with open(os.path.join(path, 'prev_save_time.pickle'), 'wb') as f:\n",
    "      pickle.dump(cnt, f)\n",
    "  \n",
    "  def initial_save (self, path):\n",
    "    with open(os.path.join(path, 'prev_save_time.pickle'), 'wb') as f:\n",
    "      pickle.dump(0, f)\n",
    "\n",
    "class InitialSaveNotifier:\n",
    "  def __init__ (self, path, initial_savers):\n",
    "    self.path = path\n",
    "    self.initial_savers = initial_savers\n",
    "\n",
    "  def initial_save (self):\n",
    "    os.makedirs(self.path, exist_ok=True)\n",
    "    for initial_saver in self.initial_savers:\n",
    "      initial_saver.initial_save(self.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePolicyGenerator:\n",
    "  @staticmethod\n",
    "  def generate (save_path):\n",
    "    prev_save_time = PrevSaveTime(0)\n",
    "    savable_save_notifier = SavableSaveNotifier(save_path, [], 5000, prev_save_time)\n",
    "    batch_size = 16\n",
    "    batch_num = 2\n",
    "    step_size = batch_size * batch_num\n",
    "    time_limit = 160\n",
    "    label_limit = 50\n",
    "    dataset = ShufflableDatasetGenerator.generate()\n",
    "    dataset_extractor = SavableDatasetStepExtractor(\n",
    "      dataset,\n",
    "      step_size,\n",
    "      DatasetSizeLimitingIterator(dataset, time_limit, label_limit, 0)\n",
    "    )\n",
    "    cur_epoch = 0\n",
    "    cnt = 0\n",
    "\n",
    "    model_meta = (256, 1024, 512, 1024, len(dataset_extractor.dataset.phoneme.phonemes))\n",
    "    transducer = Transducer(*model_meta)\n",
    "    transducer([\n",
    "      tf.keras.Input(shape=(None, 128), dtype=tf.float32),\n",
    "      tf.keras.Input(shape=(None, ), dtype=tf.int32)\n",
    "    ])\n",
    "\n",
    "    policy = SavableStepPolicy(\n",
    "      save_path,\n",
    "      savable_save_notifier,\n",
    "      tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "      20,\n",
    "      batch_size,\n",
    "      batch_num,\n",
    "      160,\n",
    "      50,\n",
    "      SavableTrainStepPerformer(\n",
    "        transducer,\n",
    "        *model_meta\n",
    "      ),\n",
    "      [StepPrinter()],\n",
    "      [EpochPrinter()],\n",
    "      dataset,\n",
    "      dataset_extractor,\n",
    "      cur_epoch,\n",
    "      cnt,\n",
    "      prev_save_time\n",
    "    )\n",
    "    return \\\n",
    "      policy, \\\n",
    "      InitialSaveNotifier(\n",
    "        save_path, [\n",
    "          policy,\n",
    "          policy.step_performer,\n",
    "          savable_save_notifier\n",
    "        ]\n",
    "      )\n",
    "\n",
    "class SimplePolicyLoader:\n",
    "  @staticmethod\n",
    "  def load (path):\n",
    "    with open(os.path.join(path, 'progress.pickle'), 'rb') as f:\n",
    "      progress = pickle.load(f)\n",
    "    dataset_extractor = DatasetStepExtractorLoader.load(os.path.join(path, 'dataset_extractor'))\n",
    "    prev_save_time = PrevSaveTime(\n",
    "      pickle.load(\n",
    "        open(os.path.join(path, 'prev_save_time.pickle'), 'rb')\n",
    "      )\n",
    "    )\n",
    "    policy = SavableStepPolicy(\n",
    "      path,\n",
    "      SavableSaveNotifier(\n",
    "        path,\n",
    "        [],\n",
    "        5000,\n",
    "        prev_save_time\n",
    "      ),\n",
    "      tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "      20,\n",
    "      16,\n",
    "      2,\n",
    "      160,\n",
    "      50,\n",
    "      SavableTrainStepPerformerLoader.load(path),\n",
    "      [StepPrinter()],\n",
    "      [EpochPrinter()],\n",
    "      dataset_extractor.dataset,\n",
    "      dataset_extractor,\n",
    "      progress[0],\n",
    "      progress[1],\n",
    "      prev_save_time\n",
    "    )\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy, initial_save_notifier = SimplePolicyGenerator.generate('train_state4')\n",
    "# initial_save_notifier.initial_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = SimplePolicyLoader.load('train_state4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.device('/gpu:0'):\n",
    "#   policy.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_to_sentence (x):\n",
    "  return ''.join([policy.dataset_extractor.dataset.phoneme.phonemes[idx] for idx in x][1: ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDecoder:\n",
    "  def __init__ (self, transducer):\n",
    "    self.transducer = transducer\n",
    "  \n",
    "  def decode (self, spectrogram):\n",
    "    label = [0]\n",
    "    for i in range(spectrogram.shape[0]):\n",
    "      sub_spectrogram = spectrogram[0: i + 1]\n",
    "      sub_spectrogram = tf.convert_to_tensor(sub_spectrogram)\n",
    "      sub_spectrogram = tf.expand_dims(sub_spectrogram, axis=0)\n",
    "      sub_label = tf.constant([label], dtype=tf.int32)\n",
    "      sub_rst = self.transducer((sub_spectrogram, sub_label))\n",
    "      # print(tf.argmax(sub_rst, axis=-1)[:, :, : ][0].numpy(), end='\\n\\n')\n",
    "      sub_rst = tf.argmax(sub_rst[0][-1][-1], axis=-1).numpy()\n",
    "      if sub_rst != 0:\n",
    "        label.append(sub_rst)\n",
    "    return label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "능력은 있는 거는 애들 질\n",
      "능력은 인정하는데 이제\n",
      "\n",
      "바다가 그 치즈님 까지 버짔어\n",
      "받아 취준생까지 받을 수 있어\n",
      "\n",
      "남자 경리를 사라면 되\n",
      "다음 세대가 없는 게 사람이 없는 게\n",
      "\n",
      "너랑 일찍 았잖아\n",
      "나 얼마 전에 국진이랑 왔었잖아\n",
      "\n",
      "지금 아래서 연락 지\n",
      "지금 말해서 놀랐지?\n",
      "\n",
      "그 발근 거\n",
      "좀 밝은 거\n",
      "\n",
      "안 와 가끔 하는 스쿠\n",
      "아 언어가 그 언어였어?\n",
      "\n",
      "나 맞아 마 만나\n",
      "우리 안 봤잖아 아 봤나?\n",
      "\n",
      "응\n",
      "응\n",
      "\n",
      "어 나라트임 하서\n",
      "너 나랑 둘이 갔어\n",
      "\n",
      "엄청 아프지\n",
      "엄청 아프지?\n",
      "\n",
      "아 그렇게 못 샀다고\n",
      "나 살 빼고 있어서 밥 안 먹어\n",
      "\n",
      "아니면은 뭔 소랑이잖아\n",
      "왜냐면 너는 오프라인이잖아\n",
      "\n",
      "아 그치 따라 비\n",
      "약간 츤데레 같애\n",
      "\n",
      "안 봤는데 뭐지 그치 자\n",
      "난 받는 거 무조건 추천\n",
      "\n",
      "아 자꼰\n",
      "아시아권은\n",
      "\n",
      "어 근데 애전 책 쓰\n",
      "어 근데 일 점대였어\n",
      "\n",
      "아 가그\n",
      "아 바로?\n",
      "\n",
      "은해 갈라\n",
      "난 헷갈려\n",
      "\n",
      "가보기싸네 메강대\n",
      "가고 싶지 한양대에 근데\n",
      "\n",
      "어 맞아 맞아\n",
      "어 맞아 맞아\n",
      "\n",
      "스카이백\n",
      "스카이뷰\n",
      "\n",
      "어 좋게 시허끼가 너도 있으면 되나\n",
      "아 어처피 휴학 기간은 또 있으니까?\n",
      "\n",
      "다 그래서 별로 쓰러\n",
      "담배피자 불렀어요?\n",
      "\n",
      "그렇구나 그래서 우리\n",
      "어 근데 저런\n",
      "\n",
      "홍이 뭐 보시빠\n",
      "보미 보고 싶다\n",
      "\n",
      "나 하지 마\n",
      "맞아\n",
      "\n",
      "그 있지\n",
      "블리치?\n",
      "\n",
      "지금 조리 만나\n",
      "어쨌 글 둘이 만나\n",
      "\n",
      "잘 맞은 거 있나 보잖고\n",
      "잘 맞았으니까 이 년 반 사귄 거지\n",
      "\n",
      "꺼\n",
      "그까\n",
      "\n",
      "내만 아닌 거 같애\n",
      "틀린 말은 아닌 거 같애\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decoder = AudioDecoder(policy.step_performer.transducer)\n",
    "datas = policy.dataset_extractor.get_value()\n",
    "\n",
    "for data in datas:\n",
    "  a = list(filter(lambda x: x != 0, decoder.decode(data[0])))\n",
    "  print(moasseugi(vector_to_sentence([0] + a)))\n",
    "  print(moasseugi(vector_to_sentence(data[1])))\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_with_hint (logits):\n",
    "  rst = []\n",
    "  x, y = 0, 0\n",
    "  while x < logits.shape[2] and y < logits.shape[1]:\n",
    "    sel = tf.argmax(logits[0][y][x], axis=-1).numpy()\n",
    "    rst.append(sel)\n",
    "    if sel == 0:\n",
    "      y += 1\n",
    "    else:\n",
    "      x += 1\n",
    "  return moasseugi(vector_to_sentence([0] + list(filter(lambda x: x != 0, rst))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "능력은 인정하는데 이제\n",
      "닌ㅡ벽은 인정하믄데 이제\n",
      "\n",
      "받아 취준생까지 받을 수 있어\n",
      "바ㅏ닸ㅏ부킺찡익ㄱ 깓을 버 어 었\n",
      "\n",
      "다음 세대가 없는 게 사람이 없는 게\n",
      "남ㅏㅊㅅ 새드가 왤넨 사 아람이 없는ㄷ게\n",
      "\n",
      "나 얼마 전에 국진이랑 왔었잖아\n",
      "ㄴ말롤마 전에 붇진이랑 싰읐잖아\n",
      "\n",
      "지금 말해서 놀랐지?\n",
      "지ㅣ븜 말해서 낼랐지\n",
      "\n",
      "좀 밝은 거\n",
      "ㄱ뿜 발근 거\n",
      "\n",
      "아 언어가 그 언어였어?\n",
      "안ㅏ너 어 ㅏ 날시ㄸ저\n",
      "\n",
      "우리 안 봤잖아 아 봤나?\n",
      "너ㅓ ㅂ 벋 와밚ㅏ아 아 밨나?\n",
      "\n",
      "응\n",
      "ㅇ\n",
      "\n",
      "너 나랑 둘이 갔어\n",
      "언ㅏ나랐 추리ㅣ 갔어\n",
      "\n",
      "엄청 아프지?\n",
      "오ㅡ 청 아프지\n",
      "\n",
      "나 살 빼고 있어서 밥 안 먹어\n",
      "안ㅏ살 빼고 았어서 봅 먼 억\n",
      "\n",
      "왜냐면 너는 오프라인이잖아\n",
      "애ㅣ냐면 너는 오스라인이잖아\n",
      "\n",
      "약간 츤데레 같애\n",
      "아ㅏ간ㅏ 중게레 같으\n",
      "\n",
      "난 받는 거 무조건 추천\n",
      "아ㅏ느곱은 제 붜지긴\n",
      "\n",
      "아시아권은\n",
      "아ㅏ지아꼰\n",
      "\n",
      "어 근데 일 점대였어\n",
      "은ㅡ드 \n",
      "\n",
      "아 바로?\n",
      "아ㅏ 아로\n",
      "\n",
      "난 헷갈려\n",
      "아ㅏ르여어ㅏ\n",
      "\n",
      "가고 싶지 한양대에 근데\n",
      "가ㅏ보 싶이 안양대   근데\n",
      "\n",
      "어 맞아 맞아\n",
      "어ㅓ 낮아 맞아\n",
      "\n",
      "스카이뷰\n",
      "사ㅏ에이배\n",
      "\n",
      "아 어처피 휴학 기간은 또 있으니까?\n",
      "아ㅓ 지디피 슈학 기간을 또 있으미까\n",
      "\n",
      "담배피자 불렀어요?\n",
      "달ㅏ애 ㅣ자 부렀ㅓ어야?\n",
      "\n",
      "어 근데 저런\n",
      "거ㅓ 근데 개련 \n",
      "\n",
      "보미 보고 싶다\n",
      "호ㅗ긴보고 싶다\n",
      "\n",
      "맞아\n",
      "ㄴ마지ㅏ \n",
      "\n",
      "블리치?\n",
      "기ㅣ\n",
      "\n",
      "어쨌 글 둘이 만나\n",
      "질ㅣ ㅓㄷ주리마 ㅏㅏ 안나\n",
      "\n",
      "잘 맞았으니까 이 년 반 사귄 거지\n",
      "잘ㅏㅎ맛았으니까 익난 밪 사노\n",
      "\n",
      "그까\n",
      "꺼ㅓ ㅏ\n",
      "\n",
      "틀린 말은 아닌 거 같애\n",
      "느ㅡ옹ㅗ 않인 어  ㅈ 저 앝\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datas = policy.dataset_extractor.get_value()\n",
    "\n",
    "for data in datas:\n",
    "  logits = policy.step_performer.transducer((tf.constant([data[0]], dtype=tf.float32), tf.constant([[0] + data[1]], dtype=tf.int32)))\n",
    "  print(moasseugi(vector_to_sentence(data[1])))\n",
    "  print(decode_with_hint(logits))\n",
    "  print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ec62f2ec7c7d49224ec335e3ef472b8ef537dc7949933ed74784e8710c2a92fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
