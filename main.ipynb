{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o/ 근데 칠십 퍼센트가 커 보이긴 하는데 이백 벌다 백 사십 벌면 빡셀걸? b/\n",
      "근데 삼 학년 때 까지는 국가장학금 바+ 받으면서 다녔던 건가?\n"
     ]
    }
   ],
   "source": [
    "test1 = \"o/ 근데 (70%)/(칠십 퍼센트)가 커 보이긴 하는데 (200)/(이백) 벌다 (140)/(백 사십) 벌면 빡셀걸? b/\"\n",
    "test2 = \"근데 (3학년)/(삼 학년) 때 까지는 국가장학금 바+ 받으면서 다녔던 건가?\"\n",
    "\n",
    "def bracket_filter(sentence):\n",
    "  new_sentence = str()\n",
    "  flag = False\n",
    "  \n",
    "  for ch in sentence:\n",
    "    if ch == '(' and flag == False:\n",
    "      flag = True\n",
    "      continue\n",
    "    if ch == '(' and flag == True:\n",
    "      flag = False\n",
    "      continue\n",
    "    if ch != ')' and flag == False:\n",
    "      new_sentence += ch\n",
    "  return new_sentence\n",
    "\n",
    "print(bracket_filter(test1))\n",
    "print(bracket_filter(test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "근데 칠십 퍼센트가 커 보이긴 하는데 이백 벌다 백 사십 벌면 빡셀걸?\n",
      "c샾 배워봤어?\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "test1 = \"o/ 근데 칠십 퍼센트가 커 보이긴 하는데 이백 벌다 백 사십 벌면 빡셀걸? b/\"\n",
    "test2 = \"c# 배워봤어?\"\n",
    "\n",
    "def special_filter(sentence):\n",
    "  SENTENCE_MARK = ['?', '!']\n",
    "  NOISE = ['o', 'n', 'u', 'b', 'l']\n",
    "  EXCEPT = ['/', '+', '*', '-', '@', '$', '^', '&', '[', ']', '=', ':', ';', '.', ',']\n",
    "  \n",
    "  new_sentence = str()\n",
    "  for idx, ch in enumerate(sentence):\n",
    "    if ch not in SENTENCE_MARK:\n",
    "      # o/, n/ 등 처리\n",
    "      if idx + 1 < len(sentence) and ch in NOISE and sentence[idx+1] == '/': \n",
    "        continue \n",
    "\n",
    "    if ch == '#': \n",
    "      new_sentence += '샾'\n",
    "\n",
    "    elif ch not in EXCEPT: \n",
    "      new_sentence += ch\n",
    "\n",
    "  pattern = re.compile(r'\\s\\s+')\n",
    "  new_sentence = re.sub(pattern, ' ', new_sentence.strip())\n",
    "  return new_sentence\n",
    "\n",
    "print(special_filter(test1))\n",
    "print(special_filter(test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아 나 쓰리 DS 갖고 싶다 쓰리 DS\n"
     ]
    }
   ],
   "source": [
    "# test = \"o/ 근데 (70%)/(칠십 퍼센트)가 커 보이긴 하는데 (200)/(이백) 벌다 (140)/(백 사십) 벌면 빡셀걸? b/\"\n",
    "test = 'b/ 아 나 (3DS)/(쓰리 DS) 갖고 싶다. (3DS)/(쓰리 DS)'\n",
    "\n",
    "def sentence_filter(raw_sentence):\n",
    "  return special_filter(bracket_filter(raw_sentence))\n",
    "\n",
    "print(sentence_filter(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosung = (\"ㄱ\", \"ㄲ\", \"ㄴ\", \"ㄷ\", \"ㄸ\", \"ㄹ\", \"ㅁ\", \"ㅂ\", \"ㅃ\", \"ㅅ\", \"ㅆ\", \"ㅇ\", \"ㅈ\", \"ㅉ\", \"ㅊ\", \"ㅋ\", \"ㅌ\", \"ㅍ\", \"ㅎ\")\n",
    "\n",
    "jungsung = (\"ㅏ\", \"ㅐ\", \"ㅑ\", \"ㅒ\", \"ㅓ\", \"ㅔ\", \"ㅕ\", \"ㅖ\", \"ㅗ\", \"ㅘ\", \"ㅙ\", \"ㅚ\", \"ㅛ\", \"ㅜ\", \"ㅝ\", \"ㅞ\", \"ㅟ\", \"ㅠ\", \"ㅡ\", \"ㅢ\", \"ㅣ\")\n",
    "\n",
    "jongsung = (\"\", \"ㄱ\", \"ㄲ\", \"ㄳ\", \"ㄴ\", \"ㄵ\", \"ㄶ\", \"ㄷ\", \"ㄹ\", \"ㄺ\", \"ㄻ\", \"ㄼ\", \"ㄽ\", \"ㄾ\", \"ㄿ\", \"ㅀ\", \"ㅁ\", \"ㅂ\", \"ㅄ\", \"ㅅ\", \"ㅆ\", \"ㅇ\", \"ㅈ\", \"ㅊ\", \"ㅋ\", \"ㅌ\", \"ㅍ\", \"ㅎ\")\n",
    "\n",
    "def isHangeul(one_character):\n",
    "    return 0xAC00 <= ord(one_character[:1]) <= 0xD7A3\n",
    "\n",
    "def hangeulExplode(one_hangeul):\n",
    "    a = one_hangeul[:1]\n",
    "    if isHangeul(a) != True:\n",
    "        return False\n",
    "    b = ord(a) - 0xAC00\n",
    "    cho = b // (21*28)\n",
    "    jung = b % (21*28) // 28\n",
    "    jong = b % 28\n",
    "    if jong == 0:\n",
    "        return (chosung[cho], jungsung[jung])\n",
    "    else:\n",
    "        return (chosung[cho], jungsung[jung], jongsung[jong])\n",
    "\n",
    "def hangeulJoin(inputlist):\n",
    "    result = \"\"\n",
    "    cho, jung, jong = 0, 0, 0\n",
    "    inputlist.insert(0, \"\")\n",
    "    while len(inputlist) > 1:\n",
    "        if inputlist[-1] in jongsung:\n",
    "            if inputlist[-2] in jungsung:\n",
    "                jong = jongsung.index(inputlist.pop())\n",
    "            \n",
    "            else:\n",
    "                result += inputlist.pop()\n",
    "        elif inputlist[-1] in jungsung:\n",
    "            if inputlist[-2] in chosung:\n",
    "                jung = jungsung.index(inputlist.pop())\n",
    "                cho = chosung.index(inputlist.pop())\n",
    "                result += chr(0xAC00 + ((cho*21)+jung)*28+jong)\n",
    "                cho, jung, jong = 0, 0, 0\n",
    "            else:\n",
    "                result += inputlist.pop()\n",
    "\n",
    "        else:\n",
    "            result += inputlist.pop()\n",
    "    else:\n",
    "        return result[::-1]\n",
    "\n",
    "def pureosseugi(inputtext):\n",
    "    result = \"\"\n",
    "    for i in inputtext:\n",
    "        if isHangeul(i) == True:\n",
    "            for j in hangeulExplode(i):\n",
    "                result += j\n",
    "        else:\n",
    "            result += i\n",
    "    \n",
    "    return result\n",
    "\n",
    "def moasseugi(inputtext):\n",
    "    t1 = []\n",
    "    for i in inputtext:\n",
    "        t1.append(i)\n",
    "\n",
    "    return hangeulJoin(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "\n",
    "class Phoneme:\n",
    "  def __init__ (self):\n",
    "    self.BLANK_INDEX = 0\n",
    "    self.START_INDEX = 1\n",
    "    self.END_INDEX = 2\n",
    "    self.SPACE_INDEX = 3\n",
    "    self.OOV_INDEX = 4\n",
    "\n",
    "class PhonemeGenerator (Phoneme):\n",
    "  def __init__ (self, labels):\n",
    "    super(PhonemeGenerator, self).__init__()\n",
    "    self.phonemes = ['<BLANK>', '<START>', '<END>', ' ']\n",
    "\n",
    "    texts = []\n",
    "    for label in labels:\n",
    "      for phoneme in label:\n",
    "        if not isHangeul(phoneme) and not phoneme == ' ' and not phoneme == '?':\n",
    "          break\n",
    "        for pure in pureosseugi(phoneme):\n",
    "          texts.append(pure)\n",
    "\n",
    "    tokenizer = Tokenizer(filters='', lower=False, oov_token='<OOV>')\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "\n",
    "    for i in range(1, len(tokenizer.index_word) + 1):\n",
    "      self.phonemes.append(tokenizer.index_word[i])\n",
    "\n",
    "    self.phoneme_index = {}\n",
    "    for i, phoneme in enumerate(self.phonemes):\n",
    "      self.phoneme_index[phoneme] = i\n",
    "    \n",
    "    self.VOCAB_SIZE = len(self.phonemes)\n",
    "  \n",
    "  def save (self, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "      pickle.dump((self.phonemes, self.phoneme_index), f)\n",
    "\n",
    "\n",
    "class PhonemeLoader (Phoneme):\n",
    "  def __init__ (self, filename):\n",
    "    super(PhonemeLoader, self).__init__()\n",
    "    with open(filename, 'rb') as f:\n",
    "      self.phonemes, self.phoneme_index = pickle.load(f)\n",
    "\n",
    "    self.VOCAB_SIZE = len(self.phonemes)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "class Dataset:\n",
    "  def get_data (self, idx):\n",
    "    return (\n",
    "      self.extract_spectrogram(self.file_phoneme_vector_pairs[idx][0]),\n",
    "      [0] + self.file_phoneme_vector_pairs[idx][1]\n",
    "    )\n",
    "  \n",
    "  def get_len (self):\n",
    "    return len(self.file_phoneme_vector_pairs)\n",
    "\n",
    "  def extract_spectrogram (self, filename):\n",
    "    waveform = np.memmap(filename, dtype='h', mode='r')\n",
    "\n",
    "    sr = 16000\n",
    "    st = 1 / sr\n",
    "\n",
    "    t = np.linspace(0, waveform.shape[0] * st, waveform.shape[0])\n",
    "\n",
    "    S = librosa.feature.melspectrogram(y=np.float32(waveform), n_fft=512, hop_length=256, window='hamming', sr=sr, n_mels=128)\n",
    "    log_S = librosa.power_to_db(S, ref=np.max)\n",
    "    log_S = np.transpose(log_S)\n",
    "\n",
    "    return log_S\n",
    "\n",
    "class RandomDataset (Dataset):\n",
    "  def __init__ (self):\n",
    "    super(RandomDataset, self).__init__()\n",
    "    with open('./ko-audio-dataset/sentence-script/KsponSpeech_scripts/train.trn', 'r', encoding='utf-8') as file:\n",
    "      lines = file.readlines()\n",
    "    def process_line (line):\n",
    "      line = line.split(' :: ')\n",
    "      line[0] = os.path.join('./ko-audio-dataset/audio/', line[0])\n",
    "      line[1] = sentence_filter(line[1])\n",
    "      return line\n",
    "    file_label_pairs = list(map(process_line, lines))\n",
    "    file_label_pairs = list(filter(lambda pair: not self.check_skip_label(pair[1]), file_label_pairs))\n",
    "    self.phoneme = PhonemeGenerator([label for file, label in file_label_pairs])\n",
    "    file_phoneme_pairs = list(map(lambda pair: (pair[0], pureosseugi(pair[1])), file_label_pairs))\n",
    "    self.file_phoneme_vector_pairs = [[pair[0], [self.phoneme.phoneme_index[phoneme] for phoneme in pair[1]]] for pair in file_phoneme_pairs]\n",
    "    random.shuffle(self.file_phoneme_vector_pairs)\n",
    "  \n",
    "  def save (self, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "      pickle.dump((self.phoneme, self.file_phoneme_vector_pairs), f)\n",
    "  \n",
    "  def check_skip_hangul (self, hangul):\n",
    "    return not isHangeul(hangul) and not hangul == ' ' and not hangul == '?'\n",
    "\n",
    "  def check_skip_label (self, label):\n",
    "    for hangul in label:\n",
    "      if self.check_skip_hangul(hangul):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "class DatasetLoader (Dataset):\n",
    "  def __init__ (self, filename):\n",
    "    super(DatasetLoader, self).__init__()\n",
    "    with open(filename, 'rb') as f:\n",
    "      self.phoneme, self.file_phoneme_vector_pairs = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = RandomDataset()\n",
    "dataset.save('dataset.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetLoader('dataset.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "811440\n"
     ]
    }
   ],
   "source": [
    "idx = 1\n",
    "print(dataset.get_data(idx)[0].shape[0] * len(dataset.get_data(idx)[1]) * len(dataset.phoneme.phonemes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetSizeLimitingIterator:\n",
    "  def __init__ (self, dataset, limit, idx):\n",
    "    self.dataset = dataset\n",
    "    self.limit = limit\n",
    "    self.idx = idx\n",
    "    while self.idx < self.dataset.get_len():\n",
    "      data = self.dataset.get_data(self.idx)\n",
    "      if data[0].shape[0] * len(data[1]) * len(self.dataset.phoneme.phonemes) <= self.limit:\n",
    "        break\n",
    "      self.idx += 1\n",
    "\n",
    "  def next (self):\n",
    "    idx = self.idx + 1\n",
    "    while idx < self.dataset.get_len():\n",
    "      data = self.dataset.get_data(idx)\n",
    "      if data[0].shape[0] * len(data[1]) * len(self.dataset.phoneme.phonemes) <= self.limit:\n",
    "        break\n",
    "      idx += 1\n",
    "    return DatasetSizeLimitingIterator(self.dataset, self.limit, idx)\n",
    "  \n",
    "  def check_end (self):\n",
    "    return self.idx >= self.dataset.get_len()\n",
    "\n",
    "  def get_value (self):\n",
    "    return self.dataset.get_data(self.idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetStepExtractor:\n",
    "  def __init__ (self, dataset, step_size, iterator):\n",
    "    self.dataset = dataset\n",
    "    self.step_size = step_size\n",
    "    self.value = []\n",
    "    while not iterator.check_end() and len(self.value) < self.step_size:\n",
    "      self.value.append(iterator.get_value())\n",
    "      iterator = iterator.next()\n",
    "    self.is_end = len(self.value) < self.step_size\n",
    "    self.next_iterator = iterator\n",
    "  \n",
    "  def check_end (self):\n",
    "    return self.is_end\n",
    "  \n",
    "  def get_value (self):\n",
    "    return self.value\n",
    "  \n",
    "  def get_next_extractor (self):\n",
    "    return DatasetStepExtractor(self.dataset, self.step_size, self.next_iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP_SIZE = 100\n",
    "SIZE_LIMIT = 100000\n",
    "extractor = DatasetStepExtractor(dataset, STEP_SIZE, DatasetSizeLimitingIterator(dataset, SIZE_LIMIT, 0))\n",
    "\n",
    "# for _ in range(10):\n",
    "#   values = extractor.get_values()\n",
    "#   print(len(values), values[0][0].shape, len(values[0][1]))\n",
    "#   extractor = extractor.get_next_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ 여기 까지 dataset 부분 ###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnnt_loss (logits, labels, time_lengths, label_lengths):\n",
    "  log_pr = tf.math.log_softmax(logits, axis=-1)\n",
    "  pr = pr_loss(log_pr, labels, time_lengths, label_lengths)\n",
    "  ret = tf.reduce_sum(pr)\n",
    "  return ret\n",
    "\n",
    "@tf.custom_gradient\n",
    "def pr_loss (log_pr, labels, time_lengths, label_lengths):\n",
    "  LOG_0 = float('-inf')\n",
    "  batch_size = log_pr.shape[0]\n",
    "  max_time_lengths = log_pr.shape[1]\n",
    "  max_label_lengths = log_pr.shape[2]\n",
    "\n",
    "  def get_truth_log_pr (log_pr, labels):\n",
    "    labels_one_hot = tf.one_hot(labels, tf.shape(log_pr)[-1], axis=-1, dtype=tf.float32)\n",
    "    labels_one_hot = tf.expand_dims(labels_one_hot, axis=1)\n",
    "    labels_one_hot = tf.repeat(labels_one_hot, tf.shape(log_pr)[1], axis=1)\n",
    "    return tf.reduce_sum(log_pr * labels_one_hot, axis=-1)\n",
    "  \n",
    "  def get_blank_log_pr (log_pr):\n",
    "    return log_pr[:, :, :, 0]\n",
    "\n",
    "  truth_log_pr = get_truth_log_pr(log_pr, labels)\n",
    "  blank_log_pr = get_blank_log_pr(log_pr)\n",
    "\n",
    "  def get_alpha (truth_log_pr, blank_log_pr):\n",
    "    reversed_truth_log_pr = tf.reverse(truth_log_pr, axis=[-2])\n",
    "    padded_truth_log_pr = tf.pad(\n",
    "      reversed_truth_log_pr,\n",
    "      [[0, 0], [0, 0], [tf.shape(reversed_truth_log_pr)[-2] - 1, 0]],\n",
    "      constant_values=LOG_0\n",
    "    )\n",
    "    truth_diag = tf.linalg.diag_part(\n",
    "      padded_truth_log_pr,\n",
    "      k=(0, tf.shape(padded_truth_log_pr)[-1] - 1),\n",
    "      padding_value=LOG_0,\n",
    "      align='LEFT_RIGHT'\n",
    "    )\n",
    "    truth_diag = tf.reverse(truth_diag, axis=[-2])\n",
    "    truth_diag = tf.pad(truth_diag, [[0, 0], [0, 0], [1, 0]], constant_values=LOG_0)\n",
    "\n",
    "    reversed_blank_log_pr = tf.reverse(blank_log_pr, axis=[-2])\n",
    "    padded_blank_log_pr = tf.pad(\n",
    "      reversed_blank_log_pr,\n",
    "      [[0, 0], [0, 0], [tf.shape(reversed_blank_log_pr)[-2] - 1, 0]],\n",
    "      constant_values=LOG_0\n",
    "    )\n",
    "    blank_diag = tf.linalg.diag_part(\n",
    "      padded_blank_log_pr,\n",
    "      k=(0, tf.shape(padded_blank_log_pr)[-1] - 1),\n",
    "      padding_value=LOG_0,\n",
    "      align='LEFT_RIGHT'\n",
    "    )\n",
    "    blank_diag = tf.reverse(blank_diag, axis=[-2])\n",
    "    blank_diag = tf.pad(blank_diag, [[0, 0], [0, 0], [0, 1]], constant_values=LOG_0)\n",
    "\n",
    "    initial_diag = tf.concat([\n",
    "      LOG_0 * tf.ones((tf.shape(blank_diag)[0], tf.shape(blank_diag)[-1] - 2), tf.float32),\n",
    "      tf.zeros((tf.shape(blank_diag)[0], 1), dtype=tf.float32)\n",
    "    ], axis=-1)\n",
    "\n",
    "    def step (a, x):\n",
    "      t, b = x\n",
    "      return (\n",
    "        tf.reduce_logsumexp(\n",
    "          tf.stack([\n",
    "            tf.concat([LOG_0 * tf.ones((tf.shape(a)[0], 1), dtype=tf.float32), a], axis=-1) + t,\n",
    "            tf.concat([a, LOG_0 * tf.ones((tf.shape(a)[0], 1), dtype=tf.float32)], axis=-1) + b\n",
    "          ]), axis=0\n",
    "        )[:, 1: ]\n",
    "      )\n",
    "\n",
    "    alpha_diag = tf.concat([\n",
    "      tf.expand_dims(initial_diag, axis=0),\n",
    "      tf.scan(step, (tf.transpose(truth_diag, perm=[1, 0, 2]), tf.transpose(blank_diag, perm=[1, 0, 2])), initial_diag)\n",
    "    ], axis=0)\n",
    "\n",
    "    alpha_diag = tf.transpose(alpha_diag, perm=[1, 0, 2])\n",
    "\n",
    "    alpha_diag = tf.reverse(alpha_diag, axis=[-1])\n",
    "\n",
    "    alpha_diag = tf.transpose(alpha_diag, perm=[0, 2, 1])\n",
    "    alpha = tf.linalg.diag_part(alpha_diag, k=(0, max_label_lengths - 1))\n",
    "    alpha = tf.reshape(alpha, (tf.shape(truth_log_pr)[0], tf.shape(truth_log_pr)[2], tf.shape(truth_log_pr)[1]))\n",
    "    alpha = tf.reverse(alpha, axis=[-2])\n",
    "    alpha = tf.transpose(alpha, perm=[0, 2, 1])\n",
    "    return alpha\n",
    "  \n",
    "  def get_beta (truth_log_pr, blank_log_pr):\n",
    "    padded_truth_log_pr = tf.reverse(truth_log_pr, axis=[-2])\n",
    "    padded_truth_log_pr = tf.pad(\n",
    "      padded_truth_log_pr,\n",
    "      [[0, 0], [0, 0], [tf.shape(padded_truth_log_pr)[-2] - 1, 0]],\n",
    "      constant_values=LOG_0\n",
    "    )\n",
    "    truth_diag = tf.linalg.diag_part(\n",
    "      padded_truth_log_pr,\n",
    "      k=(0, tf.shape(padded_truth_log_pr)[-1] - 1),\n",
    "      padding_value=LOG_0,\n",
    "      align='LEFT_RIGHT'\n",
    "    )\n",
    "    truth_diag = tf.pad(truth_diag, [[0, 0], [0, 0], [0, 1]], constant_values=LOG_0)\n",
    "\n",
    "    reversed_blank_log_pr = tf.reverse(blank_log_pr, axis=[-2])\n",
    "    padded_blank_log_pr = tf.pad(\n",
    "      reversed_blank_log_pr,\n",
    "      [[0, 0], [0, 0], [tf.shape(reversed_blank_log_pr)[-2] - 1, 0]],\n",
    "      constant_values=LOG_0\n",
    "    )\n",
    "    blank_diag = tf.linalg.diag_part(\n",
    "      padded_blank_log_pr,\n",
    "      k=(0, tf.shape(padded_blank_log_pr)[-1] - 1),\n",
    "      padding_value=LOG_0,\n",
    "      align='LEFT_RIGHT'\n",
    "    )\n",
    "    blank_diag = tf.pad(blank_diag, [[0, 0], [0, 0], [1, 0]], constant_values=LOG_0)\n",
    "\n",
    "    initial_diag = tf.concat([\n",
    "      tf.reshape(blank_log_pr[:, -1, -1], (tf.shape(blank_log_pr)[0], 1)),\n",
    "      # -0.5 * tf.ones((tf.shape(blank_diag)[0], 1), dtype=tf.float32),\n",
    "      LOG_0 * tf.ones((tf.shape(blank_diag)[0], tf.shape(blank_diag)[-1] - 2), tf.float32),\n",
    "    ], axis=-1)\n",
    "\n",
    "    def step (a, x):\n",
    "      t, b = x\n",
    "      return (\n",
    "        tf.reduce_logsumexp(\n",
    "          tf.stack([\n",
    "            tf.concat([a, LOG_0 * tf.ones((tf.shape(a)[0], 1), dtype=tf.float32)], axis=-1) + t,\n",
    "            tf.concat([LOG_0 * tf.ones((tf.shape(a)[0], 1), dtype=tf.float32), a], axis=-1) + b\n",
    "          ]), axis=0\n",
    "        )[:, : -1]\n",
    "      )\n",
    "\n",
    "    beta_diag = tf.concat([\n",
    "      tf.expand_dims(initial_diag, axis=0),\n",
    "      tf.scan(step,\n",
    "        (tf.transpose(truth_diag, perm=[1, 0, 2]),\n",
    "        tf.transpose(blank_diag, perm=[1, 0, 2])),\n",
    "        initial_diag\n",
    "      )\n",
    "    ], axis=0)\n",
    "\n",
    "    beta_diag = tf.transpose(beta_diag, [1, 0, 2])\n",
    "\n",
    "    beta = tf.linalg.diag_part(beta_diag, k=(-max_label_lengths + 1, 0))\n",
    "    beta = tf.reshape(beta, (tf.shape(truth_log_pr)[0], tf.shape(truth_log_pr)[2], tf.shape(truth_log_pr)[1]))\n",
    "    beta = tf.transpose(beta, [0, 2, 1])\n",
    "    beta = tf.reverse(beta, axis=[1, 2])\n",
    "    return beta\n",
    "\n",
    "  alpha = get_alpha(truth_log_pr, blank_log_pr)\n",
    "  beta = get_beta(truth_log_pr, blank_log_pr)\n",
    "\n",
    "  total_log_pr = beta[:, 0, 0]\n",
    "\n",
    "  def grad (upstream):\n",
    "    blank_grads = tf.exp(\n",
    "      alpha + tf.pad(beta[:, 1:, :], [[0, 0], [1, 0], [0, 0]], constant_values=LOG_0)\n",
    "      - tf.reshape(total_log_pr, shape=(tf.shape(total_log_pr)[0], 1, 1))\n",
    "    )\n",
    "    truth_grads = tf.exp(\n",
    "      alpha + tf.pad(beta[:, :, 1: ], [[0, 0], [0, 0], [1, 0]], constant_values=LOG_0)\n",
    "      - tf.reshape(total_log_pr, shape=(tf.shape(total_log_pr)[0], 1, 1))\n",
    "    )\n",
    "    blank_one_hot = tf.one_hot(tf.zeros_like(labels, dtype=tf.int32), tf.shape(log_pr)[-1], dtype=tf.float32)\n",
    "    blank_one_hot = tf.expand_dims(blank_one_hot, axis=1)\n",
    "    blank_one_hot = tf.repeat(blank_one_hot, tf.shape(log_pr)[1], axis=1)\n",
    "    truth_one_hot = tf.one_hot(labels, tf.shape(log_pr)[-1], dtype=tf.float32)\n",
    "    truth_one_hot = tf.expand_dims(truth_one_hot, axis=1)\n",
    "    truth_one_hot = tf.repeat(truth_one_hot, tf.shape(log_pr)[1], axis=1)\n",
    "    return (\n",
    "      [tf.reshape(-upstream, shape=(tf.shape(upstream)[0], 1, 1)) * (tf.expand_dims(blank_grads, axis=-1) * blank_one_hot + tf.expand_dims(truth_grads, axis=-1) * truth_one_hot)] +\n",
    "      [None] * 3\n",
    "    )\n",
    "\n",
    "  return -total_log_pr, grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 256\n",
    "UNITS = 1024\n",
    "CODER_OUTPUT_DIM = 512\n",
    "JOINT_NET_INNER_DIM = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder (tf.keras.Model):\n",
    "  def __init__ (self, units, output_dim):\n",
    "    super(Encoder, self).__init__(self)\n",
    "    self.gru = tf.keras.layers.GRU(units, recurrent_initializer='glorot_uniform', return_sequences=True, return_state=True)\n",
    "    self.fc = tf.keras.layers.Dense(output_dim)\n",
    "    \n",
    "  def call (self, inputs):\n",
    "    rnn_outputs, hidden_states = self.gru(inputs)\n",
    "    outputs = self.fc(rnn_outputs)\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder (tf.keras.Model):\n",
    "  def __init__ (self, vocab_size, embedding_dim, units, output_dim):\n",
    "    super(Decoder, self).__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(units, recurrent_initializer='glorot_uniform', return_sequences=True, return_state=True)\n",
    "    self.fc = tf.keras.layers.Dense(output_dim)\n",
    "\n",
    "  def call (self, inputs):\n",
    "    embedded = self.embedding(inputs)\n",
    "    outputs, hidden_states = self.gru(embedded)\n",
    "    outputs = self.fc(outputs)\n",
    "    return outputs, hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointNet (tf.keras.Model):\n",
    "  def __init__ (self, inner_dim, vocab_size):\n",
    "    super(JointNet, self).__init__()\n",
    "    self.forward_layer = tf.keras.layers.Dense(inner_dim, activation='tanh')\n",
    "    self.project_layer = tf.keras.layers.Dense(vocab_size)\n",
    "  \n",
    "  def call (self, inputs):\n",
    "    enc_outputs, dec_outputs = inputs\n",
    "    joint_inputs = tf.expand_dims(enc_outputs, axis=2) + tf.expand_dims(dec_outputs, axis=1)\n",
    "    outputs = self.forward_layer(joint_inputs)\n",
    "    outputs = self.project_layer(outputs)\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transducer (tf.keras.Model):\n",
    "  def __init__ (self, encoder, decoder, joint_net):\n",
    "    super(Transducer, self).__init__()\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "    self.joint_net = joint_net\n",
    "\n",
    "  def __init__ (self, embedding_dim, units, coder_output_dim, joint_net_inner_dim, vocab_size):\n",
    "    super(Transducer, self).__init__()\n",
    "    self.encoder = Encoder(units, coder_output_dim)\n",
    "    self.decoder = Decoder(vocab_size, embedding_dim, units, coder_output_dim)\n",
    "    self.joint_net = JointNet(joint_net_inner_dim, vocab_size)\n",
    "    self.vocab_size = vocab_size\n",
    "  \n",
    "  def call (self, inputs):\n",
    "    enc_inputs, dec_inputs = inputs\n",
    "    enc_outputs = self.encoder(enc_inputs)\n",
    "    dec_outputs, dec_states = self.decoder(dec_inputs)\n",
    "    outputs = self.joint_net([enc_outputs, dec_outputs])\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "class Trainer:\n",
    "  def __init__ (self, transducer, optimizer, batch_size, dataset_extractor, save_time_delta):\n",
    "    self.transducer = transducer\n",
    "    self.optimizer = optimizer\n",
    "    self.batch_size = batch_size\n",
    "    self.dataset_extractor = dataset_extractor\n",
    "    self.save_time_delta = save_time_delta\n",
    "\n",
    "  def train_step (self):\n",
    "    datas = self.dataset_extractor.get_value()\n",
    "    acc_grads = [tf.zeros_like(var) for var in self.transducer.trainable_variables]\n",
    "    acc_loss = 0\n",
    "    for i in range(0, len(datas), self.batch_size):\n",
    "      batch_datas = datas[i: i + self.batch_size]\n",
    "      label_lengths = self.get_label_lengths(batch_datas)\n",
    "      label_lengths = tf.convert_to_tensor(label_lengths, dtype=tf.int32)\n",
    "      labels = pad_sequences(list(map(lambda x: x[1], batch_datas)), maxlen=self.get_labels_maxlen(batch_datas), truncating='post', padding='post')\n",
    "      labels = tf.pad(labels, [[0, 0], [1, 0]], constant_values=0)\n",
    "      labels += 1\n",
    "      spectrograms = [x[0] for x in batch_datas]\n",
    "      spectrogram_lengths = self.get_spectrogram_lengths(spectrograms)\n",
    "      spectrogram_lengths = tf.convert_to_tensor(spectrogram_lengths, dtype=tf.int32)\n",
    "      spectrograms = tf.cast(pad_sequences(spectrograms, maxlen=self.get_spectrograms_maxlen(spectrograms), truncating='post', padding='post'), dtype=tf.float32)\n",
    "      with tf.GradientTape() as tape:\n",
    "        logits = self.transducer([spectrograms, labels])\n",
    "        loss = rnnt_loss(\n",
    "          logits,\n",
    "          labels,\n",
    "          spectrogram_lengths,\n",
    "          label_lengths\n",
    "        )\n",
    "      acc_loss += loss.numpy()\n",
    "      grads = tape.gradient(loss, self.transducer.trainable_variables)\n",
    "      acc_grads = [acc_grad + grad for acc_grad, grad in zip(acc_grads, grads)]\n",
    "    acc_grads = [acc_grad / self.dataset_extractor.step_size for acc_grad in acc_grads]\n",
    "    self.optimizer.apply_gradients(zip(acc_grads, self.transducer.trainable_variables))\n",
    "    \n",
    "    return acc_loss / self.dataset_extractor.step_size\n",
    "\n",
    "  def train (self):\n",
    "    cnt = 0\n",
    "    prev_save_time = 0\n",
    "    cur_block_size = 0\n",
    "    while not self.dataset_extractor.check_end():\n",
    "      loss = self.train_step()\n",
    "      cnt += self.dataset_extractor.step_size\n",
    "      print('cnt: {}'.format(cnt))\n",
    "      print('loss: {}'.format(loss))\n",
    "      if cnt >= prev_save_time + self.save_time_delta:\n",
    "        self.transducer.save_weights('./checkpoints/no{}/ckpt'.format(cnt))\n",
    "        prev_save_time = cnt\n",
    "      self.dataset_extractor = self.dataset_extractor.get_next_extractor()\n",
    "\n",
    "  def get_labels_maxlen (self, batch_datas):\n",
    "    ret = 0\n",
    "    for file_label_vector_pair in batch_datas:\n",
    "      ret = max(ret, len(file_label_vector_pair[1]))\n",
    "    return ret\n",
    "  \n",
    "  def get_label_lengths (self, batch_datas):\n",
    "    ret = list(map(lambda x: len(x[1]), batch_datas))\n",
    "    return ret\n",
    "  \n",
    "  def get_spectrograms_maxlen (self, spectrograms):\n",
    "    ret = 0\n",
    "    for spectrogram in spectrograms:\n",
    "      ret = max(ret, spectrogram.shape[0])\n",
    "    return ret\n",
    "\n",
    "  def get_spectrogram_lengths (self, spectrograms):\n",
    "    ret = list(map(lambda x: x.shape[0], spectrograms))\n",
    "    return ret\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = dataset.phoneme.VOCAB_SIZE\n",
    "BATCH_SIZE = 1\n",
    "BATCH_CNT = 100\n",
    "SAVE_TIME_DELTA = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transducer_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_13 (Encoder)        multiple                  4069888   \n",
      "                                                                 \n",
      " decoder_13 (Decoder)        multiple                  4477440   \n",
      "                                                                 \n",
      " joint_net_13 (JointNet)     multiple                  582712    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,130,040\n",
      "Trainable params: 9,130,040\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17956\\3161375974.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/gpu:0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m   \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17956\\2887542477.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mcur_block_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_extractor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m       \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m       \u001b[0mcnt\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_extractor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m       \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cnt: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17956\\2887542477.py\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     29\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransducer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mspectrograms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         loss = rnnt_loss(\n\u001b[0m\u001b[0;32m     32\u001b[0m           \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m           \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17956\\1872226888.py\u001b[0m in \u001b[0;36mrnnt_loss\u001b[1;34m(logits, labels, time_lengths, label_lengths)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mrnnt_loss\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m   \u001b[0mlog_pr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m   \u001b[0mpr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpr_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_pr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m   \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\custom_gradient.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *a, **k)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\custom_gradient.py\u001b[0m in \u001b[0;36mdecorated\u001b[1;34m(wrapped, args, kwargs)\u001b[0m\n\u001b[0;32m    292\u001b[0m     \u001b[1;34m\"\"\"Decorated function with custom gradient.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0m_eager_mode_decorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_graph_mode_decorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\custom_gradient.py\u001b[0m in \u001b[0;36m_eager_mode_decorator\u001b[1;34m(f, args, kwargs)\u001b[0m\n\u001b[0;32m    529\u001b[0m   \u001b[1;34m\"\"\"Implement custom gradient decorator for eager mode.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0mtape_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariableWatcher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mvariable_watcher\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m     \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m   \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m   \u001b[0mall_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17956\\1872226888.py\u001b[0m in \u001b[0;36mpr_loss\u001b[1;34m(log_pr, labels, time_lengths, label_lengths)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m   \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_alpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruth_log_pr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblank_log_pr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m   \u001b[0mbeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_beta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruth_log_pr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblank_log_pr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m   \u001b[0mtotal_log_pr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17956\\1872226888.py\u001b[0m in \u001b[0;36mget_beta\u001b[1;34m(truth_log_pr, blank_log_pr)\u001b[0m\n\u001b[0;32m    135\u001b[0m     beta_diag = tf.concat([\n\u001b[0;32m    136\u001b[0m       \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial_diag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m       tf.scan(step,\n\u001b[0m\u001b[0;32m    138\u001b[0m         (tf.transpose(truth_diag, perm=[1, 0, 2]),\n\u001b[0;32m    139\u001b[0m         tf.transpose(blank_diag, perm=[1, 0, 2])),\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1176\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1177\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1178\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    627\u001b[0m                   \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'in a future version'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m                   if date is None else ('after %s' % date), instructions)\n\u001b[1;32m--> 629\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    630\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m     doc = _add_deprecated_arg_value_notice_to_docstring(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\functional_ops.py\u001b[0m in \u001b[0;36mscan_v2\u001b[1;34m(fn, elems, initializer, parallel_iterations, back_prop, swap_memory, infer_shape, reverse, name)\u001b[0m\n\u001b[0;32m    803\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    804\u001b[0m   \"\"\"\n\u001b[1;32m--> 805\u001b[1;33m   return scan(\n\u001b[0m\u001b[0;32m    806\u001b[0m       \u001b[0mfn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    807\u001b[0m       \u001b[0melems\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0melems\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1176\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1177\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1178\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\functional_ops.py\u001b[0m in \u001b[0;36mscan\u001b[1;34m(fn, elems, initializer, parallel_iterations, back_prop, swap_memory, infer_shape, reverse, name)\u001b[0m\n\u001b[0;32m    661\u001b[0m       \u001b[0minitial_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    662\u001b[0m       \u001b[0mcondition\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_2\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 663\u001b[1;33m     _, _, r_a = control_flow_ops.while_loop(\n\u001b[0m\u001b[0;32m    664\u001b[0m         \u001b[0mcondition\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m         \u001b[0mcompute\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minitial_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_flat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccs_ta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[1;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[0;32m   2760\u001b[0m                                               list(loop_vars))\n\u001b[0;32m   2761\u001b[0m       \u001b[1;32mwhile\u001b[0m \u001b[0mcond\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2762\u001b[1;33m         \u001b[0mloop_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2763\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtry_to_pack\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_basetuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2764\u001b[0m           \u001b[0mpacked\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(i, lv)\u001b[0m\n\u001b[0;32m   2751\u001b[0m         cond = lambda i, lv: (  # pylint: disable=g-long-lambda\n\u001b[0;32m   2752\u001b[0m             math_ops.logical_and(i < maximum_iterations, orig_cond(*lv)))\n\u001b[1;32m-> 2753\u001b[1;33m         \u001b[0mbody\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlv\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morig_body\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2754\u001b[0m       \u001b[0mtry_to_pack\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2755\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\functional_ops.py\u001b[0m in \u001b[0;36mcompute\u001b[1;34m(i, a_flat, tas)\u001b[0m\n\u001b[0;32m    644\u001b[0m       \u001b[0mpacked_elems\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_pack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0melem_ta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0melem_ta\u001b[0m \u001b[1;32min\u001b[0m \u001b[0melems_ta\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m       \u001b[0mpacked_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_pack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_flat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 646\u001b[1;33m       \u001b[0ma_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpacked_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpacked_elems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    647\u001b[0m       nest.assert_same_structure(elems if initializer is None else initializer,\n\u001b[0;32m    648\u001b[0m                                  a_out)\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17956\\1872226888.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(a, x)\u001b[0m\n\u001b[0;32m    125\u001b[0m       \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m       return (\n\u001b[1;32m--> 127\u001b[1;33m         tf.reduce_logsumexp(\n\u001b[0m\u001b[0;32m    128\u001b[0m           tf.stack([\n\u001b[0;32m    129\u001b[0m             \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLOG_0\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1176\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1177\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1178\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36m_slice_helper\u001b[1;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[0;32m   1094\u001b[0m       \u001b[0mvar_empty\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1095\u001b[0m       \u001b[0mpacked_begin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpacked_end\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpacked_strides\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvar_empty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1096\u001b[1;33m     return strided_slice(\n\u001b[0m\u001b[0;32m   1097\u001b[0m         \u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m         \u001b[0mpacked_begin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1176\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1177\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1178\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[1;34m(input_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, var, name)\u001b[0m\n\u001b[0;32m   1267\u001b[0m     \u001b[0mstrides\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1269\u001b[1;33m   op = gen_array_ops.strided_slice(\n\u001b[0m\u001b[0;32m   1270\u001b[0m       \u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m       \u001b[0mbegin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[1;34m(input, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[0;32m  10669\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10670\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 10671\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m  10672\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"StridedSlice\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbegin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"begin_mask\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10673\u001b[0m         \u001b[0mbegin_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"end_mask\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ellipsis_mask\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mellipsis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# encoder = Encoder(UNITS, CODER_OUTPUT_DIM)\n",
    "# decoder = Decoder(dataset.phoneme.VOCAB_SIZE, EMBEDDING_DIM, UNITS, CODER_OUTPUT_DIM)\n",
    "# joint_net = JointNet(1024, dataset.phoneme.VOCAB_SIZE)\n",
    "# transducer = Transducer(encoder, decoder, joint_net)\n",
    "\n",
    "transducer = Transducer(EMBEDDING_DIM, UNITS, CODER_OUTPUT_DIM, JOINT_NET_INNER_DIM, dataset.phoneme.VOCAB_SIZE)\n",
    "\n",
    "transducer([\n",
    "  tf.keras.Input(shape=(None, 128), dtype=tf.float32),\n",
    "  tf.keras.Input(shape=(None, ), dtype=tf.int32)\n",
    "])\n",
    "\n",
    "transducer.summary()\n",
    "\n",
    "# transducer = Transducer(EMBEDDING_DIM, UNITS, CODER_OUTPUT_DIM, JOINT_NET_INNER_DIM, VOCAB_SIZE)\n",
    "# transducer.load_weights('./checkpoints/no5000/ckpt')\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001)\n",
    "trainer = Trainer(transducer, optimizer, BATCH_SIZE, extractor, SAVE_TIME_DELTA)\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "  trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec62f2ec7c7d49224ec335e3ef472b8ef537dc7949933ed74784e8710c2a92fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
