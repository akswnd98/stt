{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o/ 근데 칠십 퍼센트가 커 보이긴 하는데 이백 벌다 백 사십 벌면 빡셀걸? b/\n",
      "근데 삼 학년 때 까지는 국가장학금 바+ 받으면서 다녔던 건가?\n"
     ]
    }
   ],
   "source": [
    "test1 = \"o/ 근데 (70%)/(칠십 퍼센트)가 커 보이긴 하는데 (200)/(이백) 벌다 (140)/(백 사십) 벌면 빡셀걸? b/\"\n",
    "test2 = \"근데 (3학년)/(삼 학년) 때 까지는 국가장학금 바+ 받으면서 다녔던 건가?\"\n",
    "\n",
    "def bracket_filter(sentence):\n",
    "  new_sentence = str()\n",
    "  flag = False\n",
    "  \n",
    "  for ch in sentence:\n",
    "    if ch == '(' and flag == False:\n",
    "      flag = True\n",
    "      continue\n",
    "    if ch == '(' and flag == True:\n",
    "      flag = False\n",
    "      continue\n",
    "    if ch != ')' and flag == False:\n",
    "      new_sentence += ch\n",
    "  return new_sentence\n",
    "\n",
    "print(bracket_filter(test1))\n",
    "print(bracket_filter(test2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "근데 칠십 퍼센트가 커 보이긴 하는데 이백 벌다 백 사십 벌면 빡셀걸?\n",
      "c샾 배워봤어?\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "test1 = \"o/ 근데 칠십 퍼센트가 커 보이긴 하는데 이백 벌다 백 사십 벌면 빡셀걸? b/\"\n",
    "test2 = \"c# 배워봤어?\"\n",
    "\n",
    "def special_filter(sentence):\n",
    "  SENTENCE_MARK = ['?', '!']\n",
    "  NOISE = ['o', 'n', 'u', 'b', 'l']\n",
    "  EXCEPT = ['/', '+', '*', '-', '@', '$', '^', '&', '[', ']', '=', ':', ';', '.', ',']\n",
    "  \n",
    "  new_sentence = str()\n",
    "  for idx, ch in enumerate(sentence):\n",
    "    if ch not in SENTENCE_MARK:\n",
    "      # o/, n/ 등 처리\n",
    "      if idx + 1 < len(sentence) and ch in NOISE and sentence[idx+1] == '/': \n",
    "        continue \n",
    "\n",
    "    if ch == '#': \n",
    "      new_sentence += '샾'\n",
    "\n",
    "    elif ch not in EXCEPT: \n",
    "      new_sentence += ch\n",
    "\n",
    "  pattern = re.compile(r'\\s\\s+')\n",
    "  new_sentence = re.sub(pattern, ' ', new_sentence.strip())\n",
    "  return new_sentence\n",
    "\n",
    "print(special_filter(test1))\n",
    "print(special_filter(test2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아 나 쓰리 DS 갖고 싶다 쓰리 DS\n"
     ]
    }
   ],
   "source": [
    "# test = \"o/ 근데 (70%)/(칠십 퍼센트)가 커 보이긴 하는데 (200)/(이백) 벌다 (140)/(백 사십) 벌면 빡셀걸? b/\"\n",
    "test = 'b/ 아 나 (3DS)/(쓰리 DS) 갖고 싶다. (3DS)/(쓰리 DS)'\n",
    "\n",
    "def sentence_filter(raw_sentence):\n",
    "  return special_filter(bracket_filter(raw_sentence))\n",
    "\n",
    "print(sentence_filter(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosung = (\"ㄱ\", \"ㄲ\", \"ㄴ\", \"ㄷ\", \"ㄸ\", \"ㄹ\", \"ㅁ\", \"ㅂ\", \"ㅃ\", \"ㅅ\", \"ㅆ\", \"ㅇ\", \"ㅈ\", \"ㅉ\", \"ㅊ\", \"ㅋ\", \"ㅌ\", \"ㅍ\", \"ㅎ\")\n",
    "\n",
    "jungsung = (\"ㅏ\", \"ㅐ\", \"ㅑ\", \"ㅒ\", \"ㅓ\", \"ㅔ\", \"ㅕ\", \"ㅖ\", \"ㅗ\", \"ㅘ\", \"ㅙ\", \"ㅚ\", \"ㅛ\", \"ㅜ\", \"ㅝ\", \"ㅞ\", \"ㅟ\", \"ㅠ\", \"ㅡ\", \"ㅢ\", \"ㅣ\")\n",
    "\n",
    "jongsung = (\"\", \"ㄱ\", \"ㄲ\", \"ㄳ\", \"ㄴ\", \"ㄵ\", \"ㄶ\", \"ㄷ\", \"ㄹ\", \"ㄺ\", \"ㄻ\", \"ㄼ\", \"ㄽ\", \"ㄾ\", \"ㄿ\", \"ㅀ\", \"ㅁ\", \"ㅂ\", \"ㅄ\", \"ㅅ\", \"ㅆ\", \"ㅇ\", \"ㅈ\", \"ㅊ\", \"ㅋ\", \"ㅌ\", \"ㅍ\", \"ㅎ\")\n",
    "\n",
    "def isHangeul(one_character):\n",
    "    return 0xAC00 <= ord(one_character[:1]) <= 0xD7A3\n",
    "\n",
    "def hangeulExplode(one_hangeul):\n",
    "    a = one_hangeul[:1]\n",
    "    if isHangeul(a) != True:\n",
    "        return False\n",
    "    b = ord(a) - 0xAC00\n",
    "    cho = b // (21*28)\n",
    "    jung = b % (21*28) // 28\n",
    "    jong = b % 28\n",
    "    if jong == 0:\n",
    "        return (chosung[cho], jungsung[jung])\n",
    "    else:\n",
    "        return (chosung[cho], jungsung[jung], jongsung[jong])\n",
    "\n",
    "def hangeulJoin(inputlist):\n",
    "    result = \"\"\n",
    "    cho, jung, jong = 0, 0, 0\n",
    "    inputlist.insert(0, \"\")\n",
    "    while len(inputlist) > 1:\n",
    "        if inputlist[-1] in jongsung:\n",
    "            if inputlist[-2] in jungsung:\n",
    "                jong = jongsung.index(inputlist.pop())\n",
    "            \n",
    "            else:\n",
    "                result += inputlist.pop()\n",
    "        elif inputlist[-1] in jungsung:\n",
    "            if inputlist[-2] in chosung:\n",
    "                jung = jungsung.index(inputlist.pop())\n",
    "                cho = chosung.index(inputlist.pop())\n",
    "                result += chr(0xAC00 + ((cho*21)+jung)*28+jong)\n",
    "                cho, jung, jong = 0, 0, 0\n",
    "            else:\n",
    "                result += inputlist.pop()\n",
    "\n",
    "        else:\n",
    "            result += inputlist.pop()\n",
    "    else:\n",
    "        return result[::-1]\n",
    "\n",
    "def pureosseugi(inputtext):\n",
    "    result = \"\"\n",
    "    for i in inputtext:\n",
    "        if isHangeul(i) == True:\n",
    "            for j in hangeulExplode(i):\n",
    "                result += j\n",
    "        else:\n",
    "            result += i\n",
    "    \n",
    "    return result\n",
    "\n",
    "def moasseugi(inputtext):\n",
    "    t1 = []\n",
    "    for i in inputtext:\n",
    "        t1.append(i)\n",
    "\n",
    "    return hangeulJoin(t1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "\n",
    "class Phoneme:\n",
    "  def __init__ (self):\n",
    "    self.BLANK_INDEX = 0\n",
    "    self.START_INDEX = 1\n",
    "    self.END_INDEX = 2\n",
    "    self.SPACE_INDEX = 3\n",
    "    self.OOV_INDEX = 4\n",
    "\n",
    "class PhonemeGenerator (Phoneme):\n",
    "  def __init__ (self, labels):\n",
    "    super(PhonemeGenerator, self).__init__()\n",
    "    self.phonemes = ['<BLANK>', '<START>', '<END>', ' ']\n",
    "\n",
    "    texts = []\n",
    "    for label in labels:\n",
    "      for phoneme in label:\n",
    "        if not isHangeul(phoneme) and not phoneme == ' ' and not phoneme == '?':\n",
    "          break\n",
    "        for pure in pureosseugi(phoneme):\n",
    "          texts.append(pure)\n",
    "\n",
    "    tokenizer = Tokenizer(filters='', lower=False, oov_token='<OOV>')\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "\n",
    "    for i in range(1, len(tokenizer.index_word) + 1):\n",
    "      self.phonemes.append(tokenizer.index_word[i])\n",
    "\n",
    "    self.phoneme_index = {}\n",
    "    for i, phoneme in enumerate(self.phonemes):\n",
    "      self.phoneme_index[phoneme] = i\n",
    "    \n",
    "    self.VOCAB_SIZE = len(self.phonemes)\n",
    "  \n",
    "  def save (self, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "      pickle.dump((self.phonemes, self.phoneme_index), f)\n",
    "\n",
    "\n",
    "class PhonemeLoader (Phoneme):\n",
    "  def __init__ (self, filename):\n",
    "    super(PhonemeLoader, self).__init__()\n",
    "    with open(filename, 'rb') as f:\n",
    "      self.phonemes, self.phoneme_index = pickle.load(f)\n",
    "\n",
    "    self.VOCAB_SIZE = len(self.phonemes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akswnd98\\anaconda3\\lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import copy\n",
    "\n",
    "class Dataset:\n",
    "  def get_data (self, idx):\n",
    "    return (\n",
    "      self.extract_spectrogram(self.file_phoneme_vector_pairs[idx][0]),\n",
    "      [0] + self.file_phoneme_vector_pairs[idx][1]\n",
    "    )\n",
    "  \n",
    "  def get_len (self):\n",
    "    return len(self.file_phoneme_vector_pairs)\n",
    "\n",
    "  def extract_spectrogram (self, filename):\n",
    "    waveform = np.memmap(filename, dtype='h', mode='r')\n",
    "\n",
    "    sr = 16000\n",
    "    st = 1 / sr\n",
    "\n",
    "    t = np.linspace(0, waveform.shape[0] * st, waveform.shape[0])\n",
    "\n",
    "    S = librosa.feature.melspectrogram(y=np.float32(waveform), n_fft=320, hop_length=160, window='hamming', sr=sr, n_mels=128)\n",
    "    log_S = librosa.power_to_db(S, ref=np.max)\n",
    "    log_S = np.transpose(log_S)\n",
    "\n",
    "    return log_S\n",
    "\n",
    "class ShufflableDataset (Dataset):\n",
    "  def __init__ (self, phoneme, file_phoneme_vector_pairs, original_file_phoneme_vector_pairs):\n",
    "    super(ShufflableDataset, self).__init__()\n",
    "    self.phoneme = phoneme\n",
    "    self.original_file_phoneme_vector_pairs = original_file_phoneme_vector_pairs\n",
    "    self.file_phoneme_vector_pairs = file_phoneme_vector_pairs\n",
    "  \n",
    "  def shuffle (self):\n",
    "    self.file_phoneme_vector_pairs = copy.deepcopy(self.original_file_phoneme_vector_pairs)\n",
    "    random.shuffle(self.file_phoneme_vector_pairs)\n",
    "  \n",
    "  def save (self, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "      pickle.dump((self.phoneme, self.file_phoneme_vector_pairs, self.original_file_phoneme_vector_pairs), f)\n",
    "\n",
    "class ShufflableDatasetGenerator:\n",
    "  @staticmethod\n",
    "  def generate ():\n",
    "    with open('./ko-audio-dataset/sentence-script/KsponSpeech_scripts/train.trn', 'r', encoding='utf-8') as file:\n",
    "      lines = file.readlines()\n",
    "    def process_line (line):\n",
    "      line = line.split(' :: ')\n",
    "      line[0] = os.path.join('./ko-audio-dataset/audio/', line[0])\n",
    "      line[1] = sentence_filter(line[1])\n",
    "      return line\n",
    "    file_label_pairs = list(map(process_line, lines))\n",
    "    file_label_pairs = list(filter(lambda pair: not ShufflableDatasetGenerator.check_skip_label(pair[1]), file_label_pairs))\n",
    "    phoneme_generator = PhonemeGenerator([label for file, label in file_label_pairs])\n",
    "    file_phoneme_pairs = list(map(lambda pair: (pair[0], pureosseugi(pair[1])), file_label_pairs))\n",
    "    original_file_phoneme_vector_pairs = [[pair[0], [phoneme_generator.phoneme_index[phoneme] for phoneme in pair[1]]] for pair in file_phoneme_pairs]\n",
    "    file_phoneme_vector_pairs = copy.deepcopy(original_file_phoneme_vector_pairs)\n",
    "    return ShufflableDataset(phoneme_generator, file_phoneme_vector_pairs, original_file_phoneme_vector_pairs)\n",
    "\n",
    "  @staticmethod\n",
    "  def check_skip_hangul (hangul):\n",
    "    return not isHangeul(hangul) and not hangul == ' ' and not hangul == '?'\n",
    "\n",
    "  @staticmethod\n",
    "  def check_skip_label (label):\n",
    "    for hangul in label:\n",
    "      if ShufflableDatasetGenerator.check_skip_hangul(hangul):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "class ShufflableDatasetLoader:\n",
    "  @staticmethod\n",
    "  def load (filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "      phoneme, file_phoneme_vector_pairs, original_file_phoneme_vector_pairs = pickle.load(f)\n",
    "    return ShufflableDataset(phoneme, file_phoneme_vector_pairs, original_file_phoneme_vector_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetSizeLimitingIterator:\n",
    "  def __init__ (self, dataset, time_limit, label_limit, idx):\n",
    "    self.dataset = dataset\n",
    "    self.time_limit = time_limit\n",
    "    self.label_limit = label_limit\n",
    "    self.idx = idx\n",
    "    while self.idx < self.dataset.get_len():\n",
    "      data = self.dataset.get_data(self.idx)\n",
    "      if data[0].shape[0] <= self.time_limit and len(data[1]) <= self.label_limit:\n",
    "        break\n",
    "      self.idx += 1\n",
    "\n",
    "  def next (self):\n",
    "    idx = self.idx + 1\n",
    "    while idx < self.dataset.get_len():\n",
    "      data = self.dataset.get_data(idx)\n",
    "      if data[0].shape[0] <= self.time_limit and len(data[1]) <= self.label_limit:\n",
    "        break\n",
    "      idx += 1\n",
    "    return DatasetSizeLimitingIterator(self.dataset, self.time_limit, self.label_limit, idx)\n",
    "  \n",
    "  def check_end (self):\n",
    "    return self.idx >= self.dataset.get_len()\n",
    "\n",
    "  def get_value (self):\n",
    "    return self.dataset.get_data(self.idx)\n",
    "\n",
    "  def save (self, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "      pickle.dump((self.time_limit, self.label_limit,self.idx), f)\n",
    "\n",
    "class IteratorLoader:\n",
    "  @staticmethod\n",
    "  def load (dataset, filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "      time_limit, label_limit, idx = pickle.load(f)\n",
    "    return DatasetSizeLimitingIterator(dataset, time_limit, label_limit, idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class DatasetStepExtractorLoader:\n",
    "  @staticmethod\n",
    "  def load (path):\n",
    "    dataset = ShufflableDatasetLoader.load(os.path.join(path, 'dataset.pickle'))\n",
    "    return SavableDatasetStepExtractor(\n",
    "      dataset,\n",
    "      pickle.load(open(os.path.join(path, 'step_size.pickle'), 'rb')),\n",
    "      IteratorLoader.load(dataset, os.path.join(path, 'iterator.pickle'))\n",
    "    )\n",
    "\n",
    "class DatasetStepExtractor:\n",
    "  def __init__ (self, dataset, step_size, iterator):\n",
    "    self.dataset = dataset\n",
    "    self.step_size = step_size\n",
    "    self.value = []\n",
    "    self.iterator = iterator\n",
    "    while not iterator.check_end() and len(self.value) < self.step_size:\n",
    "      self.value.append(iterator.get_value())\n",
    "      iterator = iterator.next()\n",
    "    self.is_end = len(self.value) < self.step_size\n",
    "    self.next_iterator = iterator\n",
    "  \n",
    "  def check_end (self):\n",
    "    return self.is_end\n",
    "  \n",
    "  def get_value (self):\n",
    "    return self.value\n",
    "  \n",
    "  def get_next_extractor (self):\n",
    "    if self.is_end:\n",
    "      return None\n",
    "    else:\n",
    "      return SavableDatasetStepExtractor(self.dataset, self.step_size, self.next_iterator)\n",
    "\n",
    "class SavableDatasetStepExtractor (DatasetStepExtractor):\n",
    "  def save (self, saver):\n",
    "    saver.save(self.dataset, self.step_size, self.iterator)\n",
    "\n",
    "class DatasetStepExtractorSaver:\n",
    "  def __init__ (self, path):\n",
    "    self.path = path\n",
    "  \n",
    "  def save (self, dataset, step_size, iterator):\n",
    "    os.makedirs(self.path, exist_ok=True)\n",
    "\n",
    "class IteratorSaver (DatasetStepExtractorSaver):\n",
    "  def save (self, dataset, step_size, iterator):\n",
    "    super().save(dataset, step_size, iterator)\n",
    "    iterator.save(os.path.join(self.path, 'iterator.pickle'))\n",
    "  \n",
    "class DatasetSaver (DatasetStepExtractorSaver):\n",
    "  def save (self, dataset, step_size, iterator):\n",
    "    super().save(dataset, step_size, iterator)\n",
    "    dataset.save(os.path.join(self.path, 'dataset.pickle'))\n",
    "\n",
    "class StepSizeSaver (DatasetStepExtractorSaver):\n",
    "  def save (self, dataset, step_size, iterator):\n",
    "    super().save(dataset, step_size, iterator)\n",
    "    with open(os.path.join(self.path, 'step_size.pickle'), 'wb') as f:\n",
    "      pickle.dump(step_size, f)\n",
    "\n",
    "class ExtractorSaver (DatasetStepExtractorSaver):\n",
    "  def save (self, dataset, step_size, iterator):\n",
    "    super().save(dataset, step_size, iterator)\n",
    "    iterator.save(os.path.join(self.path, 'iterator.pickle'))\n",
    "    dataset.save(os.path.join(self.path, 'dataset.pickle'))\n",
    "    with open(os.path.join(self.path, 'step_size.pickle'), 'wb') as f:\n",
    "      pickle.dump(step_size, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ 여기 까지 dataset 부분 ###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_to_zero (tensor):\n",
    "  return tf.where(tf.math.is_nan(tensor), tf.zeros_like(tensor), tensor)\n",
    "\n",
    "def rnnt_loss (logits, labels, time_lengths, label_lengths):\n",
    "  log_pr = tf.math.log_softmax(logits, axis=-1)\n",
    "  pr = pr_loss(log_pr, labels, time_lengths, label_lengths)\n",
    "  ret = tf.reduce_sum(pr)\n",
    "  return ret\n",
    "\n",
    "@tf.custom_gradient\n",
    "def pr_loss (log_pr, labels, time_lengths, label_lengths):\n",
    "  LOG_0 = float('-inf')\n",
    "  batch_size = tf.shape(log_pr)[0]\n",
    "  max_time_lengths = tf.shape(log_pr)[1]\n",
    "  max_label_lengths = tf.shape(log_pr)[2]\n",
    "\n",
    "  def get_truth_log_pr (log_pr, labels):\n",
    "    labels_one_hot = tf.one_hot(labels, tf.shape(log_pr)[-1], axis=-1, dtype=tf.float32)\n",
    "    labels_one_hot = labels_one_hot[:, 1: ]\n",
    "    labels_one_hot = tf.expand_dims(labels_one_hot, axis=1)\n",
    "    labels_one_hot = tf.repeat(labels_one_hot, tf.shape(log_pr)[1], axis=1)\n",
    "    ret = tf.reduce_sum(log_pr[:, :, : -1, :] * labels_one_hot, axis=-1)\n",
    "    ret = tf.concat([\n",
    "      ret,\n",
    "      LOG_0 * tf.ones((tf.shape(log_pr)[0], tf.shape(log_pr)[1], 1), dtype=tf.float32)\n",
    "    ], axis=-1)\n",
    "    return ret\n",
    "  \n",
    "  def get_blank_log_pr (log_pr):\n",
    "    return log_pr[:, :, :, 0]\n",
    "\n",
    "  truth_log_pr = get_truth_log_pr(log_pr, labels)\n",
    "  blank_log_pr = get_blank_log_pr(log_pr)\n",
    "\n",
    "  def get_alpha (truth_log_pr, blank_log_pr):\n",
    "    reversed_truth_log_pr = tf.reverse(truth_log_pr, axis=[-1])\n",
    "    padded_truth_log_pr = tf.pad(\n",
    "      reversed_truth_log_pr,\n",
    "      [[0, 0], [0, 0], [tf.shape(reversed_truth_log_pr)[-2] - 1, 0]],\n",
    "      constant_values=LOG_0\n",
    "    )\n",
    "    truth_diag = tf.linalg.diag_part(\n",
    "      padded_truth_log_pr,\n",
    "      k=(0, tf.shape(padded_truth_log_pr)[-1] - 1),\n",
    "      padding_value=LOG_0,\n",
    "      align='LEFT_RIGHT'\n",
    "    )\n",
    "    truth_diag = tf.transpose(truth_diag, perm=[1, 0, 2])\n",
    "\n",
    "    reversed_blank_log_pr = tf.reverse(blank_log_pr, axis=[-1])\n",
    "    padded_blank_log_pr = tf.pad(\n",
    "      reversed_blank_log_pr,\n",
    "      [[0, 0], [0, 0], [tf.shape(reversed_blank_log_pr)[-2] - 1, 0]],\n",
    "      constant_values=LOG_0\n",
    "    )\n",
    "    blank_diag = tf.linalg.diag_part(\n",
    "      padded_blank_log_pr,\n",
    "      k=(0, tf.shape(padded_blank_log_pr)[-1] - 1),\n",
    "      padding_value=LOG_0,\n",
    "      align='LEFT_RIGHT'\n",
    "    )\n",
    "    blank_diag = tf.concat([\n",
    "      LOG_0 * tf.ones((tf.shape(blank_diag)[0], tf.shape(blank_diag)[1], 1), dtype=tf.float32),\n",
    "      blank_diag[:, :, : -1]\n",
    "    ], axis=-1)\n",
    "    blank_diag = tf.transpose(blank_diag, perm=[1, 0, 2])\n",
    "\n",
    "    initial_diag = tf.concat([\n",
    "      tf.zeros((tf.shape(blank_diag)[1], 1), dtype=tf.float32),\n",
    "      LOG_0 * tf.ones((tf.shape(blank_diag)[1], tf.shape(blank_diag)[-1] - 1), dtype=tf.float32)\n",
    "    ], axis=-1)\n",
    "\n",
    "    def step (a, x):\n",
    "      t, b = x\n",
    "      return (\n",
    "        tf.reduce_logsumexp(\n",
    "          tf.stack([\n",
    "              a + t,\n",
    "              tf.concat([\n",
    "                LOG_0 * tf.ones((tf.shape(a)[0], 1), dtype=tf.float32),\n",
    "                a[:, : -1]\n",
    "              ], axis=-1) + b\n",
    "            ],\n",
    "            axis=0\n",
    "          ), axis=0\n",
    "        )\n",
    "      )\n",
    "    alpha_diag =  tf.concat([\n",
    "      tf.expand_dims(initial_diag, axis=0),\n",
    "      tf.scan(step, (truth_diag, blank_diag), initial_diag)\n",
    "    ], axis=0)\n",
    "    alpha_diag = tf.transpose(alpha_diag, perm=[1, 2, 0])\n",
    "    alpha = tf.linalg.diag_part(alpha_diag, k=(0, max_label_lengths - 1))\n",
    "    alpha = tf.reverse(alpha, axis=[-2])\n",
    "    alpha = tf.transpose(alpha, perm=[0, 2, 1])\n",
    "    return alpha\n",
    "\n",
    "  def get_beta (truth_log_pr, blank_log_pr):\n",
    "    reversed_truth_log_pr = tf.reverse(truth_log_pr, axis=[-1])\n",
    "    reversed_truth_log_pr = tf.concat([\n",
    "      LOG_0 * tf.ones((tf.shape(reversed_truth_log_pr)[0], tf.shape(reversed_truth_log_pr)[1], 1), dtype=tf.float32),\n",
    "      reversed_truth_log_pr[:, :, 1: ]\n",
    "    ], axis=-1)\n",
    "    padded_truth_log_pr = tf.pad(\n",
    "      reversed_truth_log_pr,\n",
    "      [[0, 0], [0, 0], [tf.shape(reversed_truth_log_pr)[-2] - 1, 0]],\n",
    "      constant_values=LOG_0\n",
    "    )\n",
    "    truth_diag = tf.linalg.diag_part(\n",
    "      padded_truth_log_pr,\n",
    "      k=(0, tf.shape(padded_truth_log_pr)[-1] - 1),\n",
    "      padding_value=LOG_0,\n",
    "      align='LEFT_RIGHT'\n",
    "    )\n",
    "    truth_diag = tf.transpose(truth_diag, perm=[1, 0, 2])\n",
    "    truth_diag = truth_diag[: -1]\n",
    "\n",
    "    reversed_blank_log_pr = tf.reverse(blank_log_pr, axis=[-1])\n",
    "    reversed_blank_log_pr = tf.concat([\n",
    "      reversed_blank_log_pr[:, : -1, :],\n",
    "      LOG_0 * tf.ones((tf.shape(reversed_blank_log_pr)[0], 1, tf.shape(reversed_blank_log_pr)[-1]), dtype=tf.float32)\n",
    "    ], axis=-2)\n",
    "    padded_blank_log_pr = tf.pad(\n",
    "      reversed_blank_log_pr,\n",
    "      [[0, 0], [0, 0], [tf.shape(reversed_blank_log_pr)[-2] - 1, 0]],\n",
    "      constant_values=LOG_0\n",
    "    )\n",
    "    blank_diag = tf.linalg.diag_part(\n",
    "      padded_blank_log_pr,\n",
    "      k=(0, tf.shape(padded_blank_log_pr)[-1] - 1),\n",
    "      padding_value=LOG_0,\n",
    "      align='LEFT_RIGHT'\n",
    "    )\n",
    "    blank_diag = tf.transpose(blank_diag, perm=[1, 0, 2])\n",
    "    blank_diag = blank_diag[: -1]\n",
    "\n",
    "    mask = tf.sequence_mask(\n",
    "      time_lengths + label_lengths - 2,\n",
    "      tf.shape(log_pr)[1] + tf.shape(log_pr)[2] - 2,\n",
    "      dtype=tf.float32\n",
    "    )\n",
    "    mask = tf.transpose(mask, perm=[1, 0])\n",
    "\n",
    "    dp_start_value = tf.gather_nd(\n",
    "      blank_log_pr,\n",
    "      indices=tf.stack([time_lengths, label_lengths], axis=-1) - 1,\n",
    "      batch_dims=1\n",
    "    )\n",
    "\n",
    "    initial_diag_mask = tf.one_hot(time_lengths - 1, depth=tf.shape(log_pr)[1])\n",
    "    initial_diag = tf.expand_dims(dp_start_value, axis=1) * initial_diag_mask + nan_to_zero(LOG_0 * (1.0 - initial_diag_mask))\n",
    "\n",
    "    def step (a, x):\n",
    "      m, t, b = x\n",
    "      a_next = tf.reduce_logsumexp(\n",
    "        tf.stack([\n",
    "          a + t,\n",
    "          tf.concat([\n",
    "            a[:, 1: ],\n",
    "            LOG_0 * tf.ones((tf.shape(a)[0], 1), dtype=tf.float32)\n",
    "          ], axis=-1) + b\n",
    "        ], axis=0),\n",
    "        axis=0\n",
    "      )\n",
    "      masked_a_next = nan_to_zero(a_next * tf.expand_dims(m, axis=1)) + nan_to_zero(a * tf.expand_dims(1.0 - m, axis=1))\n",
    "      return masked_a_next\n",
    "\n",
    "    beta_diag = tf.concat([\n",
    "      tf.scan(step, (mask, truth_diag, blank_diag), initial_diag, reverse=True),\n",
    "      tf.expand_dims(initial_diag, axis=0)\n",
    "    ], axis=0)\n",
    "\n",
    "    beta_diag = tf.transpose(beta_diag, perm=[1, 2, 0])\n",
    "    beta = tf.linalg.diag_part(beta_diag, k=(0, tf.shape(log_pr)[2] - 1), padding_value=LOG_0)\n",
    "    beta = tf.transpose(beta, perm=[0, 2, 1])\n",
    "    beta = tf.reverse(beta, axis=[-1])\n",
    "\n",
    "    return beta\n",
    "  \n",
    "  time_mask = tf.sequence_mask(time_lengths, tf.shape(log_pr)[1], dtype=tf.float32)\n",
    "  label_mask = tf.sequence_mask(label_lengths, tf.shape(log_pr)[2], dtype=tf.float32)\n",
    "  total_mask = tf.expand_dims(time_mask, axis=2) * tf.expand_dims(label_mask, axis=1)\n",
    "\n",
    "  alpha = get_alpha(truth_log_pr, blank_log_pr)\n",
    "  alpha = alpha + nan_to_zero((1.0 - total_mask) * LOG_0)\n",
    "  beta = get_beta(truth_log_pr, blank_log_pr)\n",
    "  beta = beta + nan_to_zero((1.0 - total_mask) * LOG_0)\n",
    "\n",
    "  indices = tf.concat([\n",
    "    tf.expand_dims(tf.range(0, tf.shape(log_pr)[0]), axis=1),\n",
    "    tf.stack([\n",
    "      time_lengths,\n",
    "      label_lengths - 1\n",
    "    ], axis=-1),\n",
    "  ], axis=-1)\n",
    "  \n",
    "  beta_mask = tf.scatter_nd(\n",
    "    indices,\n",
    "    tf.ones(tf.shape(indices)[0], tf.float32),\n",
    "    (tf.shape(log_pr)[0], tf.shape(log_pr)[1] + 1, tf.shape(log_pr)[2])\n",
    "  )\n",
    "  beta_mask = 1.0 - beta_mask\n",
    "  beta = nan_to_zero(tf.pad(beta, [[0, 0], [0, 1], [0, 0]], constant_values=LOG_0) * beta_mask)\n",
    "\n",
    "  total_mask = tf.expand_dims(total_mask, axis=-1)\n",
    "\n",
    "  total_log_pr = beta[:, 0, 0]\n",
    "\n",
    "  def grad (upstream):\n",
    "    blank_grads = \\\n",
    "      alpha + beta[:, 1: , :] \\\n",
    "      - tf.reshape(total_log_pr, shape=(tf.shape(total_log_pr)[0], 1, 1))\n",
    "    truth_grads = \\\n",
    "      alpha + tf.pad(\n",
    "        beta[:, : -1, 1: ],\n",
    "        [[0, 0], [0, 0], [0, 1]],\n",
    "        constant_values=LOG_0\n",
    "      ) \\\n",
    "      - tf.reshape(total_log_pr, shape=(tf.shape(total_log_pr)[0], 1, 1))\n",
    "    blank_one_hot = tf.one_hot(tf.zeros_like(labels, dtype=tf.int32), tf.shape(log_pr)[-1], dtype=tf.float32)\n",
    "    blank_one_hot = tf.expand_dims(blank_one_hot, axis=1)\n",
    "    blank_one_hot = tf.repeat(blank_one_hot, tf.shape(log_pr)[1], axis=1)\n",
    "    blank_grads = tf.exp(tf.expand_dims(blank_grads, axis=-1) + log_pr) * blank_one_hot\n",
    "    truth_one_hot = tf.one_hot(labels[:, 1: ], tf.shape(log_pr)[-1], dtype=tf.float32)\n",
    "    truth_one_hot = tf.concat([truth_one_hot, tf.zeros((tf.shape(log_pr)[0], 1, tf.shape(log_pr)[-1]), dtype=tf.float32)], axis=-2)\n",
    "    truth_one_hot = tf.expand_dims(truth_one_hot, axis=1)\n",
    "    truth_one_hot = tf.repeat(truth_one_hot, tf.shape(log_pr)[1], axis=1)\n",
    "    truth_grads = tf.exp(tf.expand_dims(truth_grads, axis=-1) + log_pr) * truth_one_hot\n",
    "\n",
    "    grads = blank_grads + truth_grads\n",
    "\n",
    "    return (\n",
    "      [\n",
    "        tf.reshape(-upstream, shape=(tf.shape(upstream)[0], 1, 1, 1))\n",
    "        * grads * total_mask\n",
    "      ] +\n",
    "      [None] * 3\n",
    "    )\n",
    "\n",
    "  return -total_log_pr, grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "UNITS = 1024\n",
    "CODER_OUTPUT_DIM = 320\n",
    "JOINT_NET_INNER_DIM = 320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder (tf.keras.Model):\n",
    "  def __init__ (self, units, output_dim):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.gru1 = tf.keras.layers.GRU(units, recurrent_initializer='glorot_uniform', return_sequences=True, return_state=True)\n",
    "    self.grus = [tf.keras.layers.GRU(units, recurrent_initializer='glorot_uniform', return_sequences=True, return_state=True) for _ in range(5)]\n",
    "    self.fc = tf.keras.layers.Dense(output_dim)\n",
    "    \n",
    "  def call (self, inputs):\n",
    "    rnn_outputs, hidden_states = self.gru1(inputs)\n",
    "    for gru in self.grus:\n",
    "      rnn_outputs, hidden_states = gru(rnn_outputs)\n",
    "    outputs = self.fc(rnn_outputs)\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder (tf.keras.Model):\n",
    "  def __init__ (self, vocab_size, embedding_dim, units, output_dim):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru1 = tf.keras.layers.GRU(units, recurrent_initializer='glorot_uniform', return_sequences=True, return_state=True)\n",
    "    self.gru2 = tf.keras.layers.GRU(units, recurrent_initializer='glorot_uniform', return_sequences=True, return_state=True)\n",
    "    self.fc = tf.keras.layers.Dense(output_dim)\n",
    "\n",
    "  def call (self, inputs):\n",
    "    embedded = self.embedding(inputs)\n",
    "    outputs, hidden_states = self.gru1(embedded)\n",
    "    outputs, hidden_states = self.gru2(outputs)\n",
    "    outputs = self.fc(outputs)\n",
    "    return outputs, hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointNet (tf.keras.Model):\n",
    "  def __init__ (self, inner_dim, vocab_size):\n",
    "    super(JointNet, self).__init__()\n",
    "    self.forward_layer = tf.keras.layers.Dense(inner_dim, activation='tanh')\n",
    "    self.project_layer = tf.keras.layers.Dense(vocab_size)\n",
    "  \n",
    "  def call (self, inputs):\n",
    "    enc_outputs, dec_outputs = inputs\n",
    "    joint_inputs = tf.expand_dims(enc_outputs, axis=2) + tf.expand_dims(dec_outputs, axis=1)\n",
    "    outputs = self.forward_layer(joint_inputs)\n",
    "    outputs = self.project_layer(outputs)\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transducer (tf.keras.Model):\n",
    "  def __init__ (self, embedding_dim, units, coder_output_dim, joint_net_inner_dim, vocab_size):\n",
    "    super(Transducer, self).__init__()\n",
    "    self.encoder = Encoder(units, coder_output_dim)\n",
    "    self.decoder = Decoder(vocab_size, embedding_dim, units, coder_output_dim)\n",
    "    self.joint_net = JointNet(joint_net_inner_dim, vocab_size)\n",
    "    self.vocab_size = vocab_size\n",
    "  \n",
    "  def call (self, inputs):\n",
    "    enc_inputs, dec_inputs = inputs\n",
    "    enc_outputs = self.encoder(enc_inputs)\n",
    "    dec_outputs, dec_states = self.decoder(dec_inputs)\n",
    "    outputs = self.joint_net([enc_outputs, dec_outputs])\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import librosa\n",
    "from functools import reduce\n",
    "\n",
    "class Saver:\n",
    "  def save (self, path, cur_epoch, cnt):\n",
    "    pass\n",
    "\n",
    "class InitialSaver:\n",
    "  def initial_save (self):\n",
    "    pass\n",
    "\n",
    "class TrainPolicy:\n",
    "  def train (self):\n",
    "    pass\n",
    "\n",
    "class StepPolicy (TrainPolicy):\n",
    "  def __init__ (\n",
    "    self,\n",
    "    optimizer,\n",
    "    epochs,\n",
    "    batch_size,\n",
    "    batch_num,\n",
    "    time_limit,\n",
    "    label_limit,\n",
    "    step_performer,\n",
    "    step_end_handlers,\n",
    "    epoch_end_handlers,\n",
    "    dataset,\n",
    "    dataset_extractor,\n",
    "    cur_epoch,\n",
    "    cnt\n",
    "  ):\n",
    "    self.optimizer = optimizer\n",
    "    self.epochs = epochs\n",
    "    self.batch_size = batch_size\n",
    "    self.batch_num = batch_num\n",
    "    self.time_limit = time_limit\n",
    "    self.label_limit = label_limit\n",
    "\n",
    "    self.step_performer = step_performer\n",
    "    self.step_end_handlers = step_end_handlers\n",
    "    self.epoch_end_handlers = epoch_end_handlers\n",
    "\n",
    "    self.dataset = dataset\n",
    "    self.dataset_extractor = dataset_extractor\n",
    "    self.cur_epoch = cur_epoch\n",
    "    self.cnt = cnt\n",
    "\n",
    "  def train (self):\n",
    "    for i in range(self.cur_epoch, self.epochs):\n",
    "      while not self.dataset_extractor.check_end():\n",
    "        loss = self.step_performer.step(self.optimizer, self.batch_size, self.batch_num, self.dataset_extractor.get_value())\n",
    "        self.cnt += self.batch_num * self.batch_size\n",
    "        for step_end_handler in self.step_end_handlers:\n",
    "          step_end_handler.handle(self.cur_epoch, self.cnt, loss, self.batch_num * self.batch_size)\n",
    "        self.dataset_extractor = self.dataset_extractor.get_next_extractor()\n",
    "      \n",
    "      self.cur_epoch += 1\n",
    "      self.cnt = 0\n",
    "      self.dataset.shuffle()\n",
    "      self.dataset_extractor = SavableDatasetStepExtractor(\n",
    "        self.dataset,\n",
    "        self.batch_size * self.batch_num,\n",
    "        DatasetSizeLimitingIterator(self.dataset, self.time_limit, self.label_limit, 0)\n",
    "      )\n",
    "      for epoch_end_handler in self.epoch_end_handlers:\n",
    "        epoch_end_handler.handle(self.epochs, self.cur_epoch, self.cnt)\n",
    "\n",
    "class SavableStepPolicy (StepPolicy, Saver, InitialSaver):\n",
    "  def __init__ (\n",
    "    self,\n",
    "    path,\n",
    "    savable_save_notifier,\n",
    "    optimizer,\n",
    "    epochs,\n",
    "    batch_size,\n",
    "    batch_num,\n",
    "    time_limit,\n",
    "    label_limit,\n",
    "    step_performer,\n",
    "    step_end_handlers,\n",
    "    epoch_end_handlers,\n",
    "    dataset,\n",
    "    dataset_extractor,\n",
    "    cur_epoch,\n",
    "    cnt,\n",
    "    prev_save_time\n",
    "  ):\n",
    "    StepPolicy.__init__(\n",
    "      self,\n",
    "      optimizer,\n",
    "      epochs,\n",
    "      batch_size,\n",
    "      batch_num,\n",
    "      time_limit,\n",
    "      label_limit,\n",
    "      step_performer,\n",
    "      step_end_handlers,\n",
    "      epoch_end_handlers,\n",
    "      dataset,\n",
    "      dataset_extractor,\n",
    "      cur_epoch,\n",
    "      cnt\n",
    "    )\n",
    "    savable_save_notifier.savers.append(self)\n",
    "    savable_save_notifier.savers.append(self.step_performer)\n",
    "    self.step_end_handlers.append(\n",
    "      savable_save_notifier\n",
    "    )\n",
    "    self.epoch_end_handlers.append(\n",
    "      SavableEpochSaveNotifier(\n",
    "        path,\n",
    "        [\n",
    "          self,\n",
    "          self.step_performer,\n",
    "        ],\n",
    "        prev_save_time\n",
    "      )\n",
    "    )\n",
    "  \n",
    "  def save (self, path, cur_epoch, cnt):\n",
    "    with open(os.path.join(path, 'progress.pickle'), 'wb') as f:\n",
    "      pickle.dump((self.cur_epoch, self.cnt), f)\n",
    "    self.dataset_extractor.save(IteratorSaver(os.path.join(path, 'dataset_extractor')))\n",
    "\n",
    "  def initial_save (self, path):\n",
    "    with open(os.path.join(path, 'progress.pickle'), 'wb') as f:\n",
    "      pickle.dump((self.cur_epoch, self.cnt), f)\n",
    "    self.dataset_extractor.save(ExtractorSaver(os.path.join(path, 'dataset_extractor')))\n",
    "    with open(os.path.join(path, 'policy_meta.pickle'), 'wb') as f:\n",
    "      pickle.dump((self.batch_num, self.batch_size, self.time_limit, self.label_limit), f)\n",
    "\n",
    "class TrainStepPerformer:\n",
    "  def __init__ (self, transducer):\n",
    "    self.transducer = transducer\n",
    "\n",
    "  def step (self, optimizer, batch_size, batch_num, datas):\n",
    "    acc_grads = [tf.zeros_like(var) for var in self.transducer.trainable_variables]\n",
    "    acc_loss = 0\n",
    "    for i in range(0, len(datas), batch_size):\n",
    "      batch_datas = datas[i: i + batch_size]\n",
    "      label_lengths = self.get_label_lengths(batch_datas)\n",
    "      label_lengths = tf.convert_to_tensor(label_lengths, dtype=tf.int32)\n",
    "      labels = pad_sequences(list(map(lambda x: x[1], batch_datas)), maxlen=self.get_labels_maxlen(batch_datas), truncating='post', padding='post')\n",
    "      labels = list(map(lambda x: x[1], batch_datas))\n",
    "      max_label_length = reduce(lambda a, x: max(a, x), [len(label) for label in labels])\n",
    "      labels = [label + [0] * (max_label_length - len(label)) for label in labels]\n",
    "      labels = tf.constant(labels, dtype=tf.int32)\n",
    "      spectrograms = [x[0] for x in batch_datas]\n",
    "      spectrogram_lengths = self.get_spectrogram_lengths(spectrograms)\n",
    "      spectrogram_lengths = tf.convert_to_tensor(spectrogram_lengths, dtype=tf.int32)\n",
    "      spectrograms = tf.cast(pad_sequences(spectrograms, maxlen=self.get_spectrograms_maxlen(spectrograms), truncating='post', padding='post'), dtype=tf.float32)\n",
    "      with tf.GradientTape() as tape:\n",
    "        logits = self.transducer([spectrograms, labels])\n",
    "        loss = rnnt_loss(\n",
    "          logits,\n",
    "          labels,\n",
    "          spectrogram_lengths,\n",
    "          label_lengths\n",
    "        )\n",
    "      acc_loss += loss.numpy()\n",
    "      grads = tape.gradient(loss, self.transducer.trainable_variables)\n",
    "      acc_grads = [acc_grad + grad for acc_grad, grad in zip(acc_grads, grads)]\n",
    "    acc_grads = [acc_grad / (batch_size * batch_num) for acc_grad in acc_grads]\n",
    "    optimizer.apply_gradients(zip(acc_grads, self.transducer.trainable_variables))\n",
    "    \n",
    "    return acc_loss / (batch_size * batch_num)\n",
    "  \n",
    "  def get_labels_maxlen (self, batch_datas):\n",
    "    ret = 0\n",
    "    for file_label_vector_pair in batch_datas:\n",
    "      ret = max(ret, len(file_label_vector_pair[1]))\n",
    "    return ret\n",
    "  \n",
    "  def get_label_lengths (self, batch_datas):\n",
    "    ret = list(map(lambda x: len(x[1]), batch_datas))\n",
    "    return ret\n",
    "  \n",
    "  def get_spectrograms_maxlen (self, spectrograms):\n",
    "    ret = 0\n",
    "    for spectrogram in spectrograms:\n",
    "      ret = max(ret, spectrogram.shape[0])\n",
    "    return ret\n",
    "\n",
    "  def get_spectrogram_lengths (self, spectrograms):\n",
    "    ret = list(map(lambda x: x.shape[0], spectrograms))\n",
    "    return ret\n",
    "\n",
    "class SavableTrainStepPerformer (TrainStepPerformer, Saver, InitialSaver):\n",
    "  def __init__ (self, transducer, embedding_dim, units, coder_output_dim, joint_net_inner_dim, vocab_size):\n",
    "    TrainStepPerformer.__init__(self, transducer)\n",
    "    self.embedding_dim = embedding_dim\n",
    "    self.units = units\n",
    "    self.coder_output_dim = coder_output_dim\n",
    "    self.joint_net_inner_dim = joint_net_inner_dim\n",
    "    self.vocab_size = vocab_size\n",
    "\n",
    "  def save (self, path, cur_epoch, cnt):\n",
    "    self.transducer.save_weights(os.path.join(path, 'checkpoints/ep{}_no{}/ckpt'.format(cur_epoch, cnt)))\n",
    "\n",
    "  def initial_save (self, path):\n",
    "    self.transducer.save_weights(os.path.join(path, 'checkpoints/ep{}_no{}/ckpt'.format(0, 0)))\n",
    "    with open(os.path.join(path, 'model_meta.pickle'), 'wb') as f:\n",
    "      pickle.dump((\n",
    "        self.embedding_dim,\n",
    "        self.units,\n",
    "        self.coder_output_dim,\n",
    "        self.joint_net_inner_dim,\n",
    "        self.vocab_size\n",
    "      ), f)\n",
    "\n",
    "class SavableTrainStepPerformerLoader:\n",
    "  @staticmethod\n",
    "  def load (path):\n",
    "    with open(os.path.join(path, 'model_meta.pickle'), 'rb') as f:\n",
    "      model_meta = pickle.load(f)\n",
    "    with open(os.path.join(path, 'progress.pickle'), 'rb') as f:\n",
    "      progress = pickle.load(f)\n",
    "    transducer = Transducer(*model_meta)\n",
    "    transducer([\n",
    "      tf.keras.Input(shape=(None, 128), dtype=tf.float32),\n",
    "      tf.keras.Input(shape=(None, ), dtype=tf.int32)\n",
    "    ])\n",
    "    transducer.load_weights(os.path.join(path, 'checkpoints/ep{}_no{}/ckpt'.format(progress[0], progress[1])))\n",
    "    return SavableTrainStepPerformer(\n",
    "      transducer,\n",
    "      *model_meta\n",
    "    )\n",
    "\n",
    "class PrevSaveTime:\n",
    "  def __init__ (self, prev_save_time):\n",
    "    self.prev_save_time = prev_save_time\n",
    "  \n",
    "  def update (self, prev_save_time):\n",
    "    self.prev_save_time = prev_save_time\n",
    "\n",
    "class EpochEndHandler:\n",
    "  def handle (self, epochs, cur_epoch, cnt):\n",
    "    pass\n",
    "\n",
    "class EpochPrinter (EpochEndHandler):\n",
    "  def handle (self, epochs, cur_epoch, cnt):\n",
    "    print('epoch: {} / {}'.format(cur_epoch, epochs))\n",
    "\n",
    "class EpochSaveNotifier (EpochEndHandler):\n",
    "  def __init__ (self, path, savers):\n",
    "    super(EpochSaveNotifier, self).__init__()\n",
    "    self.path = path\n",
    "    self.savers = savers\n",
    "\n",
    "  def handle (self, epochs, cur_epoch, cnt):\n",
    "    for saver in self.savers:\n",
    "      saver.save(self.path, cur_epoch, cnt)\n",
    "  \n",
    "class SavableEpochSaveNotifier (EpochSaveNotifier):\n",
    "  def __init__ (self, path, savers, prev_save_time):\n",
    "    super(SavableEpochSaveNotifier, self).__init__(path, savers)\n",
    "    self.prev_save_time = prev_save_time\n",
    "    self.savers.append(self)\n",
    "  \n",
    "  def save (self, path, cur_epoch, cnt):\n",
    "    with open(os.path.join(path, 'prev_save_time.pickle'), 'wb') as f:\n",
    "      pickle.dump(0, f)\n",
    "    self.prev_save_time.update(0)\n",
    "\n",
    "class StepEndHandler:\n",
    "  def handle (self, cur_epoch, cnt, loss, step_size):\n",
    "    pass\n",
    "\n",
    "class StepPrinter (StepEndHandler):\n",
    "  def handle (self, cur_epoch, cnt, loss, step_size):\n",
    "    print('epoch, cnt: {}, {}'.format(cur_epoch, cnt))\n",
    "    print('loss: {}'.format(loss / step_size))\n",
    "\n",
    "class SaveNotifier (StepEndHandler):\n",
    "  def __init__ (self, path, savers, save_time_delta, prev_save_time):\n",
    "    super(SaveNotifier, self).__init__()\n",
    "    self.path = path\n",
    "    self.savers = savers\n",
    "    self.save_time_delta = save_time_delta\n",
    "    self.prev_save_time = prev_save_time\n",
    "\n",
    "  def handle (self, cur_epoch, cnt, loss, step_size):\n",
    "    if self.prev_save_time.prev_save_time + self.save_time_delta <= cnt:\n",
    "      for saver in self.savers:\n",
    "        saver.save(self.path, cur_epoch, cnt)\n",
    "      self.prev_save_time.update(cnt)\n",
    "\n",
    "class SavableSaveNotifier (SaveNotifier, Saver, InitialSaver):\n",
    "  def __init__ (self, path, savers, save_time_delta, prev_save_time):\n",
    "    super(SavableSaveNotifier, self).__init__(path, savers, save_time_delta, prev_save_time)\n",
    "    self.savers.append(self)\n",
    "  \n",
    "  def save (self, path, cur_epoch, cnt):\n",
    "    with open(os.path.join(path, 'prev_save_time.pickle'), 'wb') as f:\n",
    "      pickle.dump(cnt, f)\n",
    "  \n",
    "  def initial_save (self, path):\n",
    "    with open(os.path.join(path, 'prev_save_time.pickle'), 'wb') as f:\n",
    "      pickle.dump(0, f)\n",
    "\n",
    "class InitialSaveNotifier:\n",
    "  def __init__ (self, path, initial_savers):\n",
    "    self.path = path\n",
    "    self.initial_savers = initial_savers\n",
    "\n",
    "  def initial_save (self):\n",
    "    os.makedirs(self.path, exist_ok=True)\n",
    "    for initial_saver in self.initial_savers:\n",
    "      initial_saver.initial_save(self.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePolicyGenerator:\n",
    "  @staticmethod\n",
    "  def generate (epoch, batch_size, batch_num, time_limit, label_limit, learning_rate, save_path, save_time_delta):\n",
    "    prev_save_time = PrevSaveTime(0)\n",
    "    savable_save_notifier = SavableSaveNotifier(save_path, [], save_time_delta, prev_save_time)\n",
    "    step_size = batch_size * batch_num\n",
    "    dataset = ShufflableDatasetGenerator.generate()\n",
    "    dataset_extractor = SavableDatasetStepExtractor(\n",
    "      dataset,\n",
    "      step_size,\n",
    "      DatasetSizeLimitingIterator(dataset, time_limit, label_limit, 0)\n",
    "    )\n",
    "    cur_epoch = 0\n",
    "    cnt = 0\n",
    "\n",
    "    model_meta = (EMBEDDING_DIM, UNITS, CODER_OUTPUT_DIM, JOINT_NET_INNER_DIM, len(dataset_extractor.dataset.phoneme.phonemes))\n",
    "    transducer = Transducer(*model_meta)\n",
    "    transducer([\n",
    "      tf.keras.Input(shape=(None, 128), dtype=tf.float32),\n",
    "      tf.keras.Input(shape=(None, ), dtype=tf.int32)\n",
    "    ])\n",
    "\n",
    "    policy = SavableStepPolicy(\n",
    "      save_path,\n",
    "      savable_save_notifier,\n",
    "      tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "      epoch,\n",
    "      batch_size,\n",
    "      batch_num,\n",
    "      time_limit,\n",
    "      label_limit,\n",
    "      SavableTrainStepPerformer(\n",
    "        transducer,\n",
    "        *model_meta\n",
    "      ),\n",
    "      [StepPrinter()],\n",
    "      [EpochPrinter()],\n",
    "      dataset,\n",
    "      dataset_extractor,\n",
    "      cur_epoch,\n",
    "      cnt,\n",
    "      prev_save_time\n",
    "    )\n",
    "    return \\\n",
    "      policy, \\\n",
    "      InitialSaveNotifier(\n",
    "        save_path, [\n",
    "          policy,\n",
    "          policy.step_performer,\n",
    "          savable_save_notifier\n",
    "        ]\n",
    "      )\n",
    "\n",
    "class SimplePolicyLoader:\n",
    "  @staticmethod\n",
    "  def load (path, epoch, batch_size, batch_num, time_limit, label_limit, learning_rate, save_time_delta):\n",
    "    with open(os.path.join(path, 'progress.pickle'), 'rb') as f:\n",
    "      progress = pickle.load(f)\n",
    "    dataset_extractor = DatasetStepExtractorLoader.load(os.path.join(path, 'dataset_extractor'))\n",
    "    prev_save_time = PrevSaveTime(\n",
    "      pickle.load(\n",
    "        open(os.path.join(path, 'prev_save_time.pickle'), 'rb')\n",
    "      )\n",
    "    )\n",
    "    policy = SavableStepPolicy(\n",
    "      path,\n",
    "      SavableSaveNotifier(\n",
    "        path,\n",
    "        [],\n",
    "        save_time_delta,\n",
    "        prev_save_time\n",
    "      ),\n",
    "      tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "      epoch,\n",
    "      batch_size,\n",
    "      batch_num,\n",
    "      time_limit,\n",
    "      label_limit,\n",
    "      SavableTrainStepPerformerLoader.load(path),\n",
    "      [StepPrinter()],\n",
    "      [EpochPrinter()],\n",
    "      dataset_extractor.dataset,\n",
    "      dataset_extractor,\n",
    "      progress[0],\n",
    "      progress[1],\n",
    "      prev_save_time\n",
    "    )\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "BATCH_NUM = 1\n",
    "TIME_LIMIT = 200\n",
    "LABEL_LIMIT = 50\n",
    "LEARNING_RATE = 0.0001\n",
    "PATH = 'saves/train_state10'\n",
    "SAVE_TIME_DELTA = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akswnd98\\anaconda3\\lib\\site-packages\\librosa\\util\\decorators.py:88: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# policy, initial_save_notifier = SimplePolicyGenerator.generate(EPOCHS, BATCH_SIZE, BATCH_NUM, TIME_LIMIT, LABEL_LIMIT, LEARNING_RATE, PATH, SAVE_TIME_DELTA)\n",
    "# initial_save_notifier.initial_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = SimplePolicyLoader.load(PATH, EPOCHS, BATCH_SIZE, BATCH_NUM, TIME_LIMIT, LABEL_LIMIT, LEARNING_RATE, SAVE_TIME_DELTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transducer\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder (Encoder)           multiple                  35361088  \n",
      "                                                                 \n",
      " decoder (Decoder)           multiple                  10177856  \n",
      "                                                                 \n",
      " joint_net (JointNet)        multiple                  120696    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 45,659,640\n",
      "Trainable params: 45,659,640\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "policy.step_performer.transducer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch, cnt: 0, 32\n",
      "loss: 21.478227615356445\n",
      "epoch, cnt: 0, 64\n",
      "loss: 5.421279430389404\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "{{function_node __wrapped__TanhGrad_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[32,236,45,320] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:TanhGrad]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14676\\912290154.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/gpu:0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m   \u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14676\\1561014044.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m       \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_extractor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_performer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_extractor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcnt\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_num\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mstep_end_handler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_end_handlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14676\\1561014044.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, optimizer, batch_size, batch_num, datas)\u001b[0m\n\u001b[0;32m    163\u001b[0m         )\n\u001b[0;32m    164\u001b[0m       \u001b[0macc_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m       \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransducer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m       \u001b[0macc_grads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0macc_grad\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mgrad\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0macc_grad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc_grads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[0macc_grads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0macc_grad\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatch_num\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0macc_grad\u001b[0m \u001b[1;32min\u001b[0m \u001b[0macc_grads\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1111\u001b[0m                           for x in output_gradients]\n\u001b[0;32m   1112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1113\u001b[1;33m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[0;32m   1114\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1115\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     65\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[0;32m     68\u001b[0m       \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    158\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\math_grad.py\u001b[0m in \u001b[0;36m_TanhGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m    778\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mtanh_grad\u001b[1;34m(y, dy, name)\u001b[0m\n\u001b[0;32m  11480\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11481\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 11482\u001b[1;33m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m  11483\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11484\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7207\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7208\u001b[0m   \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7209\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__TanhGrad_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[32,236,45,320] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:TanhGrad]"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "  policy.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_to_sentence (x):\n",
    "  return ''.join([policy.dataset_extractor.dataset.phoneme.phonemes[idx] for idx in x][1: ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram, labels = policy.dataset_extractor.get_value()[15]\n",
    "\n",
    "logits = policy.step_performer.transducer((tf.convert_to_tensor([spectrogram]), tf.convert_to_tensor([labels])))\n",
    "print(tf.argmax(logits, axis=-1)[:, :, : ][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDecoder:\n",
    "  def __init__ (self, transducer):\n",
    "    self.transducer = transducer\n",
    "  \n",
    "  def decode (self, spectrogram):\n",
    "    label = [0]\n",
    "    for i in range(spectrogram.shape[0]):\n",
    "      sub_spectrogram = spectrogram[0: i + 1]\n",
    "      sub_spectrogram = tf.convert_to_tensor(sub_spectrogram)\n",
    "      sub_spectrogram = tf.expand_dims(sub_spectrogram, axis=0)\n",
    "      sub_label = tf.constant([label], dtype=tf.int32)\n",
    "      sub_rst = self.transducer((sub_spectrogram, sub_label))\n",
    "      sub_rst = tf.argmax(sub_rst[0][-1][-1], axis=-1).numpy()\n",
    "      label.append(sub_rst)\n",
    "    return label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = AudioDecoder(policy.step_performer.transducer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(filter(lambda x: x != 0, decoder.decode(spectrogram)))\n",
    "print(moasseugi(vector_to_sentence([0] + a)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(moasseugi(vector_to_sentence(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rst = []\n",
    "x, y = 0, 0\n",
    "while x < logits.shape[2] and y < logits.shape[1]:\n",
    "  sel = tf.argmax(logits[0][y][x], axis=-1).numpy()\n",
    "  rst.append(sel)\n",
    "  if sel == 0:\n",
    "    y += 1\n",
    "  else:\n",
    "    x += 1\n",
    "\n",
    "print(moasseugi(vector_to_sentence([0] + list(filter(lambda x: x != 0, rst)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ec62f2ec7c7d49224ec335e3ef472b8ef537dc7949933ed74784e8710c2a92fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
