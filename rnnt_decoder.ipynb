{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o/ 근데 칠십 퍼센트가 커 보이긴 하는데 이백 벌다 백 사십 벌면 빡셀걸? b/\n",
      "근데 삼 학년 때 까지는 국가장학금 바+ 받으면서 다녔던 건가?\n"
     ]
    }
   ],
   "source": [
    "test1 = \"o/ 근데 (70%)/(칠십 퍼센트)가 커 보이긴 하는데 (200)/(이백) 벌다 (140)/(백 사십) 벌면 빡셀걸? b/\"\n",
    "test2 = \"근데 (3학년)/(삼 학년) 때 까지는 국가장학금 바+ 받으면서 다녔던 건가?\"\n",
    "\n",
    "def bracket_filter(sentence):\n",
    "  new_sentence = str()\n",
    "  flag = False\n",
    "  \n",
    "  for ch in sentence:\n",
    "    if ch == '(' and flag == False:\n",
    "      flag = True\n",
    "      continue\n",
    "    if ch == '(' and flag == True:\n",
    "      flag = False\n",
    "      continue\n",
    "    if ch != ')' and flag == False:\n",
    "      new_sentence += ch\n",
    "  return new_sentence\n",
    "\n",
    "print(bracket_filter(test1))\n",
    "print(bracket_filter(test2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "근데 칠십 퍼센트가 커 보이긴 하는데 이백 벌다 백 사십 벌면 빡셀걸?\n",
      "c샾 배워봤어?\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "test1 = \"o/ 근데 칠십 퍼센트가 커 보이긴 하는데 이백 벌다 백 사십 벌면 빡셀걸? b/\"\n",
    "test2 = \"c# 배워봤어?\"\n",
    "\n",
    "def special_filter(sentence):\n",
    "  SENTENCE_MARK = ['?', '!']\n",
    "  NOISE = ['o', 'n', 'u', 'b', 'l']\n",
    "  EXCEPT = ['/', '+', '*', '-', '@', '$', '^', '&', '[', ']', '=', ':', ';', '.', ',']\n",
    "  \n",
    "  new_sentence = str()\n",
    "  for idx, ch in enumerate(sentence):\n",
    "    if ch not in SENTENCE_MARK:\n",
    "      # o/, n/ 등 처리\n",
    "      if idx + 1 < len(sentence) and ch in NOISE and sentence[idx+1] == '/': \n",
    "        continue \n",
    "\n",
    "    if ch == '#': \n",
    "      new_sentence += '샾'\n",
    "\n",
    "    elif ch not in EXCEPT: \n",
    "      new_sentence += ch\n",
    "\n",
    "  pattern = re.compile(r'\\s\\s+')\n",
    "  new_sentence = re.sub(pattern, ' ', new_sentence.strip())\n",
    "  return new_sentence\n",
    "\n",
    "print(special_filter(test1))\n",
    "print(special_filter(test2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아 나 쓰리 DS 갖고 싶다 쓰리 DS\n"
     ]
    }
   ],
   "source": [
    "# test = \"o/ 근데 (70%)/(칠십 퍼센트)가 커 보이긴 하는데 (200)/(이백) 벌다 (140)/(백 사십) 벌면 빡셀걸? b/\"\n",
    "test = 'b/ 아 나 (3DS)/(쓰리 DS) 갖고 싶다. (3DS)/(쓰리 DS)'\n",
    "\n",
    "def sentence_filter(raw_sentence):\n",
    "  return special_filter(bracket_filter(raw_sentence))\n",
    "\n",
    "print(sentence_filter(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosung = (\"ㄱ\", \"ㄲ\", \"ㄴ\", \"ㄷ\", \"ㄸ\", \"ㄹ\", \"ㅁ\", \"ㅂ\", \"ㅃ\", \"ㅅ\", \"ㅆ\", \"ㅇ\", \"ㅈ\", \"ㅉ\", \"ㅊ\", \"ㅋ\", \"ㅌ\", \"ㅍ\", \"ㅎ\")\n",
    "\n",
    "jungsung = (\"ㅏ\", \"ㅐ\", \"ㅑ\", \"ㅒ\", \"ㅓ\", \"ㅔ\", \"ㅕ\", \"ㅖ\", \"ㅗ\", \"ㅘ\", \"ㅙ\", \"ㅚ\", \"ㅛ\", \"ㅜ\", \"ㅝ\", \"ㅞ\", \"ㅟ\", \"ㅠ\", \"ㅡ\", \"ㅢ\", \"ㅣ\")\n",
    "\n",
    "jongsung = (\"\", \"ㄱ\", \"ㄲ\", \"ㄳ\", \"ㄴ\", \"ㄵ\", \"ㄶ\", \"ㄷ\", \"ㄹ\", \"ㄺ\", \"ㄻ\", \"ㄼ\", \"ㄽ\", \"ㄾ\", \"ㄿ\", \"ㅀ\", \"ㅁ\", \"ㅂ\", \"ㅄ\", \"ㅅ\", \"ㅆ\", \"ㅇ\", \"ㅈ\", \"ㅊ\", \"ㅋ\", \"ㅌ\", \"ㅍ\", \"ㅎ\")\n",
    "\n",
    "def isHangeul(one_character):\n",
    "    return 0xAC00 <= ord(one_character[:1]) <= 0xD7A3\n",
    "\n",
    "def hangeulExplode(one_hangeul):\n",
    "    a = one_hangeul[:1]\n",
    "    if isHangeul(a) != True:\n",
    "        return False\n",
    "    b = ord(a) - 0xAC00\n",
    "    cho = b // (21*28)\n",
    "    jung = b % (21*28) // 28\n",
    "    jong = b % 28\n",
    "    if jong == 0:\n",
    "        return (chosung[cho], jungsung[jung])\n",
    "    else:\n",
    "        return (chosung[cho], jungsung[jung], jongsung[jong])\n",
    "\n",
    "def hangeulJoin(inputlist):\n",
    "    result = \"\"\n",
    "    cho, jung, jong = 0, 0, 0\n",
    "    inputlist.insert(0, \"\")\n",
    "    while len(inputlist) > 1:\n",
    "        if inputlist[-1] in jongsung:\n",
    "            if inputlist[-2] in jungsung:\n",
    "                jong = jongsung.index(inputlist.pop())\n",
    "            \n",
    "            else:\n",
    "                result += inputlist.pop()\n",
    "        elif inputlist[-1] in jungsung:\n",
    "            if inputlist[-2] in chosung:\n",
    "                jung = jungsung.index(inputlist.pop())\n",
    "                cho = chosung.index(inputlist.pop())\n",
    "                result += chr(0xAC00 + ((cho*21)+jung)*28+jong)\n",
    "                cho, jung, jong = 0, 0, 0\n",
    "            else:\n",
    "                result += inputlist.pop()\n",
    "\n",
    "        else:\n",
    "            result += inputlist.pop()\n",
    "    else:\n",
    "        return result[::-1]\n",
    "\n",
    "def pureosseugi(inputtext):\n",
    "    result = \"\"\n",
    "    for i in inputtext:\n",
    "        if isHangeul(i) == True:\n",
    "            for j in hangeulExplode(i):\n",
    "                result += j\n",
    "        else:\n",
    "            result += i\n",
    "    \n",
    "    return result\n",
    "\n",
    "def moasseugi(inputtext):\n",
    "    t1 = []\n",
    "    for i in inputtext:\n",
    "        t1.append(i)\n",
    "\n",
    "    return hangeulJoin(t1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "\n",
    "class Phoneme:\n",
    "  def __init__ (self):\n",
    "    self.BLANK_INDEX = 0\n",
    "    self.START_INDEX = 1\n",
    "    self.END_INDEX = 2\n",
    "    self.SPACE_INDEX = 3\n",
    "    self.OOV_INDEX = 4\n",
    "\n",
    "class PhonemeGenerator (Phoneme):\n",
    "  def __init__ (self, labels):\n",
    "    super(PhonemeGenerator, self).__init__()\n",
    "    self.phonemes = ['<BLANK>', '<START>', '<END>', ' ']\n",
    "\n",
    "    texts = []\n",
    "    for label in labels:\n",
    "      for phoneme in label:\n",
    "        if not isHangeul(phoneme) and not phoneme == ' ' and not phoneme == '?':\n",
    "          break\n",
    "        for pure in pureosseugi(phoneme):\n",
    "          texts.append(pure)\n",
    "\n",
    "    tokenizer = Tokenizer(filters='', lower=False, oov_token='<OOV>')\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "\n",
    "    for i in range(1, len(tokenizer.index_word) + 1):\n",
    "      self.phonemes.append(tokenizer.index_word[i])\n",
    "\n",
    "    self.phoneme_index = {}\n",
    "    for i, phoneme in enumerate(self.phonemes):\n",
    "      self.phoneme_index[phoneme] = i\n",
    "    \n",
    "    self.VOCAB_SIZE = len(self.phonemes)\n",
    "  \n",
    "  def save (self, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "      pickle.dump((self.phonemes, self.phoneme_index), f)\n",
    "\n",
    "\n",
    "class PhonemeLoader (Phoneme):\n",
    "  def __init__ (self, filename):\n",
    "    super(PhonemeLoader, self).__init__()\n",
    "    with open(filename, 'rb') as f:\n",
    "      self.phonemes, self.phoneme_index = pickle.load(f)\n",
    "\n",
    "    self.VOCAB_SIZE = len(self.phonemes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akswnd98\\anaconda3\\lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "class Dataset:\n",
    "  def get_data (self, idx):\n",
    "    return (\n",
    "      self.extract_spectrogram(self.file_phoneme_vector_pairs[idx][0]),\n",
    "      [0] + self.file_phoneme_vector_pairs[idx][1]\n",
    "    )\n",
    "  \n",
    "  def get_len (self):\n",
    "    return len(self.file_phoneme_vector_pairs)\n",
    "\n",
    "  def extract_spectrogram (self, filename):\n",
    "    waveform = np.memmap(filename, dtype='h', mode='r')\n",
    "\n",
    "    sr = 16000\n",
    "    st = 1 / sr\n",
    "\n",
    "    t = np.linspace(0, waveform.shape[0] * st, waveform.shape[0])\n",
    "\n",
    "    S = librosa.feature.melspectrogram(y=np.float32(waveform), n_fft=512, hop_length=256, window='hamming', sr=sr, n_mels=128)\n",
    "    log_S = librosa.power_to_db(S, ref=np.max)\n",
    "    log_S = np.transpose(log_S)\n",
    "\n",
    "    return log_S\n",
    "  \n",
    "  def save (self, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "      pickle.dump((self.phoneme, self.file_phoneme_vector_pairs), f)\n",
    "\n",
    "class RandomDataset (Dataset):\n",
    "  def __init__ (self):\n",
    "    super(RandomDataset, self).__init__()\n",
    "    with open('./ko-audio-dataset/sentence-script/KsponSpeech_scripts/train.trn', 'r', encoding='utf-8') as file:\n",
    "      lines = file.readlines()\n",
    "    def process_line (line):\n",
    "      line = line.split(' :: ')\n",
    "      line[0] = os.path.join('./ko-audio-dataset/audio/', line[0])\n",
    "      line[1] = sentence_filter(line[1])\n",
    "      return line\n",
    "    file_label_pairs = list(map(process_line, lines))\n",
    "    file_label_pairs = list(filter(lambda pair: not self.check_skip_label(pair[1]), file_label_pairs))\n",
    "    self.phoneme = PhonemeGenerator([label for file, label in file_label_pairs])\n",
    "    file_phoneme_pairs = list(map(lambda pair: (pair[0], pureosseugi(pair[1])), file_label_pairs))\n",
    "    self.file_phoneme_vector_pairs = [[pair[0], [self.phoneme.phoneme_index[phoneme] for phoneme in pair[1]]] for pair in file_phoneme_pairs]\n",
    "    random.shuffle(self.file_phoneme_vector_pairs)\n",
    "  \n",
    "  def check_skip_hangul (self, hangul):\n",
    "    return not isHangeul(hangul) and not hangul == ' ' and not hangul == '?'\n",
    "\n",
    "  def check_skip_label (self, label):\n",
    "    for hangul in label:\n",
    "      if self.check_skip_hangul(hangul):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "class LoadedDataset (Dataset):\n",
    "  def __init__ (self, filename):\n",
    "    super(LoadedDataset, self).__init__()\n",
    "    with open(filename, 'rb') as f:\n",
    "      self.phoneme, self.file_phoneme_vector_pairs = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetSizeLimitingIterator:\n",
    "  def __init__ (self, dataset, time_limit, label_limit, idx):\n",
    "    self.dataset = dataset\n",
    "    self.time_limit = time_limit\n",
    "    self.label_limit = label_limit\n",
    "    self.idx = idx\n",
    "    while self.idx < self.dataset.get_len():\n",
    "      data = self.dataset.get_data(self.idx)\n",
    "      if data[0].shape[0] <= self.time_limit and len(data[1]) <= self.label_limit:\n",
    "        break\n",
    "      self.idx += 1\n",
    "\n",
    "  def next (self):\n",
    "    idx = self.idx + 1\n",
    "    while idx < self.dataset.get_len():\n",
    "      data = self.dataset.get_data(idx)\n",
    "      if data[0].shape[0] <= self.time_limit and len(data[1]) <= self.label_limit:\n",
    "        break\n",
    "      idx += 1\n",
    "    return DatasetSizeLimitingIterator(self.dataset, self.time_limit, self.label_limit, idx)\n",
    "  \n",
    "  def check_end (self):\n",
    "    return self.idx >= self.dataset.get_len()\n",
    "\n",
    "  def get_value (self):\n",
    "    return self.dataset.get_data(self.idx)\n",
    "\n",
    "  def save (self, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "      pickle.dump((self.time_limit, self.label_limit,self.idx), f)\n",
    "\n",
    "class IteratorLoader:\n",
    "  @staticmethod\n",
    "  def load (dataset, filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "      time_limit, label_limit, idx = pickle.load(f)\n",
    "    return DatasetSizeLimitingIterator(dataset, time_limit, label_limit, idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class DatasetStepExtractorLoader:\n",
    "  @staticmethod\n",
    "  def load (path):\n",
    "    dataset = LoadedDataset(os.path.join(path, 'dataset.pickle'))\n",
    "    return SavableDatasetStepExtractor(\n",
    "      dataset,\n",
    "      pickle.load(open(os.path.join(path, 'step_size.pickle'), 'rb')),\n",
    "      IteratorLoader.load(dataset, os.path.join(path, 'iterator.pickle'))\n",
    "    )\n",
    "\n",
    "class DatasetStepExtractor:\n",
    "  def __init__ (self, dataset, step_size, iterator):\n",
    "    self.dataset = dataset\n",
    "    self.step_size = step_size\n",
    "    self.value = []\n",
    "    self.iterator = iterator\n",
    "    while not iterator.check_end() and len(self.value) < self.step_size:\n",
    "      self.value.append(iterator.get_value())\n",
    "      iterator = iterator.next()\n",
    "    self.is_end = len(self.value) < self.step_size\n",
    "    self.next_iterator = iterator\n",
    "  \n",
    "  def check_end (self):\n",
    "    return self.is_end\n",
    "  \n",
    "  def get_value (self):\n",
    "    return self.value\n",
    "  \n",
    "  def get_next_extractor (self):\n",
    "    return DatasetStepExtractor(self.dataset, self.step_size, self.next_iterator)\n",
    "\n",
    "class SavableDatasetStepExtractor (DatasetStepExtractor):\n",
    "  def save (self, saver):\n",
    "    saver.save(self.dataset, self.step_size, self.iterator)\n",
    "\n",
    "class DatasetStepExtractorSaver:\n",
    "  def __init__ (self, path):\n",
    "    self.path = path\n",
    "  \n",
    "  def save (self, dataset, step_size, iterator):\n",
    "    os.makedirs(self.path, exist_ok=True)\n",
    "\n",
    "class IteratorSaver (DatasetStepExtractorSaver):\n",
    "  def save (self, dataset, step_size, iterator):\n",
    "    super().save(dataset, step_size, iterator)\n",
    "    iterator.save(os.path.join(self.path, 'iterator.pickle'))\n",
    "  \n",
    "class DatasetSaver (DatasetStepExtractorSaver):\n",
    "  def save (self, dataset, step_size, iterator):\n",
    "    super().save(dataset, step_size, iterator)\n",
    "    dataset.save(os.path.join(self.path, 'dataset.pickle'))\n",
    "\n",
    "class StepSizeSaver (DatasetStepExtractorSaver):\n",
    "  def save (self, dataset, step_size, iterator):\n",
    "    super().save(dataset, step_size, iterator)\n",
    "    with open(os.path.join(self.path, 'step_size.pickle'), 'wb') as f:\n",
    "      pickle.dump(step_size, f)\n",
    "\n",
    "class DatasetStepExtractorSaver (DatasetStepExtractorSaver):\n",
    "  def save (self, dataset, step_size, iterator):\n",
    "    super().save(dataset, step_size, iterator)\n",
    "    iterator.save(os.path.join(self.path, 'iterator.pickle'))\n",
    "    dataset.save(os.path.join(self.path, 'dataset.pickle'))\n",
    "    with open(os.path.join(self.path, 'step_size.pickle'), 'wb') as f:\n",
    "      pickle.dump(step_size, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder (tf.keras.Model):\n",
    "  def __init__ (self, units, output_dim):\n",
    "    super(Encoder, self).__init__(self)\n",
    "    self.gru = tf.keras.layers.GRU(units, recurrent_initializer='glorot_uniform', return_sequences=True, return_state=True)\n",
    "    self.fc = tf.keras.layers.Dense(output_dim)\n",
    "    \n",
    "  def call (self, inputs):\n",
    "    rnn_outputs, hidden_states = self.gru(inputs)\n",
    "    outputs = self.fc(rnn_outputs)\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder (tf.keras.Model):\n",
    "  def __init__ (self, vocab_size, embedding_dim, units, output_dim):\n",
    "    super(Decoder, self).__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(units, recurrent_initializer='glorot_uniform', return_sequences=True, return_state=True)\n",
    "    self.fc = tf.keras.layers.Dense(output_dim)\n",
    "\n",
    "  def call (self, inputs):\n",
    "    embedded = self.embedding(inputs)\n",
    "    outputs, hidden_states = self.gru(embedded)\n",
    "    outputs = self.fc(outputs)\n",
    "    return outputs, hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointNet (tf.keras.Model):\n",
    "  def __init__ (self, inner_dim, vocab_size):\n",
    "    super(JointNet, self).__init__()\n",
    "    self.forward_layer = tf.keras.layers.Dense(inner_dim, activation='tanh')\n",
    "    self.project_layer = tf.keras.layers.Dense(vocab_size)\n",
    "  \n",
    "  def call (self, inputs):\n",
    "    enc_outputs, dec_outputs = inputs\n",
    "    joint_inputs = tf.expand_dims(enc_outputs, axis=2) + tf.expand_dims(dec_outputs, axis=1)\n",
    "    outputs = self.forward_layer(joint_inputs)\n",
    "    outputs = self.project_layer(outputs)\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transducer (tf.keras.Model):\n",
    "  def __init__ (self, encoder, decoder, joint_net):\n",
    "    super(Transducer, self).__init__()\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "    self.joint_net = joint_net\n",
    "\n",
    "  def __init__ (self, embedding_dim, units, coder_output_dim, joint_net_inner_dim, vocab_size):\n",
    "    super(Transducer, self).__init__()\n",
    "    self.encoder = Encoder(units, coder_output_dim)\n",
    "    self.decoder = Decoder(vocab_size, embedding_dim, units, coder_output_dim)\n",
    "    self.joint_net = JointNet(joint_net_inner_dim, vocab_size)\n",
    "    self.vocab_size = vocab_size\n",
    "  \n",
    "  def call (self, inputs):\n",
    "    enc_inputs, dec_inputs = inputs\n",
    "    enc_outputs = self.encoder(enc_inputs)\n",
    "    dec_outputs, dec_states = self.decoder(dec_inputs)\n",
    "    outputs = self.joint_net([enc_outputs, dec_outputs])\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDecoder:\n",
    "  def __init__ (self, transducer):\n",
    "    self.transducer = transducer\n",
    "  \n",
    "  def decode (self, spectrogram):\n",
    "    label = [0]\n",
    "    for i in range(spectrogram.shape[0]):\n",
    "      sub_spectrogram = spectrogram[0: i + 1]\n",
    "      sub_spectrogram = tf.convert_to_tensor(sub_spectrogram)\n",
    "      sub_spectrogram = tf.expand_dims(sub_spectrogram, axis=0)\n",
    "      sub_label = tf.constant([label], dtype=tf.int32)\n",
    "      sub_rst = self.transducer((sub_spectrogram, sub_label))\n",
    "      sub_rst = tf.argmax(sub_rst[0][-1][-1], axis=-1).numpy()\n",
    "      label.append(sub_rst)\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20960\\579671075.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mextractor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDatasetStepExtractorLoader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_state2/dataset_extractor'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20960\\458075800.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      4\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLoadedDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'dataset.pickle'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     return SavableDatasetStepExtractor(\n\u001b[0;32m      8\u001b[0m       \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20960\\2001916110.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLoadedDataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mphoneme\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile_phoneme_vector_pairs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "extractor = DatasetStepExtractorLoader.load('train_state2/dataset_extractor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = extractor.dataset.phoneme.VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 256\n",
    "UNITS = 1024\n",
    "CODER_OUTPUT_DIM = 512\n",
    "JOINT_NET_INNER_DIM = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x1850f5650a0>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transducer = Transducer(EMBEDDING_DIM, UNITS, CODER_OUTPUT_DIM, JOINT_NET_INNER_DIM, VOCAB_SIZE)\n",
    "transducer.load_weights('./train_state2/checkpoints/ep0_no35168/ckpt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = AudioDecoder(transducer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>', '<BLANK>']\n"
     ]
    }
   ],
   "source": [
    "rst = decoder.decode(extractor.get_value()[0][0])\n",
    "ret = []\n",
    "for i in rst:\n",
    "  ret.append(extractor.dataset.phoneme.phonemes[i])\n",
    "\n",
    "print(ret)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]\n",
      "  [0 6 0 0 9 0 5 6 0 9 9 0]]], shape=(1, 78, 12), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "spectrogram, labels = extractor.get_value()[0]\n",
    "\n",
    "logits = transducer((tf.convert_to_tensor([spectrogram], dtype=tf.float32), tf.convert_to_tensor([labels], dtype=tf.int32)))\n",
    "print(tf.argmax(logits, axis=-1))\n",
    "# print(logits[:, -30: , 0: ,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_to_sentence (x):\n",
    "  return ''.join([extractor.dataset.phoneme.phonemes[idx] for idx in x][1: ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rst = []\n",
    "x, y = 0, 0\n",
    "while x < logits.shape[2] and y < logits.shape[1]:\n",
    "  sel = tf.argmax(logits[0][y][x], axis=-1).numpy()\n",
    "  rst.append(sel)\n",
    "  if sel == 0:\n",
    "    y += 1\n",
    "  else:\n",
    "    x += 1\n",
    "\n",
    "print(rst)\n",
    "print(vector_to_sentence([0] + list(filter(lambda x: x != 0, rst))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "멜로 멜로\n"
     ]
    }
   ],
   "source": [
    "print(moasseugi(vector_to_sentence(labels)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ec62f2ec7c7d49224ec335e3ef472b8ef537dc7949933ed74784e8710c2a92fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
