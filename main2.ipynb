{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o/ 근데 칠십 퍼센트가 커 보이긴 하는데 이백 벌다 백 사십 벌면 빡셀걸? b/\n",
      "근데 삼 학년 때 까지는 국가장학금 바+ 받으면서 다녔던 건가?\n"
     ]
    }
   ],
   "source": [
    "test1 = \"o/ 근데 (70%)/(칠십 퍼센트)가 커 보이긴 하는데 (200)/(이백) 벌다 (140)/(백 사십) 벌면 빡셀걸? b/\"\n",
    "test2 = \"근데 (3학년)/(삼 학년) 때 까지는 국가장학금 바+ 받으면서 다녔던 건가?\"\n",
    "\n",
    "def bracket_filter(sentence):\n",
    "  new_sentence = str()\n",
    "  flag = False\n",
    "  \n",
    "  for ch in sentence:\n",
    "    if ch == '(' and flag == False:\n",
    "      flag = True\n",
    "      continue\n",
    "    if ch == '(' and flag == True:\n",
    "      flag = False\n",
    "      continue\n",
    "    if ch != ')' and flag == False:\n",
    "      new_sentence += ch\n",
    "  return new_sentence\n",
    "\n",
    "print(bracket_filter(test1))\n",
    "print(bracket_filter(test2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "근데 칠십 퍼센트가 커 보이긴 하는데 이백 벌다 백 사십 벌면 빡셀걸?\n",
      "c샾 배워봤어?\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "test1 = \"o/ 근데 칠십 퍼센트가 커 보이긴 하는데 이백 벌다 백 사십 벌면 빡셀걸? b/\"\n",
    "test2 = \"c# 배워봤어?\"\n",
    "\n",
    "def special_filter(sentence):\n",
    "  SENTENCE_MARK = ['?', '!']\n",
    "  NOISE = ['o', 'n', 'u', 'b', 'l']\n",
    "  EXCEPT = ['/', '+', '*', '-', '@', '$', '^', '&', '[', ']', '=', ':', ';', '.', ',']\n",
    "  \n",
    "  new_sentence = str()\n",
    "  for idx, ch in enumerate(sentence):\n",
    "    if ch not in SENTENCE_MARK:\n",
    "      # o/, n/ 등 처리\n",
    "      if idx + 1 < len(sentence) and ch in NOISE and sentence[idx+1] == '/': \n",
    "        continue \n",
    "\n",
    "    if ch == '#': \n",
    "      new_sentence += '샾'\n",
    "\n",
    "    elif ch not in EXCEPT: \n",
    "      new_sentence += ch\n",
    "\n",
    "  pattern = re.compile(r'\\s\\s+')\n",
    "  new_sentence = re.sub(pattern, ' ', new_sentence.strip())\n",
    "  return new_sentence\n",
    "\n",
    "print(special_filter(test1))\n",
    "print(special_filter(test2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아 나 쓰리 DS 갖고 싶다 쓰리 DS\n"
     ]
    }
   ],
   "source": [
    "# test = \"o/ 근데 (70%)/(칠십 퍼센트)가 커 보이긴 하는데 (200)/(이백) 벌다 (140)/(백 사십) 벌면 빡셀걸? b/\"\n",
    "test = 'b/ 아 나 (3DS)/(쓰리 DS) 갖고 싶다. (3DS)/(쓰리 DS)'\n",
    "\n",
    "def sentence_filter(raw_sentence):\n",
    "  return special_filter(bracket_filter(raw_sentence))\n",
    "\n",
    "print(sentence_filter(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosung = (\"ㄱ\", \"ㄲ\", \"ㄴ\", \"ㄷ\", \"ㄸ\", \"ㄹ\", \"ㅁ\", \"ㅂ\", \"ㅃ\", \"ㅅ\", \"ㅆ\", \"ㅇ\", \"ㅈ\", \"ㅉ\", \"ㅊ\", \"ㅋ\", \"ㅌ\", \"ㅍ\", \"ㅎ\")\n",
    "\n",
    "jungsung = (\"ㅏ\", \"ㅐ\", \"ㅑ\", \"ㅒ\", \"ㅓ\", \"ㅔ\", \"ㅕ\", \"ㅖ\", \"ㅗ\", \"ㅘ\", \"ㅙ\", \"ㅚ\", \"ㅛ\", \"ㅜ\", \"ㅝ\", \"ㅞ\", \"ㅟ\", \"ㅠ\", \"ㅡ\", \"ㅢ\", \"ㅣ\")\n",
    "\n",
    "jongsung = (\"\", \"ㄱ\", \"ㄲ\", \"ㄳ\", \"ㄴ\", \"ㄵ\", \"ㄶ\", \"ㄷ\", \"ㄹ\", \"ㄺ\", \"ㄻ\", \"ㄼ\", \"ㄽ\", \"ㄾ\", \"ㄿ\", \"ㅀ\", \"ㅁ\", \"ㅂ\", \"ㅄ\", \"ㅅ\", \"ㅆ\", \"ㅇ\", \"ㅈ\", \"ㅊ\", \"ㅋ\", \"ㅌ\", \"ㅍ\", \"ㅎ\")\n",
    "\n",
    "def isHangeul(one_character):\n",
    "    return 0xAC00 <= ord(one_character[:1]) <= 0xD7A3\n",
    "\n",
    "def hangeulExplode(one_hangeul):\n",
    "    a = one_hangeul[:1]\n",
    "    if isHangeul(a) != True:\n",
    "        return False\n",
    "    b = ord(a) - 0xAC00\n",
    "    cho = b // (21*28)\n",
    "    jung = b % (21*28) // 28\n",
    "    jong = b % 28\n",
    "    if jong == 0:\n",
    "        return (chosung[cho], jungsung[jung])\n",
    "    else:\n",
    "        return (chosung[cho], jungsung[jung], jongsung[jong])\n",
    "\n",
    "def hangeulJoin(inputlist):\n",
    "    result = \"\"\n",
    "    cho, jung, jong = 0, 0, 0\n",
    "    inputlist.insert(0, \"\")\n",
    "    while len(inputlist) > 1:\n",
    "        if inputlist[-1] in jongsung:\n",
    "            if inputlist[-2] in jungsung:\n",
    "                jong = jongsung.index(inputlist.pop())\n",
    "            \n",
    "            else:\n",
    "                result += inputlist.pop()\n",
    "        elif inputlist[-1] in jungsung:\n",
    "            if inputlist[-2] in chosung:\n",
    "                jung = jungsung.index(inputlist.pop())\n",
    "                cho = chosung.index(inputlist.pop())\n",
    "                result += chr(0xAC00 + ((cho*21)+jung)*28+jong)\n",
    "                cho, jung, jong = 0, 0, 0\n",
    "            else:\n",
    "                result += inputlist.pop()\n",
    "\n",
    "        else:\n",
    "            result += inputlist.pop()\n",
    "    else:\n",
    "        return result[::-1]\n",
    "\n",
    "def pureosseugi(inputtext):\n",
    "    result = \"\"\n",
    "    for i in inputtext:\n",
    "        if isHangeul(i) == True:\n",
    "            for j in hangeulExplode(i):\n",
    "                result += j\n",
    "        else:\n",
    "            result += i\n",
    "    \n",
    "    return result\n",
    "\n",
    "def moasseugi(inputtext):\n",
    "    t1 = []\n",
    "    for i in inputtext:\n",
    "        t1.append(i)\n",
    "\n",
    "    return hangeulJoin(t1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "\n",
    "class Phoneme:\n",
    "  def __init__ (self):\n",
    "    self.BLANK_INDEX = 0\n",
    "    self.START_INDEX = 1\n",
    "    self.END_INDEX = 2\n",
    "    self.SPACE_INDEX = 3\n",
    "    self.OOV_INDEX = 4\n",
    "\n",
    "class PhonemeGenerator (Phoneme):\n",
    "  def __init__ (self, labels):\n",
    "    super(PhonemeGenerator, self).__init__()\n",
    "    self.phonemes = ['<BLANK>', '<START>', '<END>', ' ']\n",
    "\n",
    "    texts = []\n",
    "    for label in labels:\n",
    "      for phoneme in label:\n",
    "        if not isHangeul(phoneme) and not phoneme == ' ' and not phoneme == '?':\n",
    "          break\n",
    "        for pure in pureosseugi(phoneme):\n",
    "          texts.append(pure)\n",
    "\n",
    "    tokenizer = Tokenizer(filters='', lower=False, oov_token='<OOV>')\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "\n",
    "    for i in range(1, len(tokenizer.index_word) + 1):\n",
    "      self.phonemes.append(tokenizer.index_word[i])\n",
    "\n",
    "    self.phoneme_index = {}\n",
    "    for i, phoneme in enumerate(self.phonemes):\n",
    "      self.phoneme_index[phoneme] = i\n",
    "    \n",
    "    self.VOCAB_SIZE = len(self.phonemes)\n",
    "  \n",
    "  def save (self, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "      pickle.dump((self.phonemes, self.phoneme_index), f)\n",
    "\n",
    "\n",
    "class PhonemeLoader (Phoneme):\n",
    "  def __init__ (self, filename):\n",
    "    super(PhonemeLoader, self).__init__()\n",
    "    with open(filename, 'rb') as f:\n",
    "      self.phonemes, self.phoneme_index = pickle.load(f)\n",
    "\n",
    "    self.VOCAB_SIZE = len(self.phonemes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akswnd98\\anaconda3\\lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "class Dataset:\n",
    "  def get_data (self, idx):\n",
    "    return (\n",
    "      self.extract_spectrogram(self.file_phoneme_vector_pairs[idx][0]),\n",
    "      [0] + self.file_phoneme_vector_pairs[idx][1]\n",
    "    )\n",
    "  \n",
    "  def get_len (self):\n",
    "    return len(self.file_phoneme_vector_pairs)\n",
    "\n",
    "  def extract_spectrogram (self, filename):\n",
    "    waveform = np.memmap(filename, dtype='h', mode='r')\n",
    "\n",
    "    sr = 16000\n",
    "    st = 1 / sr\n",
    "\n",
    "    t = np.linspace(0, waveform.shape[0] * st, waveform.shape[0])\n",
    "\n",
    "    S = librosa.feature.melspectrogram(y=np.float32(waveform), n_fft=512, hop_length=256, window='hamming', sr=sr, n_mels=128)\n",
    "    log_S = librosa.power_to_db(S, ref=np.max)\n",
    "    log_S = np.transpose(log_S)\n",
    "\n",
    "    return log_S\n",
    "  \n",
    "  def save (self, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "      pickle.dump((self.phoneme, self.file_phoneme_vector_pairs), f)\n",
    "\n",
    "class RandomDataset (Dataset):\n",
    "  def __init__ (self):\n",
    "    super(RandomDataset, self).__init__()\n",
    "    with open('./ko-audio-dataset/sentence-script/KsponSpeech_scripts/train.trn', 'r', encoding='utf-8') as file:\n",
    "      lines = file.readlines()\n",
    "    def process_line (line):\n",
    "      line = line.split(' :: ')\n",
    "      line[0] = os.path.join('./ko-audio-dataset/audio/', line[0])\n",
    "      line[1] = sentence_filter(line[1])\n",
    "      return line\n",
    "    file_label_pairs = list(map(process_line, lines))\n",
    "    file_label_pairs = list(filter(lambda pair: not self.check_skip_label(pair[1]), file_label_pairs))\n",
    "    self.phoneme = PhonemeGenerator([label for file, label in file_label_pairs])\n",
    "    file_phoneme_pairs = list(map(lambda pair: (pair[0], pureosseugi(pair[1])), file_label_pairs))\n",
    "    self.file_phoneme_vector_pairs = [[pair[0], [self.phoneme.phoneme_index[phoneme] for phoneme in pair[1]]] for pair in file_phoneme_pairs]\n",
    "    random.shuffle(self.file_phoneme_vector_pairs)\n",
    "  \n",
    "  def check_skip_hangul (self, hangul):\n",
    "    return not isHangeul(hangul) and not hangul == ' ' and not hangul == '?'\n",
    "\n",
    "  def check_skip_label (self, label):\n",
    "    for hangul in label:\n",
    "      if self.check_skip_hangul(hangul):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "class LoadedDataset (Dataset):\n",
    "  def __init__ (self, filename):\n",
    "    super(LoadedDataset, self).__init__()\n",
    "    with open(filename, 'rb') as f:\n",
    "      self.phoneme, self.file_phoneme_vector_pairs = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = RandomDataset()\n",
    "dataset.save('dataset2.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LoadedDataset('dataset2.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetSizeLimitingIterator:\n",
    "  def __init__ (self, dataset, limit, idx):\n",
    "    self.dataset = dataset\n",
    "    self.limit = limit\n",
    "    self.idx = idx\n",
    "    while self.idx < self.dataset.get_len():\n",
    "      data = self.dataset.get_data(self.idx)\n",
    "      if data[0].shape[0] * len(data[1]) * len(self.dataset.phoneme.phonemes) <= self.limit:\n",
    "        break\n",
    "      self.idx += 1\n",
    "\n",
    "  def next (self):\n",
    "    idx = self.idx + 1\n",
    "    while idx < self.dataset.get_len():\n",
    "      data = self.dataset.get_data(idx)\n",
    "      if data[0].shape[0] * len(data[1]) * len(self.dataset.phoneme.phonemes) <= self.limit:\n",
    "        break\n",
    "      idx += 1\n",
    "    return DatasetSizeLimitingIterator(self.dataset, self.limit, idx)\n",
    "  \n",
    "  def check_end (self):\n",
    "    return self.idx >= self.dataset.get_len()\n",
    "\n",
    "  def get_value (self):\n",
    "    return self.dataset.get_data(self.idx)\n",
    "\n",
    "  def save (self, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "      pickle.dump((self.limit, self.idx), f)\n",
    "\n",
    "class IteratorLoader:\n",
    "  @staticmethod\n",
    "  def load (dataset, filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "      limit, idx = pickle.load(f)\n",
    "    return DatasetSizeLimitingIterator(dataset, limit, idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = DatasetSizeLimitingIterator(dataset, 100000, 0)\n",
    "iterator.save('iterator2.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = IteratorLoader.load(dataset, 'iterator2.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class DatasetStepExtractorLoader:\n",
    "  @staticmethod\n",
    "  def load (path):\n",
    "    dataset = LoadedDataset(os.path.join(path, 'dataset.pickle'))\n",
    "    return SavableDatasetStepExtractor(\n",
    "      dataset,\n",
    "      pickle.load(open(os.path.join(path, 'step_size.pickle'), 'rb')),\n",
    "      IteratorLoader.load(dataset, os.path.join(path, 'iterator.pickle'))\n",
    "    )\n",
    "\n",
    "class DatasetStepExtractor:\n",
    "  def __init__ (self, dataset, step_size, iterator):\n",
    "    self.dataset = dataset\n",
    "    self.step_size = step_size\n",
    "    self.value = []\n",
    "    self.iterator = iterator\n",
    "    while not iterator.check_end() and len(self.value) < self.step_size:\n",
    "      self.value.append(iterator.get_value())\n",
    "      iterator = iterator.next()\n",
    "    self.is_end = len(self.value) < self.step_size\n",
    "    self.next_iterator = iterator\n",
    "  \n",
    "  def check_end (self):\n",
    "    return self.is_end\n",
    "  \n",
    "  def get_value (self):\n",
    "    return self.value\n",
    "  \n",
    "  def get_next_extractor (self):\n",
    "    return DatasetStepExtractor(self.dataset, self.step_size, self.next_iterator)\n",
    "\n",
    "class SavableDatasetStepExtractor (DatasetStepExtractor):\n",
    "  def save (self, saver):\n",
    "    saver.save(self.dataset, self.step_size, self.iterator)\n",
    "\n",
    "class DatasetStepExtractorSaver:\n",
    "  def __init__ (self, path):\n",
    "    self.path = path\n",
    "  \n",
    "  def save (self, dataset, step_size, iterator):\n",
    "    os.makedirs(self.path, exist_ok=True)\n",
    "\n",
    "class IteratorSaver (DatasetStepExtractorSaver):\n",
    "  def save (self, dataset, step_size, iterator):\n",
    "    super().save(dataset, step_size, iterator)\n",
    "    iterator.save(os.path.join(self.path, 'iterator.pickle'))\n",
    "  \n",
    "class DatasetSaver (DatasetStepExtractorSaver):\n",
    "  def save (self, dataset, step_size, iterator):\n",
    "    super().save(dataset, step_size, iterator)\n",
    "    dataset.save(os.path.join(self.path, 'dataset.pickle'))\n",
    "\n",
    "class StepSizeSaver (DatasetStepExtractorSaver):\n",
    "  def save (self, dataset, step_size, iterator):\n",
    "    super().save(dataset, step_size, iterator)\n",
    "    with open(os.path.join(self.path, 'step_size.pickle'), 'wb') as f:\n",
    "      pickle.dump(step_size, f)\n",
    "\n",
    "class DatasetStepExtractorSaver (DatasetStepExtractorSaver):\n",
    "  def save (self, dataset, step_size, iterator):\n",
    "    super().save(dataset, step_size, iterator)\n",
    "    iterator.save(os.path.join(self.path, 'iterator.pickle'))\n",
    "    dataset.save(os.path.join(self.path, 'dataset.pickle'))\n",
    "    with open(os.path.join(self.path, 'step_size.pickle'), 'wb') as f:\n",
    "      pickle.dump(step_size, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = SavableDatasetStepExtractor(dataset, 100, iterator)\n",
    "extractor.save(DatasetStepExtractorSaver('test_extractor2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = DatasetStepExtractorLoader.load('test_extractor2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "BATCH_CNT = 1\n",
    "STEP_SIZE = BATCH_SIZE * BATCH_CNT\n",
    "SIZE_LIMIT = 2000000\n",
    "extractor = SavableDatasetStepExtractor(dataset, STEP_SIZE, DatasetSizeLimitingIterator(dataset, SIZE_LIMIT, 0))\n",
    "extractor.save(DatasetStepExtractorSaver('extractor2'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extractor = DatasetStepExtractorLoader.load('extractor2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ 여기 까지 dataset 부분 ###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnnt_loss (logits, labels, time_lengths, label_lengths):\n",
    "  log_pr = tf.math.log_softmax(logits, axis=-1)\n",
    "  pr = pr_loss(log_pr, labels, time_lengths, label_lengths)\n",
    "  ret = tf.reduce_sum(pr)\n",
    "  return ret\n",
    "\n",
    "@tf.custom_gradient\n",
    "def pr_loss (log_pr, labels, time_lengths, label_lengths):\n",
    "  LOG_0 = float('-inf')\n",
    "  batch_size = log_pr.shape[0]\n",
    "  max_time_lengths = log_pr.shape[1]\n",
    "  max_label_lengths = log_pr.shape[2]\n",
    "\n",
    "  def get_truth_log_pr (log_pr, labels):\n",
    "    labels_one_hot = tf.one_hot(labels[:, 1: ], tf.shape(log_pr)[-1], axis=-1, dtype=tf.float32)\n",
    "    labels_one_hot = tf.expand_dims(labels_one_hot, axis=1)\n",
    "    labels_one_hot = tf.repeat(labels_one_hot, tf.shape(log_pr)[1], axis=1)\n",
    "    ret = tf.reduce_sum(log_pr[:, :, : -1, :] * labels_one_hot, axis=-1)\n",
    "    ret = tf.concat([ret, LOG_0 * tf.ones((tf.shape(log_pr)[0], tf.shape(log_pr)[1], 1))], axis = -1)\n",
    "    return ret\n",
    "  \n",
    "  def get_blank_log_pr (log_pr):\n",
    "    return log_pr[:, :, :, 0]\n",
    "\n",
    "  truth_log_pr = get_truth_log_pr(log_pr, labels)\n",
    "  blank_log_pr = get_blank_log_pr(log_pr)\n",
    "\n",
    "  def get_alpha (truth_log_pr, blank_log_pr):\n",
    "    reversed_truth_log_pr = tf.reverse(truth_log_pr, axis=[-2])\n",
    "    padded_truth_log_pr = tf.pad(\n",
    "      reversed_truth_log_pr,\n",
    "      [[0, 0], [0, 0], [tf.shape(reversed_truth_log_pr)[-2] - 1, 0]],\n",
    "      constant_values=LOG_0\n",
    "    )\n",
    "    truth_diag = tf.linalg.diag_part(\n",
    "      padded_truth_log_pr,\n",
    "      k=(0, tf.shape(padded_truth_log_pr)[-1] - 1),\n",
    "      padding_value=LOG_0,\n",
    "      align='LEFT_RIGHT'\n",
    "    )\n",
    "    truth_diag = tf.reverse(truth_diag, axis=[-2])\n",
    "    truth_diag = tf.pad(truth_diag, [[0, 0], [0, 0], [1, 0]], constant_values=LOG_0)\n",
    "\n",
    "    reversed_blank_log_pr = tf.reverse(blank_log_pr, axis=[-2])\n",
    "    padded_blank_log_pr = tf.pad(\n",
    "      reversed_blank_log_pr,\n",
    "      [[0, 0], [0, 0], [tf.shape(reversed_blank_log_pr)[-2] - 1, 0]],\n",
    "      constant_values=LOG_0\n",
    "    )\n",
    "    blank_diag = tf.linalg.diag_part(\n",
    "      padded_blank_log_pr,\n",
    "      k=(0, tf.shape(padded_blank_log_pr)[-1] - 1),\n",
    "      padding_value=LOG_0,\n",
    "      align='LEFT_RIGHT'\n",
    "    )\n",
    "    blank_diag = tf.reverse(blank_diag, axis=[-2])\n",
    "    blank_diag = tf.pad(blank_diag, [[0, 0], [0, 0], [0, 1]], constant_values=LOG_0)\n",
    "\n",
    "    initial_diag = tf.concat([\n",
    "      LOG_0 * tf.ones((tf.shape(blank_diag)[0], tf.shape(blank_diag)[-1] - 2), tf.float32),\n",
    "      tf.zeros((tf.shape(blank_diag)[0], 1), dtype=tf.float32)\n",
    "    ], axis=-1)\n",
    "\n",
    "    def step (a, x):\n",
    "      t, b = x\n",
    "      return (\n",
    "        tf.reduce_logsumexp(\n",
    "          tf.stack([\n",
    "            tf.concat([LOG_0 * tf.ones((tf.shape(a)[0], 1), dtype=tf.float32), a], axis=-1) + t,\n",
    "            tf.concat([a, LOG_0 * tf.ones((tf.shape(a)[0], 1), dtype=tf.float32)], axis=-1) + b\n",
    "          ]), axis=0\n",
    "        )[:, 1: ]\n",
    "      )\n",
    "\n",
    "    alpha_diag = tf.concat([\n",
    "      tf.expand_dims(initial_diag, axis=0),\n",
    "      tf.scan(step, (tf.transpose(truth_diag, perm=[1, 0, 2]), tf.transpose(blank_diag, perm=[1, 0, 2])), initial_diag)\n",
    "    ], axis=0)\n",
    "\n",
    "    alpha_diag = tf.transpose(alpha_diag, perm=[1, 0, 2])\n",
    "\n",
    "    alpha_diag = tf.reverse(alpha_diag, axis=[-1])\n",
    "\n",
    "    alpha_diag = tf.transpose(alpha_diag, perm=[0, 2, 1])\n",
    "    alpha = tf.linalg.diag_part(alpha_diag, k=(0, max_label_lengths - 1))\n",
    "    alpha = tf.reshape(alpha, (tf.shape(truth_log_pr)[0], tf.shape(truth_log_pr)[2], tf.shape(truth_log_pr)[1]))\n",
    "    alpha = tf.reverse(alpha, axis=[-2])\n",
    "    alpha = tf.transpose(alpha, perm=[0, 2, 1])\n",
    "    return alpha\n",
    "\n",
    "  def get_beta (truth_log_pr, blank_log_pr):\n",
    "    reversed_truth_log_pr = tf.reverse(truth_log_pr, axis=[-2])\n",
    "    reversed_truth_log_pr = tf.concat([\n",
    "      reversed_truth_log_pr[:, :, : -1],\n",
    "      LOG_0 * tf.ones((tf.shape(reversed_truth_log_pr)[0], tf.shape(reversed_truth_log_pr)[-2], 1), dtype=tf.float32)\n",
    "    ], axis=-1)\n",
    "    padded_truth_log_pr = tf.pad(\n",
    "      reversed_truth_log_pr,\n",
    "      [[0, 0], [0, 0], [tf.shape(reversed_truth_log_pr)[-2] - 1, 0]],\n",
    "      constant_values=LOG_0\n",
    "    )\n",
    "    truth_diag = tf.linalg.diag_part(\n",
    "      padded_truth_log_pr,\n",
    "      k=(0, tf.shape(padded_truth_log_pr)[-1] - 1),\n",
    "      padding_value=LOG_0,\n",
    "      align='LEFT_RIGHT'\n",
    "    )\n",
    "    truth_diag = tf.transpose(truth_diag, perm=[1, 0, 2])\n",
    "\n",
    "    reversed_blank_log_pr = tf.reverse(blank_log_pr, axis=[-2])\n",
    "    reversed_blank_log_pr = tf.concat([\n",
    "      LOG_0 * tf.ones((tf.shape(reversed_blank_log_pr)[0], 1, tf.shape(reversed_blank_log_pr)[-1]), dtype=tf.float32),\n",
    "      reversed_blank_log_pr[:, 1:, :]\n",
    "    ], axis=-2)\n",
    "    padded_blank_log_pr = tf.pad(\n",
    "      reversed_blank_log_pr,\n",
    "      [[0, 0], [0, 0], [tf.shape(reversed_blank_log_pr)[-2] - 1, 0]],\n",
    "      constant_values=LOG_0\n",
    "    )\n",
    "    blank_diag = tf.linalg.diag_part(\n",
    "      padded_blank_log_pr,\n",
    "      k=(0, tf.shape(padded_blank_log_pr)[-1] - 1),\n",
    "      padding_value=LOG_0,\n",
    "      align='LEFT_RIGHT'\n",
    "    )\n",
    "    blank_diag = tf.transpose(blank_diag, perm=[1, 0, 2])\n",
    "\n",
    "    initial_diag = tf.concat([\n",
    "      tf.reshape(blank_log_pr[:, -1, -1], (tf.shape(blank_log_pr)[0], 1)),\n",
    "      # -0.5 * tf.ones((tf.shape(blank_diag)[0], 1), dtype=tf.float32),\n",
    "      LOG_0 * tf.ones((tf.shape(blank_log_pr)[0], tf.shape(blank_log_pr)[-2] - 1), tf.float32)\n",
    "    ], axis=-1)\n",
    "\n",
    "    def step (a, x):\n",
    "      t, b = x\n",
    "      return (\n",
    "        tf.reduce_logsumexp(\n",
    "          tf.stack([\n",
    "            a + t,\n",
    "            tf.concat([LOG_0 * tf.ones((tf.shape(a)[0], 1), dtype=tf.float32), a[:, : -1]], axis=-1) + b\n",
    "          ]), axis=0\n",
    "        )\n",
    "      )\n",
    "\n",
    "    beta_diag = tf.concat([\n",
    "      tf.expand_dims(initial_diag, axis=0),\n",
    "      tf.scan(step, (truth_diag[1: ], blank_diag[1: ]), initial_diag)\n",
    "    ], axis=0)\n",
    "\n",
    "    beta_diag = tf.transpose(beta_diag, [1, 0, 2])\n",
    "\n",
    "    beta = tf.linalg.diag_part(\n",
    "      beta_diag,\n",
    "      k=(-max_label_lengths + 1, 0),\n",
    "      padding_value=LOG_0,\n",
    "      align='LEFT_RIGHT'\n",
    "    )\n",
    "    beta = tf.reshape(beta, (tf.shape(truth_log_pr)[0], tf.shape(truth_log_pr)[2], tf.shape(truth_log_pr)[1]))\n",
    "    beta = tf.transpose(beta, [0, 2, 1])\n",
    "    beta = tf.reverse(beta, axis=[1, 2])\n",
    "    return beta\n",
    "\n",
    "  alpha = get_alpha(truth_log_pr, blank_log_pr)\n",
    "  beta = get_beta(truth_log_pr, blank_log_pr)\n",
    "\n",
    "  total_log_pr = beta[:, 0, 0]\n",
    "\n",
    "  def grad (upstream):\n",
    "    blank_grads = \\\n",
    "      alpha + tf.concat([\n",
    "        beta[:, 1:, :],\n",
    "        tf.pad(LOG_0 * tf.ones((tf.shape(beta)[0], 1, tf.shape(beta)[-1] - 1), dtype=tf.float32), [[0, 0], [0, 0], [0, 1]], constant_values=0)\n",
    "      ], axis=1) \\\n",
    "      - tf.reshape(total_log_pr, shape=(tf.shape(total_log_pr)[0], 1, 1))\n",
    "    truth_grads = \\\n",
    "      alpha + tf.pad(beta[:, :, 1: ], [[0, 0], [0, 0], [0, 1]], constant_values=LOG_0) \\\n",
    "      - tf.reshape(total_log_pr, shape=(tf.shape(total_log_pr)[0], 1, 1))\n",
    "    blank_one_hot = tf.one_hot(tf.zeros_like(labels, dtype=tf.int32), tf.shape(log_pr)[-1], dtype=tf.float32)\n",
    "    blank_one_hot = tf.expand_dims(blank_one_hot, axis=1)\n",
    "    blank_one_hot = tf.repeat(blank_one_hot, tf.shape(log_pr)[1], axis=1)\n",
    "    truth_one_hot = tf.one_hot(labels[:, 1: ], tf.shape(log_pr)[-1], dtype=tf.float32)\n",
    "    truth_one_hot = tf.concat([truth_one_hot, tf.zeros((tf.shape(log_pr)[0], 1, tf.shape(log_pr)[-1]), dtype=tf.float32)], axis=-2)\n",
    "    truth_one_hot = tf.expand_dims(truth_one_hot, axis=1)\n",
    "    truth_one_hot = tf.repeat(truth_one_hot, tf.shape(log_pr)[1], axis=1)\n",
    "    return (\n",
    "      [\n",
    "        tf.reshape(-upstream, shape=(tf.shape(upstream)[0], 1, 1))\n",
    "        * (\n",
    "          tf.exp(tf.expand_dims(blank_grads, axis=-1) + log_pr) * blank_one_hot\n",
    "          + tf.exp(tf.expand_dims(truth_grads, axis=-1) + log_pr) * truth_one_hot\n",
    "        )\n",
    "      ] +\n",
    "      [None] * 3\n",
    "    )\n",
    "\n",
    "  return -total_log_pr, grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 256\n",
    "UNITS = 1024\n",
    "CODER_OUTPUT_DIM = 512\n",
    "JOINT_NET_INNER_DIM = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder (tf.keras.Model):\n",
    "  def __init__ (self, units, output_dim):\n",
    "    super(Encoder, self).__init__(self)\n",
    "    self.gru = tf.keras.layers.GRU(units, recurrent_initializer='glorot_uniform', return_sequences=True, return_state=True)\n",
    "    self.fc = tf.keras.layers.Dense(output_dim)\n",
    "    \n",
    "  def call (self, inputs):\n",
    "    rnn_outputs, hidden_states = self.gru(inputs)\n",
    "    outputs = self.fc(rnn_outputs)\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder (tf.keras.Model):\n",
    "  def __init__ (self, vocab_size, embedding_dim, units, output_dim):\n",
    "    super(Decoder, self).__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(units, recurrent_initializer='glorot_uniform', return_sequences=True, return_state=True)\n",
    "    self.fc = tf.keras.layers.Dense(output_dim)\n",
    "\n",
    "  def call (self, inputs):\n",
    "    embedded = self.embedding(inputs)\n",
    "    outputs, hidden_states = self.gru(embedded)\n",
    "    outputs = self.fc(outputs)\n",
    "    return outputs, hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointNet (tf.keras.Model):\n",
    "  def __init__ (self, inner_dim, vocab_size):\n",
    "    super(JointNet, self).__init__()\n",
    "    self.forward_layer = tf.keras.layers.Dense(inner_dim, activation='tanh')\n",
    "    self.project_layer = tf.keras.layers.Dense(vocab_size)\n",
    "  \n",
    "  def call (self, inputs):\n",
    "    enc_outputs, dec_outputs = inputs\n",
    "    joint_inputs = tf.expand_dims(enc_outputs, axis=2) + tf.expand_dims(dec_outputs, axis=1)\n",
    "    outputs = self.forward_layer(joint_inputs)\n",
    "    outputs = self.project_layer(outputs)\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transducer (tf.keras.Model):\n",
    "  def __init__ (self, encoder, decoder, joint_net):\n",
    "    super(Transducer, self).__init__()\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "    self.joint_net = joint_net\n",
    "\n",
    "  def __init__ (self, embedding_dim, units, coder_output_dim, joint_net_inner_dim, vocab_size):\n",
    "    super(Transducer, self).__init__()\n",
    "    self.encoder = Encoder(units, coder_output_dim)\n",
    "    self.decoder = Decoder(vocab_size, embedding_dim, units, coder_output_dim)\n",
    "    self.joint_net = JointNet(joint_net_inner_dim, vocab_size)\n",
    "    self.vocab_size = vocab_size\n",
    "  \n",
    "  def call (self, inputs):\n",
    "    enc_inputs, dec_inputs = inputs\n",
    "    enc_outputs = self.encoder(enc_inputs)\n",
    "    dec_outputs, dec_states = self.decoder(dec_inputs)\n",
    "    outputs = self.joint_net([enc_outputs, dec_outputs])\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "class Trainer:\n",
    "  def __init__ (self, transducer, optimizer, batch_size, dataset_extractor, save_time_delta):\n",
    "    self.transducer = transducer\n",
    "    self.optimizer = optimizer\n",
    "    self.batch_size = batch_size\n",
    "    self.dataset_extractor = dataset_extractor\n",
    "    self.save_time_delta = save_time_delta\n",
    "\n",
    "  def train_step (self):\n",
    "    datas = self.dataset_extractor.get_value()\n",
    "    acc_grads = [tf.zeros_like(var) for var in self.transducer.trainable_variables]\n",
    "    acc_loss = 0\n",
    "    for i in range(0, len(datas), self.batch_size):\n",
    "      batch_datas = datas[i: i + self.batch_size]\n",
    "      label_lengths = self.get_label_lengths(batch_datas)\n",
    "      label_lengths = tf.convert_to_tensor(label_lengths, dtype=tf.int32)\n",
    "      labels = pad_sequences(list(map(lambda x: x[1], batch_datas)), maxlen=self.get_labels_maxlen(batch_datas), truncating='post', padding='post')\n",
    "      labels = list(map(lambda x: x[1], batch_datas))\n",
    "      labels = tf.constant(labels, dtype=tf.int32)\n",
    "      spectrograms = [x[0] for x in batch_datas]\n",
    "      spectrogram_lengths = self.get_spectrogram_lengths(spectrograms)\n",
    "      spectrogram_lengths = tf.convert_to_tensor(spectrogram_lengths, dtype=tf.int32)\n",
    "      spectrograms = tf.cast(pad_sequences(spectrograms, maxlen=self.get_spectrograms_maxlen(spectrograms), truncating='post', padding='post'), dtype=tf.float32)\n",
    "      with tf.GradientTape() as tape:\n",
    "        logits = self.transducer([spectrograms, labels])\n",
    "        loss = rnnt_loss(\n",
    "          logits,\n",
    "          labels,\n",
    "          spectrogram_lengths,\n",
    "          label_lengths\n",
    "        )\n",
    "      acc_loss += loss.numpy()\n",
    "      grads = tape.gradient(loss, self.transducer.trainable_variables)\n",
    "      acc_grads = [acc_grad + grad for acc_grad, grad in zip(acc_grads, grads)]\n",
    "    acc_grads = [acc_grad / self.dataset_extractor.step_size for acc_grad in acc_grads]\n",
    "    self.optimizer.apply_gradients(zip(acc_grads, self.transducer.trainable_variables))\n",
    "    self.dataset_extractor = self.dataset_extractor.get_next_extractor()\n",
    "    extractor.save(IteratorSaver('extractor2'))\n",
    "    \n",
    "    return acc_loss / self.dataset_extractor.step_size\n",
    "\n",
    "  def train (self):\n",
    "    loss = 0\n",
    "    cnt = 0\n",
    "    prev_save_time = 0\n",
    "    cur_block_size = 0\n",
    "    while not self.dataset_extractor.check_end():\n",
    "      # loss = self.train_step()\n",
    "      # cnt += self.dataset_extractor.step_size\n",
    "      loss += self.train_step()\n",
    "      cnt += self.dataset_extractor.step_size\n",
    "      if cnt % 100 == 0:\n",
    "        print('cnt: {}'.format(cnt))\n",
    "        print('loss: {}'.format(loss / 100))\n",
    "        loss = 0\n",
    "      if cnt >= prev_save_time + self.save_time_delta:\n",
    "        self.transducer.save_weights('./checkpoints/no{}/ckpt'.format(cnt))\n",
    "        prev_save_time = cnt\n",
    "\n",
    "  def get_labels_maxlen (self, batch_datas):\n",
    "    ret = 0\n",
    "    for file_label_vector_pair in batch_datas:\n",
    "      ret = max(ret, len(file_label_vector_pair[1]))\n",
    "    return ret\n",
    "  \n",
    "  def get_label_lengths (self, batch_datas):\n",
    "    ret = list(map(lambda x: len(x[1]), batch_datas))\n",
    "    return ret\n",
    "  \n",
    "  def get_spectrograms_maxlen (self, spectrograms):\n",
    "    ret = 0\n",
    "    for spectrogram in spectrograms:\n",
    "      ret = max(ret, spectrogram.shape[0])\n",
    "    return ret\n",
    "\n",
    "  def get_spectrogram_lengths (self, spectrograms):\n",
    "    ret = list(map(lambda x: x.shape[0], spectrograms))\n",
    "    return ret\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = dataset.phoneme.VOCAB_SIZE\n",
    "SAVE_TIME_DELTA = 5000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transducer\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder (Encoder)           multiple                  4069888   \n",
      "                                                                 \n",
      " decoder (Decoder)           multiple                  4477440   \n",
      "                                                                 \n",
      " joint_net (JointNet)        multiple                  582712    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,130,040\n",
      "Trainable params: 9,130,040\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "cnt: 100\n",
      "loss: 149.0256834411621\n",
      "cnt: 200\n",
      "loss: 127.6614616394043\n",
      "cnt: 300\n",
      "loss: 142.0180096912384\n",
      "cnt: 400\n",
      "loss: 127.00405046463013\n",
      "cnt: 500\n",
      "loss: 126.21211320400238\n",
      "cnt: 600\n",
      "loss: 121.41114798545837\n",
      "cnt: 700\n",
      "loss: 111.66791275978089\n",
      "cnt: 800\n",
      "loss: 118.10227319717407\n",
      "cnt: 900\n",
      "loss: 105.90308545351029\n",
      "cnt: 1000\n",
      "loss: 139.45795241355896\n",
      "cnt: 1100\n",
      "loss: 122.65191409111023\n",
      "cnt: 1200\n",
      "loss: 135.39421362400054\n",
      "cnt: 1300\n",
      "loss: 128.46074142217637\n",
      "cnt: 1400\n",
      "loss: 137.9222071170807\n",
      "cnt: 1500\n",
      "loss: 103.86527417421341\n",
      "cnt: 1600\n",
      "loss: 114.82646808624267\n",
      "cnt: 1700\n",
      "loss: 123.81878952980041\n",
      "cnt: 1800\n",
      "loss: 118.50532117843628\n",
      "cnt: 1900\n",
      "loss: 121.50115092277527\n",
      "cnt: 2000\n",
      "loss: 124.5372604560852\n",
      "cnt: 2100\n",
      "loss: 131.35584276199342\n",
      "cnt: 2200\n",
      "loss: 127.76849672317505\n",
      "cnt: 2300\n",
      "loss: 115.86850858688355\n",
      "cnt: 2400\n",
      "loss: 126.34999019622802\n",
      "cnt: 2500\n",
      "loss: 118.31378657102584\n",
      "cnt: 2600\n",
      "loss: 124.14938404083252\n",
      "cnt: 2700\n",
      "loss: 119.68164223670959\n",
      "cnt: 2800\n",
      "loss: 123.70934673309326\n",
      "cnt: 2900\n",
      "loss: 136.15096243143083\n",
      "cnt: 3000\n",
      "loss: 126.4869522857666\n",
      "cnt: 3100\n",
      "loss: 119.55110635757447\n",
      "cnt: 3200\n",
      "loss: 113.55041897296906\n",
      "cnt: 3300\n",
      "loss: 130.39586720466613\n",
      "cnt: 3400\n",
      "loss: 134.60218994617463\n",
      "cnt: 3500\n",
      "loss: 140.53537152290343\n",
      "cnt: 3600\n",
      "loss: 122.90375318527222\n",
      "cnt: 3700\n",
      "loss: 117.79360858678818\n",
      "cnt: 3800\n",
      "loss: 122.04848861694336\n",
      "cnt: 3900\n",
      "loss: 126.30518607139588\n",
      "cnt: 4000\n",
      "loss: 129.2407234811783\n",
      "cnt: 4100\n",
      "loss: 124.09753270149231\n",
      "cnt: 4200\n",
      "loss: 120.2742738366127\n",
      "cnt: 4300\n",
      "loss: 116.30136421442032\n",
      "cnt: 4400\n",
      "loss: 124.82463791370392\n",
      "cnt: 4500\n",
      "loss: 124.26494004249572\n",
      "cnt: 4600\n",
      "loss: 122.88082790374756\n",
      "cnt: 4700\n",
      "loss: 121.43855386972427\n",
      "cnt: 4800\n",
      "loss: 113.61907603263855\n",
      "cnt: 4900\n",
      "loss: 115.01757349014282\n",
      "cnt: 5000\n",
      "loss: 113.03823878288269\n",
      "cnt: 5100\n",
      "loss: 112.9874397945404\n",
      "cnt: 5200\n",
      "loss: 107.32014258384704\n",
      "cnt: 5300\n",
      "loss: 118.71597066879272\n",
      "cnt: 5400\n",
      "loss: 104.67750933647156\n",
      "cnt: 5500\n",
      "loss: 110.83362472534179\n",
      "cnt: 5600\n",
      "loss: 105.0911388015747\n",
      "cnt: 5700\n",
      "loss: 119.83386535644532\n",
      "cnt: 5800\n",
      "loss: 112.4285518193245\n",
      "cnt: 5900\n",
      "loss: 111.30160395145417\n",
      "cnt: 6000\n",
      "loss: 108.61806723833084\n",
      "cnt: 6100\n",
      "loss: 118.68743598937988\n",
      "cnt: 6200\n",
      "loss: 116.89824794769287\n",
      "cnt: 6300\n",
      "loss: 104.3603705739975\n",
      "cnt: 6400\n",
      "loss: 111.45553141117097\n",
      "cnt: 6500\n",
      "loss: 119.41617580413818\n",
      "cnt: 6600\n",
      "loss: 100.55658622264862\n",
      "cnt: 6700\n",
      "loss: 115.54318730354309\n",
      "cnt: 6800\n",
      "loss: 112.21986315727234\n",
      "cnt: 6900\n",
      "loss: 116.36149397134781\n",
      "cnt: 7000\n",
      "loss: 115.0883385181427\n",
      "cnt: 7100\n",
      "loss: 123.09852822303772\n",
      "cnt: 7200\n",
      "loss: 106.10830624580383\n",
      "cnt: 7300\n",
      "loss: 108.7218314743042\n",
      "cnt: 7400\n",
      "loss: 126.56710327625275\n",
      "cnt: 7500\n",
      "loss: 106.63072279453277\n",
      "cnt: 7600\n",
      "loss: 118.83933012962342\n",
      "cnt: 7700\n",
      "loss: 125.35950309753417\n",
      "cnt: 7800\n",
      "loss: 108.39672300338745\n",
      "cnt: 7900\n",
      "loss: 115.00395583629609\n",
      "cnt: 8000\n",
      "loss: 115.62318787097931\n",
      "cnt: 8100\n",
      "loss: 107.47742938995361\n",
      "cnt: 8200\n",
      "loss: 109.91382702827454\n",
      "cnt: 8300\n",
      "loss: 102.59075600624084\n",
      "cnt: 8400\n",
      "loss: 116.62491330623627\n",
      "cnt: 8500\n",
      "loss: 108.95355086326599\n",
      "cnt: 8600\n",
      "loss: 112.99092080116272\n",
      "cnt: 8700\n",
      "loss: 105.77302306175233\n",
      "cnt: 8800\n",
      "loss: 119.70151807308197\n",
      "cnt: 8900\n",
      "loss: 110.94389045715332\n",
      "cnt: 9000\n",
      "loss: 109.0908449935913\n",
      "cnt: 9100\n",
      "loss: 116.12024316787719\n",
      "cnt: 9200\n",
      "loss: 118.30376211166381\n",
      "cnt: 9300\n",
      "loss: 102.73795882701874\n",
      "cnt: 9400\n",
      "loss: 113.26239193916321\n",
      "cnt: 9500\n",
      "loss: 106.739056224823\n",
      "cnt: 9600\n",
      "loss: 118.96315483093262\n",
      "cnt: 9700\n",
      "loss: 112.32818756103515\n",
      "cnt: 9800\n",
      "loss: 114.74373626232148\n",
      "cnt: 9900\n",
      "loss: 101.517670378685\n",
      "cnt: 10000\n",
      "loss: 118.22471263408661\n",
      "cnt: 10100\n",
      "loss: 110.6625023984909\n",
      "cnt: 10200\n",
      "loss: 111.7086086845398\n",
      "cnt: 10300\n",
      "loss: 102.61078372001649\n",
      "cnt: 10400\n",
      "loss: 114.60043382644653\n",
      "cnt: 10500\n",
      "loss: 114.82948908805847\n",
      "cnt: 10600\n",
      "loss: 109.7791105222702\n",
      "cnt: 10700\n",
      "loss: 112.45073292732239\n",
      "cnt: 10800\n",
      "loss: 120.45614091873169\n",
      "cnt: 10900\n",
      "loss: 115.9129823064804\n",
      "cnt: 11000\n",
      "loss: 110.16361930847168\n",
      "cnt: 11100\n",
      "loss: 119.21558785915374\n",
      "cnt: 11200\n",
      "loss: 113.07222405910493\n",
      "cnt: 11300\n",
      "loss: 105.27253861904144\n",
      "cnt: 11400\n",
      "loss: 120.4980244922638\n",
      "cnt: 11500\n",
      "loss: 107.93482307434083\n",
      "cnt: 11600\n",
      "loss: 120.09462403774262\n",
      "cnt: 11700\n",
      "loss: 102.45439388275146\n",
      "cnt: 11800\n",
      "loss: 113.15885385990143\n",
      "cnt: 11900\n",
      "loss: 107.20064334392548\n",
      "cnt: 12000\n",
      "loss: 113.49844644546509\n",
      "cnt: 12100\n",
      "loss: 102.73628732681274\n",
      "cnt: 12200\n",
      "loss: 104.80268286705017\n",
      "cnt: 12300\n",
      "loss: 121.45516674757003\n",
      "cnt: 12400\n",
      "loss: 109.72148807525635\n",
      "cnt: 12500\n",
      "loss: 118.82717745780946\n",
      "cnt: 12600\n",
      "loss: 119.7066900730133\n",
      "cnt: 12700\n",
      "loss: 114.33427752017975\n",
      "cnt: 12800\n",
      "loss: 106.50187261819839\n",
      "cnt: 12900\n",
      "loss: 112.09385077476502\n",
      "cnt: 13000\n",
      "loss: 114.30138989448547\n",
      "cnt: 13100\n",
      "loss: 106.32713057994843\n",
      "cnt: 13200\n",
      "loss: 94.14369739294052\n",
      "cnt: 13300\n",
      "loss: 106.73191687583923\n",
      "cnt: 13400\n",
      "loss: 114.14328989982604\n",
      "cnt: 13500\n",
      "loss: 118.22078448295593\n",
      "cnt: 13600\n",
      "loss: 104.50695821285248\n",
      "cnt: 13700\n",
      "loss: 110.90620648860931\n",
      "cnt: 13800\n",
      "loss: 107.44344814300537\n",
      "cnt: 13900\n",
      "loss: 107.69751739501953\n",
      "cnt: 14000\n",
      "loss: 110.29591791391373\n",
      "cnt: 14100\n",
      "loss: 93.6364796113968\n",
      "cnt: 14200\n",
      "loss: 103.5776339495182\n",
      "cnt: 14300\n",
      "loss: 101.65474446296692\n",
      "cnt: 14400\n",
      "loss: 107.66673007011414\n",
      "cnt: 14500\n",
      "loss: 110.16003652572631\n",
      "cnt: 14600\n",
      "loss: 98.28548819065094\n",
      "cnt: 14700\n",
      "loss: 109.24931144237519\n",
      "cnt: 14800\n",
      "loss: 97.2730464053154\n",
      "cnt: 14900\n",
      "loss: 119.175983543396\n",
      "cnt: 15000\n",
      "loss: 107.31457874298096\n",
      "cnt: 15100\n",
      "loss: 113.31695879936218\n",
      "cnt: 15200\n",
      "loss: 100.14360978126525\n",
      "cnt: 15300\n",
      "loss: 89.56191249847411\n",
      "cnt: 15400\n",
      "loss: 99.49764011859894\n",
      "cnt: 15500\n",
      "loss: 100.19337944030762\n",
      "cnt: 15600\n",
      "loss: 115.4883545589447\n",
      "cnt: 15700\n",
      "loss: 104.80545035362243\n",
      "cnt: 15800\n",
      "loss: 120.87937396764755\n",
      "cnt: 15900\n",
      "loss: 111.03595653533935\n",
      "cnt: 16000\n",
      "loss: 104.30872038841248\n",
      "cnt: 16100\n",
      "loss: 99.39080379724503\n",
      "cnt: 16200\n",
      "loss: 101.87641988515854\n",
      "cnt: 16300\n",
      "loss: 102.1932052564621\n",
      "cnt: 16400\n",
      "loss: 110.44571330547333\n",
      "cnt: 16500\n",
      "loss: 115.07931240320205\n",
      "cnt: 16600\n",
      "loss: 111.59410450935364\n",
      "cnt: 16700\n",
      "loss: 111.37252252578736\n",
      "cnt: 16800\n",
      "loss: 103.109217979908\n",
      "cnt: 16900\n",
      "loss: 107.90982401847839\n",
      "cnt: 17000\n",
      "loss: 106.48676447629929\n",
      "cnt: 17100\n",
      "loss: 115.27743941307068\n",
      "cnt: 17200\n",
      "loss: 108.97782762050629\n",
      "cnt: 17300\n",
      "loss: 104.89619112491607\n",
      "cnt: 17400\n",
      "loss: 107.33395882606506\n",
      "cnt: 17500\n",
      "loss: 121.01367542266846\n",
      "cnt: 17600\n",
      "loss: 105.23464773178101\n",
      "cnt: 17700\n",
      "loss: 113.23071180343628\n",
      "cnt: 17800\n",
      "loss: 106.38421928405762\n",
      "cnt: 17900\n",
      "loss: 110.01266936302186\n",
      "cnt: 18000\n",
      "loss: 93.70419226646423\n",
      "cnt: 18100\n",
      "loss: 111.44693710803986\n",
      "cnt: 18200\n",
      "loss: 109.85288064002991\n",
      "cnt: 18300\n",
      "loss: 120.2465109372139\n",
      "cnt: 18400\n",
      "loss: 108.0047911453247\n",
      "cnt: 18500\n",
      "loss: 106.05190311431885\n",
      "cnt: 18600\n",
      "loss: 115.92814679145813\n",
      "cnt: 18700\n",
      "loss: 114.81654303550721\n",
      "cnt: 18800\n",
      "loss: 99.30023833990097\n",
      "cnt: 18900\n",
      "loss: 97.77428430080414\n",
      "cnt: 19000\n",
      "loss: 117.33373859405518\n",
      "cnt: 19100\n",
      "loss: 106.8771078801155\n",
      "cnt: 19200\n",
      "loss: 103.39760612726212\n",
      "cnt: 19300\n",
      "loss: 102.46007146835328\n",
      "cnt: 19400\n",
      "loss: 113.35795686721802\n",
      "cnt: 19500\n",
      "loss: 109.2294545841217\n",
      "cnt: 19600\n",
      "loss: 112.63700126647949\n",
      "cnt: 19700\n",
      "loss: 101.72668176412583\n",
      "cnt: 19800\n",
      "loss: 106.96600417613983\n",
      "cnt: 19900\n",
      "loss: 105.99188455343247\n",
      "cnt: 20000\n",
      "loss: 105.51051550388337\n",
      "cnt: 20100\n",
      "loss: 112.7085880446434\n",
      "cnt: 20200\n",
      "loss: 104.00374500274658\n",
      "cnt: 20300\n",
      "loss: 108.37506178855897\n",
      "cnt: 20400\n",
      "loss: 105.62333294868469\n",
      "cnt: 20500\n",
      "loss: 100.14546773195266\n",
      "cnt: 20600\n",
      "loss: 100.02734808683395\n",
      "cnt: 20700\n",
      "loss: 120.77197016716003\n",
      "cnt: 20800\n",
      "loss: 107.5845017337799\n",
      "cnt: 20900\n",
      "loss: 115.9496197628975\n",
      "cnt: 21000\n",
      "loss: 98.04207606315613\n",
      "cnt: 21100\n",
      "loss: 96.07989473819732\n",
      "cnt: 21200\n",
      "loss: 106.22401536941528\n",
      "cnt: 21300\n",
      "loss: 106.34002776145935\n",
      "cnt: 21400\n",
      "loss: 99.58738229751587\n",
      "cnt: 21500\n",
      "loss: 104.60013899803161\n",
      "cnt: 21600\n",
      "loss: 117.43098456382751\n",
      "cnt: 21700\n",
      "loss: 96.17424604415893\n",
      "cnt: 21800\n",
      "loss: 108.65984564542771\n",
      "cnt: 21900\n",
      "loss: 105.5308996295929\n",
      "cnt: 22000\n",
      "loss: 100.71645614624023\n",
      "cnt: 22100\n",
      "loss: 105.08773042678833\n",
      "cnt: 22200\n",
      "loss: 100.15830332756042\n",
      "cnt: 22300\n",
      "loss: 106.88038122653961\n",
      "cnt: 22400\n",
      "loss: 123.59253177642822\n",
      "cnt: 22500\n",
      "loss: 96.64724637985229\n",
      "cnt: 22600\n",
      "loss: 100.85700252056122\n",
      "cnt: 22700\n",
      "loss: 103.98998485088349\n",
      "cnt: 22800\n",
      "loss: 104.21356986045838\n",
      "cnt: 22900\n",
      "loss: 105.17190875530243\n",
      "cnt: 23000\n",
      "loss: 93.54784884929657\n",
      "cnt: 23100\n",
      "loss: 96.64971737623215\n",
      "cnt: 23200\n",
      "loss: 103.51812571287155\n",
      "cnt: 23300\n",
      "loss: 89.34730331420899\n",
      "cnt: 23400\n",
      "loss: 109.44451385498047\n",
      "cnt: 23500\n",
      "loss: 102.26241039514542\n",
      "cnt: 23600\n",
      "loss: 113.28031579017639\n",
      "cnt: 23700\n",
      "loss: 107.51487401008606\n",
      "cnt: 23800\n",
      "loss: 100.28798303723335\n",
      "cnt: 23900\n",
      "loss: 103.15665395259857\n",
      "cnt: 24000\n",
      "loss: 107.94854507446288\n",
      "cnt: 24100\n",
      "loss: 112.54382637500763\n",
      "cnt: 24200\n",
      "loss: 101.69488807678222\n",
      "cnt: 24300\n",
      "loss: 105.44731106758118\n",
      "cnt: 24400\n",
      "loss: 96.48551624298096\n",
      "cnt: 24500\n",
      "loss: 96.45866115808487\n",
      "cnt: 24600\n",
      "loss: 98.41254395484924\n",
      "cnt: 24700\n",
      "loss: 102.83690561294556\n",
      "cnt: 24800\n",
      "loss: 97.8204989361763\n",
      "cnt: 24900\n",
      "loss: 107.69367390155793\n",
      "cnt: 25000\n",
      "loss: 103.510313975811\n",
      "cnt: 25100\n",
      "loss: 100.00516547441482\n",
      "cnt: 25200\n",
      "loss: 104.2427820444107\n",
      "cnt: 25300\n",
      "loss: 96.07018065452576\n",
      "cnt: 25400\n",
      "loss: 96.79408390045165\n",
      "cnt: 25500\n",
      "loss: 112.57114815711975\n",
      "cnt: 25600\n",
      "loss: 98.95412115097047\n",
      "cnt: 25700\n",
      "loss: 110.41448860645295\n",
      "cnt: 25800\n",
      "loss: 99.04712907791138\n",
      "cnt: 25900\n",
      "loss: 91.53953685760499\n",
      "cnt: 26000\n",
      "loss: 98.07005649089814\n",
      "cnt: 26100\n",
      "loss: 104.93595505952835\n",
      "cnt: 26200\n",
      "loss: 101.43467827320099\n",
      "cnt: 26300\n",
      "loss: 100.72005754470825\n",
      "cnt: 26400\n",
      "loss: 91.25203158855439\n",
      "cnt: 26500\n",
      "loss: 104.6527485704422\n",
      "cnt: 26600\n",
      "loss: 105.56109513282776\n",
      "cnt: 26700\n",
      "loss: 109.5653927898407\n",
      "cnt: 26800\n",
      "loss: 112.75846467018127\n",
      "cnt: 26900\n",
      "loss: 94.89320862293243\n",
      "cnt: 27000\n",
      "loss: 100.36559323310853\n",
      "cnt: 27100\n",
      "loss: 109.00349931001664\n",
      "cnt: 27200\n",
      "loss: 97.27177017211915\n",
      "cnt: 27300\n",
      "loss: 101.21543724060058\n",
      "cnt: 27400\n",
      "loss: 100.08276782035827\n",
      "cnt: 27500\n",
      "loss: 95.0454337143898\n",
      "cnt: 27600\n",
      "loss: 98.58140239715576\n",
      "cnt: 27700\n",
      "loss: 106.77244601726532\n",
      "cnt: 27800\n",
      "loss: 104.12370937585831\n",
      "cnt: 27900\n",
      "loss: 100.87311197280884\n",
      "cnt: 28000\n",
      "loss: 105.8476615166664\n",
      "cnt: 28100\n",
      "loss: 97.25074377059937\n",
      "cnt: 28200\n",
      "loss: 105.51386992454529\n",
      "cnt: 28300\n",
      "loss: 97.4752871608734\n",
      "cnt: 28400\n",
      "loss: 97.21954921722413\n",
      "cnt: 28500\n",
      "loss: 101.58430284023285\n",
      "cnt: 28600\n",
      "loss: 113.45922003746033\n",
      "cnt: 28700\n",
      "loss: 104.73613704681397\n",
      "cnt: 28800\n",
      "loss: 105.96332594394684\n",
      "cnt: 28900\n",
      "loss: 105.406549077034\n",
      "cnt: 29000\n",
      "loss: 93.46661157369614\n",
      "cnt: 29100\n",
      "loss: 89.9828442108631\n",
      "cnt: 29200\n",
      "loss: 98.46848689556121\n",
      "cnt: 29300\n",
      "loss: 103.99864669799804\n",
      "cnt: 29400\n",
      "loss: 103.38030633449554\n",
      "cnt: 29500\n",
      "loss: 109.90496076583862\n",
      "cnt: 29600\n",
      "loss: 104.6145621562004\n",
      "cnt: 29700\n",
      "loss: 99.34162461280823\n",
      "cnt: 29800\n",
      "loss: 93.20864921569824\n",
      "cnt: 29900\n",
      "loss: 112.55097623109818\n",
      "cnt: 30000\n",
      "loss: 107.29807453632355\n",
      "cnt: 30100\n",
      "loss: 104.59342186927796\n",
      "cnt: 30200\n",
      "loss: 106.70162593841553\n",
      "cnt: 30300\n",
      "loss: 109.57363510847091\n",
      "cnt: 30400\n",
      "loss: 110.79763109207153\n",
      "cnt: 30500\n",
      "loss: 99.6279200577736\n",
      "cnt: 30600\n",
      "loss: 103.34221247673035\n",
      "cnt: 30700\n",
      "loss: 104.32156568288804\n",
      "cnt: 30800\n",
      "loss: 100.90222913742065\n",
      "cnt: 30900\n",
      "loss: 104.96118654251099\n",
      "cnt: 31000\n",
      "loss: 96.96123394012452\n",
      "cnt: 31100\n",
      "loss: 101.09006801009178\n",
      "cnt: 31200\n",
      "loss: 109.63740614891053\n",
      "cnt: 31300\n",
      "loss: 101.59649610042572\n",
      "cnt: 31400\n",
      "loss: 96.6180990600586\n",
      "cnt: 31500\n",
      "loss: 104.21125399827957\n",
      "cnt: 31600\n",
      "loss: 101.36947465419769\n",
      "cnt: 31700\n",
      "loss: 109.82657135009765\n",
      "cnt: 31800\n",
      "loss: 98.89073903560639\n",
      "cnt: 31900\n",
      "loss: 95.54698843479156\n",
      "cnt: 32000\n",
      "loss: 104.90461248397827\n",
      "cnt: 32100\n",
      "loss: 110.73699065446854\n",
      "cnt: 32200\n",
      "loss: 105.85238282203674\n",
      "cnt: 32300\n",
      "loss: 102.71533277988433\n",
      "cnt: 32400\n",
      "loss: 101.7309958410263\n",
      "cnt: 32500\n",
      "loss: 113.64140090942382\n",
      "cnt: 32600\n",
      "loss: 100.99171960830688\n",
      "cnt: 32700\n",
      "loss: 100.7518681716919\n",
      "cnt: 32800\n",
      "loss: 100.78816907882691\n",
      "cnt: 32900\n",
      "loss: 110.236099421978\n",
      "cnt: 33000\n",
      "loss: 97.86537442207336\n",
      "cnt: 33100\n",
      "loss: 107.73937285423278\n",
      "cnt: 33200\n",
      "loss: 110.4385174369812\n",
      "cnt: 33300\n",
      "loss: 94.6362215423584\n",
      "cnt: 33400\n",
      "loss: 109.5930189704895\n",
      "cnt: 33500\n",
      "loss: 108.53290728569031\n",
      "cnt: 33600\n",
      "loss: 108.10363726615905\n",
      "cnt: 33700\n",
      "loss: 108.27161159038543\n",
      "cnt: 33800\n",
      "loss: 97.22836436033249\n",
      "cnt: 33900\n",
      "loss: 109.73511973142624\n",
      "cnt: 34000\n",
      "loss: 110.05854522705079\n",
      "cnt: 34100\n",
      "loss: 104.22623283863068\n",
      "cnt: 34200\n",
      "loss: 96.41011881113053\n",
      "cnt: 34300\n",
      "loss: 116.07875213623046\n",
      "cnt: 34400\n",
      "loss: 112.34058122634887\n",
      "cnt: 34500\n",
      "loss: 100.52853873729705\n",
      "cnt: 34600\n",
      "loss: 105.26842296600341\n",
      "cnt: 34700\n",
      "loss: 105.81090099334716\n",
      "cnt: 34800\n",
      "loss: 102.03972995758056\n",
      "cnt: 34900\n",
      "loss: 98.91984314918518\n",
      "cnt: 35000\n",
      "loss: 103.73614339828491\n",
      "cnt: 35100\n",
      "loss: 116.56845341682434\n",
      "cnt: 35200\n",
      "loss: 85.34863055706025\n",
      "cnt: 35300\n",
      "loss: 108.09896679639816\n",
      "cnt: 35400\n",
      "loss: 104.32474122047424\n",
      "cnt: 35500\n",
      "loss: 103.97235110282898\n",
      "cnt: 35600\n",
      "loss: 102.25081176757813\n",
      "cnt: 35700\n",
      "loss: 102.58645554542541\n",
      "cnt: 35800\n",
      "loss: 108.77458912849426\n",
      "cnt: 35900\n",
      "loss: 96.4052153301239\n",
      "cnt: 36000\n",
      "loss: 99.90012706756592\n",
      "cnt: 36100\n",
      "loss: 109.45192266464234\n",
      "cnt: 36200\n",
      "loss: 100.72377695083618\n",
      "cnt: 36300\n",
      "loss: 102.48412359952927\n",
      "cnt: 36400\n",
      "loss: 106.0700858592987\n",
      "cnt: 36500\n",
      "loss: 89.00010485649109\n",
      "cnt: 36600\n",
      "loss: 94.35190842151641\n",
      "cnt: 36700\n",
      "loss: 105.89851169586181\n",
      "cnt: 36800\n",
      "loss: 109.18096734046937\n",
      "cnt: 36900\n",
      "loss: 105.90482081413269\n",
      "cnt: 37000\n",
      "loss: 102.21114500999451\n",
      "cnt: 37100\n",
      "loss: 114.45275974273682\n",
      "cnt: 37200\n",
      "loss: 111.67023860931397\n",
      "cnt: 37300\n",
      "loss: 105.0740140914917\n",
      "cnt: 37400\n",
      "loss: 101.77470098495483\n",
      "cnt: 37500\n",
      "loss: 104.29111912727356\n",
      "cnt: 37600\n",
      "loss: 98.1955808544159\n",
      "cnt: 37700\n",
      "loss: 111.10564638614655\n",
      "cnt: 37800\n",
      "loss: 109.81057207822799\n",
      "cnt: 37900\n",
      "loss: 98.23145349502563\n",
      "cnt: 38000\n",
      "loss: 102.07515820503235\n",
      "cnt: 38100\n",
      "loss: 97.0534111404419\n",
      "cnt: 38200\n",
      "loss: 97.84555166959763\n",
      "cnt: 38300\n",
      "loss: 101.76214631080627\n",
      "cnt: 38400\n",
      "loss: 94.30866788864135\n",
      "cnt: 38500\n",
      "loss: 110.16151971340179\n",
      "cnt: 38600\n",
      "loss: 98.80865061759948\n",
      "cnt: 38700\n",
      "loss: 108.58408213615418\n",
      "cnt: 38800\n",
      "loss: 101.57705677986145\n",
      "cnt: 38900\n",
      "loss: 98.84100822448731\n",
      "cnt: 39000\n",
      "loss: 88.93455503463746\n",
      "cnt: 39100\n",
      "loss: 109.26423601150513\n",
      "cnt: 39200\n",
      "loss: 103.11298100471497\n",
      "cnt: 39300\n",
      "loss: 98.31896611213683\n",
      "cnt: 39400\n",
      "loss: 103.68729751110077\n",
      "cnt: 39500\n",
      "loss: 102.46921075582505\n",
      "cnt: 39600\n",
      "loss: 99.94335021972657\n",
      "cnt: 39700\n",
      "loss: 100.38756538391114\n",
      "cnt: 39800\n",
      "loss: 107.49778604507446\n",
      "cnt: 39900\n",
      "loss: 96.915261759758\n",
      "cnt: 40000\n",
      "loss: 89.86265001773835\n",
      "cnt: 40100\n",
      "loss: 94.90945742607117\n",
      "cnt: 40200\n",
      "loss: 96.56623704195023\n",
      "cnt: 40300\n",
      "loss: 96.95941503047943\n",
      "cnt: 40400\n",
      "loss: 96.51720468044282\n",
      "cnt: 40500\n",
      "loss: 98.94979724884033\n",
      "cnt: 40600\n",
      "loss: 99.2676101255417\n",
      "cnt: 40700\n",
      "loss: 100.49293098449706\n",
      "cnt: 40800\n",
      "loss: 98.12694568634033\n",
      "cnt: 40900\n",
      "loss: 104.94940455436706\n",
      "cnt: 41000\n",
      "loss: 94.6441482925415\n",
      "cnt: 41100\n",
      "loss: 91.7534158039093\n",
      "cnt: 41200\n",
      "loss: 97.84442904472351\n",
      "cnt: 41300\n",
      "loss: 95.33984565734863\n",
      "cnt: 41400\n",
      "loss: 100.68341576099395\n",
      "cnt: 41500\n",
      "loss: 101.20081897258758\n",
      "cnt: 41600\n",
      "loss: 108.40376826286315\n",
      "cnt: 41700\n",
      "loss: 100.20968988895416\n",
      "cnt: 41800\n",
      "loss: 96.20796302795411\n",
      "cnt: 41900\n",
      "loss: 100.1721225643158\n",
      "cnt: 42000\n",
      "loss: 100.2008111858368\n",
      "cnt: 42100\n",
      "loss: 101.43667598724365\n",
      "cnt: 42200\n",
      "loss: 105.21121384620666\n",
      "cnt: 42300\n",
      "loss: 101.92823254585267\n",
      "cnt: 42400\n",
      "loss: 102.06560440063477\n",
      "cnt: 42500\n",
      "loss: 105.02121224403382\n",
      "cnt: 42600\n",
      "loss: 98.65936423540116\n",
      "cnt: 42700\n",
      "loss: 104.74060401916503\n",
      "cnt: 42800\n",
      "loss: 98.63124035835266\n",
      "cnt: 42900\n",
      "loss: 93.87299048662186\n",
      "cnt: 43000\n",
      "loss: 107.49629625797272\n",
      "cnt: 43100\n",
      "loss: 99.83468994379044\n",
      "cnt: 43200\n",
      "loss: 102.4315219593048\n",
      "cnt: 43300\n",
      "loss: 99.22234882831573\n",
      "cnt: 43400\n",
      "loss: 91.89625725746154\n",
      "cnt: 43500\n",
      "loss: 106.8145424747467\n",
      "cnt: 43600\n",
      "loss: 97.43923898696899\n",
      "cnt: 43700\n",
      "loss: 104.2655731344223\n",
      "cnt: 43800\n",
      "loss: 95.67536026000977\n",
      "cnt: 43900\n",
      "loss: 113.93364724159241\n",
      "cnt: 44000\n",
      "loss: 97.98332845687867\n",
      "cnt: 44100\n",
      "loss: 105.82424700736999\n",
      "cnt: 44200\n",
      "loss: 119.58815570831298\n",
      "cnt: 44300\n",
      "loss: 99.41684008598328\n",
      "cnt: 44400\n",
      "loss: 84.31670566082\n",
      "cnt: 44500\n",
      "loss: 104.57495790958404\n",
      "cnt: 44600\n",
      "loss: 104.68427787780762\n",
      "cnt: 44700\n",
      "loss: 102.6115805053711\n",
      "cnt: 44800\n",
      "loss: 102.4698443222046\n",
      "cnt: 44900\n",
      "loss: 89.53434021234513\n",
      "cnt: 45000\n",
      "loss: 95.30480693340301\n",
      "cnt: 45100\n",
      "loss: 96.56034171104432\n",
      "cnt: 45200\n",
      "loss: 103.18917800426483\n",
      "cnt: 45300\n",
      "loss: 101.15085968971252\n",
      "cnt: 45400\n",
      "loss: 104.71698632240296\n",
      "cnt: 45500\n",
      "loss: 90.96470959186554\n",
      "cnt: 45600\n",
      "loss: 98.10671476840973\n",
      "cnt: 45700\n",
      "loss: 93.1621801185608\n",
      "cnt: 45800\n",
      "loss: 94.93518049240112\n",
      "cnt: 45900\n",
      "loss: 106.13493301391601\n",
      "cnt: 46000\n",
      "loss: 96.09045095443726\n",
      "cnt: 46100\n",
      "loss: 100.66009241104126\n",
      "cnt: 46200\n",
      "loss: 95.36447106361389\n",
      "cnt: 46300\n",
      "loss: 102.11259044647217\n",
      "cnt: 46400\n",
      "loss: 106.0529160118103\n",
      "cnt: 46500\n",
      "loss: 97.24267837524414\n",
      "cnt: 46600\n",
      "loss: 103.66349723815918\n",
      "cnt: 46700\n",
      "loss: 96.71669693470001\n",
      "cnt: 46800\n",
      "loss: 90.87100652694703\n",
      "cnt: 46900\n",
      "loss: 99.2454102420807\n",
      "cnt: 47000\n",
      "loss: 100.09126199245453\n",
      "cnt: 47100\n",
      "loss: 105.67271460533142\n",
      "cnt: 47200\n",
      "loss: 98.56491712570191\n",
      "cnt: 47300\n",
      "loss: 104.5766583108902\n",
      "cnt: 47400\n",
      "loss: 97.80623126983643\n",
      "cnt: 47500\n",
      "loss: 102.36132790088654\n",
      "cnt: 47600\n",
      "loss: 89.94414828300476\n",
      "cnt: 47700\n",
      "loss: 94.85081416130066\n",
      "cnt: 47800\n",
      "loss: 95.38854789733887\n",
      "cnt: 47900\n",
      "loss: 98.94148066997528\n",
      "cnt: 48000\n",
      "loss: 104.93080101490021\n",
      "cnt: 48100\n",
      "loss: 110.17661481380463\n",
      "cnt: 48200\n",
      "loss: 97.63360416412354\n",
      "cnt: 48300\n",
      "loss: 98.12865822792054\n",
      "cnt: 48400\n",
      "loss: 97.79265228271484\n",
      "cnt: 48500\n",
      "loss: 103.97856926441193\n",
      "cnt: 48600\n",
      "loss: 118.48539131164551\n",
      "cnt: 48700\n",
      "loss: 98.3181519126892\n",
      "cnt: 48800\n",
      "loss: 98.30629596710205\n",
      "cnt: 48900\n",
      "loss: 91.0085570526123\n",
      "cnt: 49000\n",
      "loss: 108.46070840835571\n",
      "cnt: 49100\n",
      "loss: 100.6008723449707\n",
      "cnt: 49200\n",
      "loss: 114.06554731369019\n",
      "cnt: 49300\n",
      "loss: 107.21211508750916\n",
      "cnt: 49400\n",
      "loss: 98.84731714010239\n",
      "cnt: 49500\n",
      "loss: 105.28777571678161\n",
      "cnt: 49600\n",
      "loss: 85.70381414413453\n",
      "cnt: 49700\n",
      "loss: 93.02134997367858\n",
      "cnt: 49800\n",
      "loss: 107.56207703590393\n",
      "cnt: 49900\n",
      "loss: 98.30352700233459\n",
      "cnt: 50000\n",
      "loss: 107.51143825531005\n",
      "cnt: 50100\n",
      "loss: 98.47587192058563\n",
      "cnt: 50200\n",
      "loss: 90.79027286052704\n",
      "cnt: 50300\n",
      "loss: 98.73630712985992\n",
      "cnt: 50400\n",
      "loss: 98.50788259506226\n",
      "cnt: 50500\n",
      "loss: 89.49740668773651\n",
      "cnt: 50600\n",
      "loss: 98.92585997104645\n",
      "cnt: 50700\n",
      "loss: 98.1555924987793\n",
      "cnt: 50800\n",
      "loss: 89.14282846450806\n",
      "cnt: 50900\n",
      "loss: 101.08986782073974\n",
      "cnt: 51000\n",
      "loss: 91.58939019203186\n",
      "cnt: 51100\n",
      "loss: 94.1352341389656\n",
      "cnt: 51200\n",
      "loss: 100.67623310089111\n",
      "cnt: 51300\n",
      "loss: 99.0671581029892\n",
      "cnt: 51400\n",
      "loss: 101.52675645828248\n",
      "cnt: 51500\n",
      "loss: 103.20390790939331\n",
      "cnt: 51600\n",
      "loss: 103.6902277469635\n",
      "cnt: 51700\n",
      "loss: 95.02850944042206\n",
      "cnt: 51800\n",
      "loss: 89.637645072937\n",
      "cnt: 51900\n",
      "loss: 98.86203796386718\n",
      "cnt: 52000\n",
      "loss: 97.8049417257309\n",
      "cnt: 52100\n",
      "loss: 108.85316655158996\n",
      "cnt: 52200\n",
      "loss: 96.12152040481567\n",
      "cnt: 52300\n",
      "loss: 92.19773122787475\n",
      "cnt: 52400\n",
      "loss: 102.28183762550354\n",
      "cnt: 52500\n",
      "loss: 94.10750710010528\n",
      "cnt: 52600\n",
      "loss: 100.51921753883362\n",
      "cnt: 52700\n",
      "loss: 105.79991136074067\n",
      "cnt: 52800\n",
      "loss: 103.70407220840454\n",
      "cnt: 52900\n",
      "loss: 101.906655023098\n",
      "cnt: 53000\n",
      "loss: 98.9163126206398\n",
      "cnt: 53100\n",
      "loss: 96.74774281024933\n",
      "cnt: 53200\n",
      "loss: 91.95696475982666\n",
      "cnt: 53300\n",
      "loss: 107.64157138586044\n",
      "cnt: 53400\n",
      "loss: 98.4740664100647\n",
      "cnt: 53500\n",
      "loss: 92.25993673324585\n",
      "cnt: 53600\n",
      "loss: 96.99297051906586\n",
      "cnt: 53700\n",
      "loss: 98.18879598617553\n",
      "cnt: 53800\n",
      "loss: 96.13535362720489\n",
      "cnt: 53900\n",
      "loss: 84.5215864276886\n",
      "cnt: 54000\n",
      "loss: 100.8020251083374\n",
      "cnt: 54100\n",
      "loss: 91.10666676044464\n",
      "cnt: 54200\n",
      "loss: 100.5396577501297\n",
      "cnt: 54300\n",
      "loss: 100.93116231918334\n",
      "cnt: 54400\n",
      "loss: 92.00486184120179\n",
      "cnt: 54500\n",
      "loss: 94.86785493373871\n",
      "cnt: 54600\n",
      "loss: 99.71724236249923\n",
      "cnt: 54700\n",
      "loss: 106.5316418838501\n",
      "cnt: 54800\n",
      "loss: 108.8122046470642\n",
      "cnt: 54900\n",
      "loss: 95.28139414787293\n",
      "cnt: 55000\n",
      "loss: 100.64306577920914\n",
      "cnt: 55100\n",
      "loss: 104.48459096908569\n",
      "cnt: 55200\n",
      "loss: 101.35411203384399\n",
      "cnt: 55300\n",
      "loss: 100.13800932884216\n",
      "cnt: 55400\n",
      "loss: 92.57751320123673\n",
      "cnt: 55500\n",
      "loss: 106.48309433460236\n",
      "cnt: 55600\n",
      "loss: 98.24602831840515\n",
      "cnt: 55700\n",
      "loss: 98.6883364892006\n",
      "cnt: 55800\n",
      "loss: 101.24921873569488\n",
      "cnt: 55900\n",
      "loss: 97.57131339073182\n",
      "cnt: 56000\n",
      "loss: 98.87339277267456\n",
      "cnt: 56100\n",
      "loss: 100.40953349590302\n",
      "cnt: 56200\n",
      "loss: 106.71449556350709\n",
      "cnt: 56300\n",
      "loss: 101.68010686397552\n",
      "cnt: 56400\n",
      "loss: 96.4564351773262\n",
      "cnt: 56500\n",
      "loss: 107.25757332801818\n",
      "cnt: 56600\n",
      "loss: 106.62099355697632\n",
      "cnt: 56700\n",
      "loss: 94.9238735985756\n",
      "cnt: 56800\n",
      "loss: 93.63483073711396\n",
      "cnt: 56900\n",
      "loss: 91.27663042783738\n",
      "cnt: 57000\n",
      "loss: 108.85578925132751\n",
      "cnt: 57100\n",
      "loss: 95.05129778861999\n",
      "cnt: 57200\n",
      "loss: 100.75097897529602\n",
      "cnt: 57300\n",
      "loss: 103.03250567436218\n",
      "cnt: 57400\n",
      "loss: 104.2557229924202\n",
      "cnt: 57500\n",
      "loss: 101.62746382713318\n",
      "cnt: 57600\n",
      "loss: 101.97152036190033\n",
      "cnt: 57700\n",
      "loss: 95.87767745256424\n",
      "cnt: 57800\n",
      "loss: 95.56634135723114\n",
      "cnt: 57900\n",
      "loss: 94.32811492204667\n",
      "cnt: 58000\n",
      "loss: 100.74129941940308\n",
      "cnt: 58100\n",
      "loss: 106.67985858917237\n",
      "cnt: 58200\n",
      "loss: 90.91656505584717\n",
      "cnt: 58300\n",
      "loss: 101.33697989463806\n",
      "cnt: 58400\n",
      "loss: 101.73691526412964\n",
      "cnt: 58500\n",
      "loss: 93.8124716258049\n",
      "cnt: 58600\n",
      "loss: 90.35041852951049\n",
      "cnt: 58700\n",
      "loss: 100.26258255004883\n",
      "cnt: 58800\n",
      "loss: 95.90884930610656\n",
      "cnt: 58900\n",
      "loss: 92.06975454330444\n",
      "cnt: 59000\n",
      "loss: 86.49884299993515\n",
      "cnt: 59100\n",
      "loss: 96.94165673613549\n",
      "cnt: 59200\n",
      "loss: 108.04730499267578\n",
      "cnt: 59300\n",
      "loss: 95.73416071414948\n",
      "cnt: 59400\n",
      "loss: 97.70878173828125\n",
      "cnt: 59500\n",
      "loss: 97.8255860209465\n",
      "cnt: 59600\n",
      "loss: 87.11665626049042\n",
      "cnt: 59700\n",
      "loss: 93.0266069316864\n",
      "cnt: 59800\n",
      "loss: 102.47706681728363\n",
      "cnt: 59900\n",
      "loss: 106.9193239402771\n",
      "cnt: 60000\n",
      "loss: 101.44160343647003\n",
      "cnt: 60100\n",
      "loss: 105.91486696243287\n",
      "cnt: 60200\n",
      "loss: 105.89468707799911\n",
      "cnt: 60300\n",
      "loss: 95.89954178333282\n",
      "cnt: 60400\n",
      "loss: 103.73380976676941\n",
      "cnt: 60500\n",
      "loss: 104.85754744529724\n",
      "cnt: 60600\n",
      "loss: 95.21006942749024\n",
      "cnt: 60700\n",
      "loss: 98.69766076803208\n",
      "cnt: 60800\n",
      "loss: 104.95540253639221\n",
      "cnt: 60900\n",
      "loss: 96.37594007730485\n",
      "cnt: 61000\n",
      "loss: 93.21167940616607\n",
      "cnt: 61100\n",
      "loss: 95.96765097141265\n",
      "cnt: 61200\n",
      "loss: 95.2382253932953\n",
      "cnt: 61300\n",
      "loss: 100.15330264568328\n",
      "cnt: 61400\n",
      "loss: 101.66720052719116\n",
      "cnt: 61500\n",
      "loss: 101.46497676372528\n",
      "cnt: 61600\n",
      "loss: 109.62871363401413\n",
      "cnt: 61700\n",
      "loss: 91.55387462615967\n",
      "cnt: 61800\n",
      "loss: 95.15428970336914\n",
      "cnt: 61900\n",
      "loss: 100.47939750671387\n",
      "cnt: 62000\n",
      "loss: 102.3026930642128\n",
      "cnt: 62100\n",
      "loss: 104.62018678665162\n",
      "cnt: 62200\n",
      "loss: 94.69389612197875\n",
      "cnt: 62300\n",
      "loss: 96.51250816583634\n",
      "cnt: 62400\n",
      "loss: 91.31628916740418\n",
      "cnt: 62500\n",
      "loss: 89.55843212842942\n",
      "cnt: 62600\n",
      "loss: 98.36690068244934\n",
      "cnt: 62700\n",
      "loss: 92.02440786361694\n",
      "cnt: 62800\n",
      "loss: 99.31008464336395\n",
      "cnt: 62900\n",
      "loss: 95.57305333137512\n",
      "cnt: 63000\n",
      "loss: 99.0790120458603\n",
      "cnt: 63100\n",
      "loss: 94.2859746837616\n",
      "cnt: 63200\n",
      "loss: 106.25079585790634\n",
      "cnt: 63300\n",
      "loss: 101.816808385849\n",
      "cnt: 63400\n",
      "loss: 102.0630690574646\n",
      "cnt: 63500\n",
      "loss: 103.44312522888184\n",
      "cnt: 63600\n",
      "loss: 93.77165453910828\n",
      "cnt: 63700\n",
      "loss: 95.88272015333176\n",
      "cnt: 63800\n",
      "loss: 91.75573656082153\n",
      "cnt: 63900\n",
      "loss: 93.92783039569855\n",
      "cnt: 64000\n",
      "loss: 92.04560995101929\n",
      "cnt: 64100\n",
      "loss: 100.1497904086113\n",
      "cnt: 64200\n",
      "loss: 107.63308669090272\n",
      "cnt: 64300\n",
      "loss: 97.7927301645279\n",
      "cnt: 64400\n",
      "loss: 100.11094663619996\n",
      "cnt: 64500\n",
      "loss: 85.64193810224533\n",
      "cnt: 64600\n",
      "loss: 92.50700908184052\n",
      "cnt: 64700\n",
      "loss: 105.46826990127563\n",
      "cnt: 64800\n",
      "loss: 108.06862003326415\n",
      "cnt: 64900\n",
      "loss: 102.16799783706665\n",
      "cnt: 65000\n",
      "loss: 106.63262159347533\n",
      "cnt: 65100\n",
      "loss: 92.00684103012085\n",
      "cnt: 65200\n",
      "loss: 110.53284411430359\n",
      "cnt: 65300\n",
      "loss: 95.30148330688476\n",
      "cnt: 65400\n",
      "loss: 100.50960466384888\n",
      "cnt: 65500\n",
      "loss: 105.37411883592605\n",
      "cnt: 65600\n",
      "loss: 107.2754714012146\n",
      "cnt: 65700\n",
      "loss: 104.9862320804596\n",
      "cnt: 65800\n",
      "loss: 96.31305467605591\n",
      "cnt: 65900\n",
      "loss: 98.22357749938965\n",
      "cnt: 66000\n",
      "loss: 100.65900849342346\n",
      "cnt: 66100\n",
      "loss: 93.33327900886536\n",
      "cnt: 66200\n",
      "loss: 100.29047940731049\n",
      "cnt: 66300\n",
      "loss: 102.0078384256363\n",
      "cnt: 66400\n",
      "loss: 104.33903503417969\n",
      "cnt: 66500\n",
      "loss: 93.43818614959717\n",
      "cnt: 66600\n",
      "loss: 103.71389256238938\n",
      "cnt: 66700\n",
      "loss: 95.59354927301406\n",
      "cnt: 66800\n",
      "loss: 100.42454626083374\n",
      "cnt: 66900\n",
      "loss: 104.51489361763001\n",
      "cnt: 67000\n",
      "loss: 90.18295537948609\n",
      "cnt: 67100\n",
      "loss: 96.02466999053955\n",
      "cnt: 67200\n",
      "loss: 92.21630440711975\n",
      "cnt: 67300\n",
      "loss: 97.84127706050873\n",
      "cnt: 67400\n",
      "loss: 97.41440559387208\n",
      "cnt: 67500\n",
      "loss: 91.77558888435364\n",
      "cnt: 67600\n",
      "loss: 95.49385987281799\n",
      "cnt: 67700\n",
      "loss: 89.49088952064514\n",
      "cnt: 67800\n",
      "loss: 105.28929728507995\n",
      "cnt: 67900\n",
      "loss: 106.11620845794678\n",
      "cnt: 68000\n",
      "loss: 83.80487668991088\n",
      "cnt: 68100\n",
      "loss: 102.1709792995453\n",
      "cnt: 68200\n",
      "loss: 106.20221335411071\n",
      "cnt: 68300\n",
      "loss: 97.73374712467194\n",
      "cnt: 68400\n",
      "loss: 102.43745378494263\n",
      "cnt: 68500\n",
      "loss: 100.56898916244506\n",
      "cnt: 68600\n",
      "loss: 92.74689109802246\n",
      "cnt: 68700\n",
      "loss: 112.44561017990112\n",
      "cnt: 68800\n",
      "loss: 108.98567975997925\n",
      "cnt: 68900\n",
      "loss: 90.66540534973144\n",
      "cnt: 69000\n",
      "loss: 97.69350205898284\n",
      "cnt: 69100\n",
      "loss: 102.64972243070602\n",
      "cnt: 69200\n",
      "loss: 102.46328746795655\n",
      "cnt: 69300\n",
      "loss: 99.13064772605895\n",
      "cnt: 69400\n",
      "loss: 94.99004323482514\n",
      "cnt: 69500\n",
      "loss: 106.91364225387574\n",
      "cnt: 69600\n",
      "loss: 112.65602948188781\n",
      "cnt: 69700\n",
      "loss: 102.61092867612838\n",
      "cnt: 69800\n",
      "loss: 96.56786904335021\n",
      "cnt: 69900\n",
      "loss: 95.26570411920548\n",
      "cnt: 70000\n",
      "loss: 90.21970234155656\n",
      "cnt: 70100\n",
      "loss: 100.02622752189636\n",
      "cnt: 70200\n",
      "loss: 103.24851296186448\n",
      "cnt: 70300\n",
      "loss: 96.36576167583466\n",
      "cnt: 70400\n",
      "loss: 88.99305919408798\n",
      "cnt: 70500\n",
      "loss: 100.9054967045784\n",
      "cnt: 70600\n",
      "loss: 97.43622455596923\n",
      "cnt: 70700\n",
      "loss: 106.92715754508973\n",
      "cnt: 70800\n",
      "loss: 90.11161118745804\n",
      "cnt: 70900\n",
      "loss: 94.30608855724334\n",
      "cnt: 71000\n",
      "loss: 107.33973618745804\n",
      "cnt: 71100\n",
      "loss: 96.36111151695252\n",
      "cnt: 71200\n",
      "loss: 102.10751134872436\n",
      "cnt: 71300\n",
      "loss: 95.23854263305664\n",
      "cnt: 71400\n",
      "loss: 92.73671802520752\n",
      "cnt: 71500\n",
      "loss: 87.53580729007722\n",
      "cnt: 71600\n",
      "loss: 88.7002891921997\n",
      "cnt: 71700\n",
      "loss: 91.418372631073\n",
      "cnt: 71800\n",
      "loss: 105.87549772262574\n",
      "cnt: 71900\n",
      "loss: 99.92000002861023\n",
      "cnt: 72000\n",
      "loss: 100.63003929138183\n",
      "cnt: 72100\n",
      "loss: 102.55699703216553\n",
      "cnt: 72200\n",
      "loss: 103.7926278591156\n",
      "cnt: 72300\n",
      "loss: 106.64589237213134\n",
      "cnt: 72400\n",
      "loss: 89.74248792409897\n",
      "cnt: 72500\n",
      "loss: 101.66690011978149\n",
      "cnt: 72600\n",
      "loss: 99.98380056858063\n",
      "cnt: 72700\n",
      "loss: 110.20209424972535\n",
      "cnt: 72800\n",
      "loss: 97.7291944694519\n",
      "cnt: 72900\n",
      "loss: 95.21413461685181\n",
      "cnt: 73000\n",
      "loss: 107.11528474807739\n",
      "cnt: 73100\n",
      "loss: 107.92626493930817\n",
      "cnt: 73200\n",
      "loss: 95.60478894233704\n",
      "cnt: 73300\n",
      "loss: 99.85015669822693\n",
      "cnt: 73400\n",
      "loss: 100.06947573661805\n",
      "cnt: 73500\n",
      "loss: 101.51463814735412\n",
      "cnt: 73600\n",
      "loss: 103.77487237453461\n",
      "cnt: 73700\n",
      "loss: 98.73331914901733\n",
      "cnt: 73800\n",
      "loss: 101.90970343828201\n",
      "cnt: 73900\n",
      "loss: 93.25059267520905\n",
      "cnt: 74000\n",
      "loss: 93.72023889064789\n",
      "cnt: 74100\n",
      "loss: 91.01839780211449\n",
      "cnt: 74200\n",
      "loss: 89.52156292915345\n",
      "cnt: 74300\n",
      "loss: 87.40956121444702\n",
      "cnt: 74400\n",
      "loss: 95.99551151275635\n",
      "cnt: 74500\n",
      "loss: 95.69444870471955\n",
      "cnt: 74600\n",
      "loss: 86.96255000114441\n",
      "cnt: 74700\n",
      "loss: 104.75283093452454\n",
      "cnt: 74800\n",
      "loss: 103.82643465042115\n",
      "cnt: 74900\n",
      "loss: 101.79723140716553\n",
      "cnt: 75000\n",
      "loss: 98.69132740020751\n",
      "cnt: 75100\n",
      "loss: 97.12549001693725\n",
      "cnt: 75200\n",
      "loss: 105.25660664081573\n",
      "cnt: 75300\n",
      "loss: 98.01379621982575\n",
      "cnt: 75400\n",
      "loss: 91.62987785339355\n",
      "cnt: 75500\n",
      "loss: 90.37535591125489\n",
      "cnt: 75600\n",
      "loss: 98.15631944656371\n",
      "cnt: 75700\n",
      "loss: 100.0095749092102\n",
      "cnt: 75800\n",
      "loss: 99.95158204078675\n",
      "cnt: 75900\n",
      "loss: 101.6791100025177\n",
      "cnt: 76000\n",
      "loss: 99.70864231109618\n",
      "cnt: 76100\n",
      "loss: 93.02228132247924\n",
      "cnt: 76200\n",
      "loss: 104.89425640106201\n",
      "cnt: 76300\n",
      "loss: 95.3606717824936\n",
      "cnt: 76400\n",
      "loss: 86.68324797630311\n",
      "cnt: 76500\n",
      "loss: 89.72877443313598\n",
      "cnt: 76600\n",
      "loss: 92.57075181961059\n",
      "cnt: 76700\n",
      "loss: 94.02330773830414\n",
      "cnt: 76800\n",
      "loss: 99.0292176103592\n",
      "cnt: 76900\n",
      "loss: 93.60592388153076\n",
      "cnt: 77000\n",
      "loss: 102.99181593418122\n",
      "cnt: 77100\n",
      "loss: 96.25492080688477\n",
      "cnt: 77200\n",
      "loss: 90.10466059684754\n",
      "cnt: 77300\n",
      "loss: 96.60986535072327\n",
      "cnt: 77400\n",
      "loss: 98.5349873638153\n",
      "cnt: 77500\n",
      "loss: 96.48145398139954\n",
      "cnt: 77600\n",
      "loss: 104.7035434436798\n",
      "cnt: 77700\n",
      "loss: 82.42345049142837\n",
      "cnt: 77800\n",
      "loss: 95.05546168327332\n",
      "cnt: 77900\n",
      "loss: 112.85699100494385\n",
      "cnt: 78000\n",
      "loss: 92.71554829120636\n",
      "cnt: 78100\n",
      "loss: 99.61218682289123\n",
      "cnt: 78200\n",
      "loss: 97.7019141292572\n",
      "cnt: 78300\n",
      "loss: 106.18513540744782\n",
      "cnt: 78400\n",
      "loss: 92.9916618347168\n",
      "cnt: 78500\n",
      "loss: 104.27343596458435\n",
      "cnt: 78600\n",
      "loss: 92.41483273506165\n",
      "cnt: 78700\n",
      "loss: 105.24410598516464\n",
      "cnt: 78800\n",
      "loss: 97.88573435783387\n",
      "cnt: 78900\n",
      "loss: 108.18732402801514\n",
      "cnt: 79000\n",
      "loss: 96.13448763847352\n",
      "cnt: 79100\n",
      "loss: 107.65177398681641\n",
      "cnt: 79200\n",
      "loss: 96.1166178226471\n",
      "cnt: 79300\n",
      "loss: 104.31809612751007\n",
      "cnt: 79400\n",
      "loss: 95.98589647531509\n",
      "cnt: 79500\n",
      "loss: 103.36005234718323\n",
      "cnt: 79600\n",
      "loss: 101.2850100183487\n",
      "cnt: 79700\n",
      "loss: 103.0132234764099\n",
      "cnt: 79800\n",
      "loss: 104.12670410633088\n",
      "cnt: 79900\n",
      "loss: 100.27736045360565\n",
      "cnt: 80000\n",
      "loss: 100.21641155481339\n",
      "cnt: 80100\n",
      "loss: 97.9192280960083\n",
      "cnt: 80200\n",
      "loss: 94.48876671552658\n",
      "cnt: 80300\n",
      "loss: 106.91186980009078\n",
      "cnt: 80400\n",
      "loss: 100.00614255905151\n",
      "cnt: 80500\n",
      "loss: 101.56520418167115\n",
      "cnt: 80600\n",
      "loss: 93.1941911315918\n",
      "cnt: 80700\n",
      "loss: 105.77737894058228\n",
      "cnt: 80800\n",
      "loss: 93.53978196144104\n",
      "cnt: 80900\n",
      "loss: 105.88712172746658\n",
      "cnt: 81000\n",
      "loss: 110.62931765556336\n",
      "cnt: 81100\n",
      "loss: 89.75390701770783\n",
      "cnt: 81200\n",
      "loss: 99.26691013813019\n",
      "cnt: 81300\n",
      "loss: 101.29595994472504\n",
      "cnt: 81400\n",
      "loss: 98.36208155632019\n",
      "cnt: 81500\n",
      "loss: 109.35872510910035\n",
      "cnt: 81600\n",
      "loss: 99.75652944087982\n",
      "cnt: 81700\n",
      "loss: 86.18493172645569\n",
      "cnt: 81800\n",
      "loss: 96.63081346988677\n",
      "cnt: 81900\n",
      "loss: 103.47706670761109\n",
      "cnt: 82000\n",
      "loss: 103.95870903968812\n",
      "cnt: 82100\n",
      "loss: 94.3422715139389\n",
      "cnt: 82200\n",
      "loss: 99.14887407302857\n",
      "cnt: 82300\n",
      "loss: 105.02970267295838\n",
      "cnt: 82400\n",
      "loss: 100.1593130016327\n",
      "cnt: 82500\n",
      "loss: 95.43917402267456\n",
      "cnt: 82600\n",
      "loss: 108.55053681373596\n",
      "cnt: 82700\n",
      "loss: 105.93065947532654\n",
      "cnt: 82800\n",
      "loss: 95.44892268180847\n",
      "cnt: 82900\n",
      "loss: 99.06408952713012\n",
      "cnt: 83000\n",
      "loss: 103.15776120185852\n",
      "cnt: 83100\n",
      "loss: 98.30967314720154\n",
      "cnt: 83200\n",
      "loss: 100.77990097045898\n",
      "cnt: 83300\n",
      "loss: 93.76867980480193\n",
      "cnt: 83400\n",
      "loss: 97.3561450099945\n",
      "cnt: 83500\n",
      "loss: 94.29264114379883\n",
      "cnt: 83600\n",
      "loss: 104.20649145126343\n",
      "cnt: 83700\n",
      "loss: 94.68467679023743\n",
      "cnt: 83800\n",
      "loss: 115.09986048698426\n",
      "cnt: 83900\n",
      "loss: 104.71245891809464\n",
      "cnt: 84000\n",
      "loss: 94.8914197063446\n",
      "cnt: 84100\n",
      "loss: 97.75101537942886\n",
      "cnt: 84200\n",
      "loss: 97.80352628469467\n",
      "cnt: 84300\n",
      "loss: 89.8349693775177\n",
      "cnt: 84400\n",
      "loss: 114.54095789909363\n",
      "cnt: 84500\n",
      "loss: 100.72630333900452\n",
      "cnt: 84600\n",
      "loss: 95.02869946479797\n",
      "cnt: 84700\n",
      "loss: 95.7925599193573\n",
      "cnt: 84800\n",
      "loss: 102.53417371749877\n",
      "cnt: 84900\n",
      "loss: 93.57125467300415\n",
      "cnt: 85000\n",
      "loss: 101.49221937179566\n",
      "cnt: 85100\n",
      "loss: 97.92793236255646\n",
      "cnt: 85200\n",
      "loss: 86.61970913410187\n",
      "cnt: 85300\n",
      "loss: 97.30434567689896\n",
      "cnt: 85400\n",
      "loss: 111.6918510556221\n",
      "cnt: 85500\n",
      "loss: 97.48303317070007\n",
      "cnt: 85600\n",
      "loss: 89.15556873321533\n",
      "cnt: 85700\n",
      "loss: 94.83614682674408\n",
      "cnt: 85800\n",
      "loss: 108.40714944839478\n",
      "cnt: 85900\n",
      "loss: 105.33463135719299\n",
      "cnt: 86000\n",
      "loss: 98.53446075201035\n",
      "cnt: 86100\n",
      "loss: 111.54776700258255\n",
      "cnt: 86200\n",
      "loss: 99.58358599185944\n",
      "cnt: 86300\n",
      "loss: 101.7496829843521\n",
      "cnt: 86400\n",
      "loss: 95.77293857097625\n",
      "cnt: 86500\n",
      "loss: 98.41001423835755\n",
      "cnt: 86600\n",
      "loss: 107.74434018135071\n",
      "cnt: 86700\n",
      "loss: 94.47155277252197\n",
      "cnt: 86800\n",
      "loss: 106.26343426704406\n",
      "cnt: 86900\n",
      "loss: 101.45669628143311\n",
      "cnt: 87000\n",
      "loss: 99.87050135612488\n",
      "cnt: 87100\n",
      "loss: 99.11108234405518\n",
      "cnt: 87200\n",
      "loss: 106.6824342060089\n",
      "cnt: 87300\n",
      "loss: 106.37806311368942\n",
      "cnt: 87400\n",
      "loss: 103.60477317810059\n",
      "cnt: 87500\n",
      "loss: 103.74702389240265\n",
      "cnt: 87600\n",
      "loss: 97.78140484809876\n",
      "cnt: 87700\n",
      "loss: 104.94207723617554\n",
      "cnt: 87800\n",
      "loss: 104.11987597942353\n",
      "cnt: 87900\n",
      "loss: 94.95262415885925\n",
      "cnt: 88000\n",
      "loss: 105.28453779459\n",
      "cnt: 88100\n",
      "loss: 95.6710132408142\n",
      "cnt: 88200\n",
      "loss: 107.3526263165474\n",
      "cnt: 88300\n",
      "loss: 97.43190501689911\n",
      "cnt: 88400\n",
      "loss: 104.47284324645996\n",
      "cnt: 88500\n",
      "loss: 110.12360599517822\n",
      "cnt: 88600\n",
      "loss: 109.76889604568481\n",
      "cnt: 88700\n",
      "loss: 111.89652327537537\n",
      "cnt: 88800\n",
      "loss: 107.29014465808868\n",
      "cnt: 88900\n",
      "loss: 90.13043287277222\n",
      "cnt: 89000\n",
      "loss: 94.6566787481308\n",
      "cnt: 89100\n",
      "loss: 105.35758347511292\n",
      "cnt: 89200\n",
      "loss: 96.70883444786072\n",
      "cnt: 89300\n",
      "loss: 106.09152832508087\n",
      "cnt: 89400\n",
      "loss: 99.45039900779724\n",
      "cnt: 89500\n",
      "loss: 91.0214985370636\n",
      "cnt: 89600\n",
      "loss: 102.71930820941925\n",
      "cnt: 89700\n",
      "loss: 92.47483120441437\n",
      "cnt: 89800\n",
      "loss: 95.87235721588135\n",
      "cnt: 89900\n",
      "loss: 99.13452282428742\n",
      "cnt: 90000\n",
      "loss: 108.11057109832764\n",
      "cnt: 90100\n",
      "loss: 96.02636021137238\n",
      "cnt: 90200\n",
      "loss: 102.98143564224243\n",
      "cnt: 90300\n",
      "loss: 90.1676296377182\n",
      "cnt: 90400\n",
      "loss: 104.61151616096497\n",
      "cnt: 90500\n",
      "loss: 102.86782183408737\n",
      "cnt: 90600\n",
      "loss: 110.2311817932129\n",
      "cnt: 90700\n",
      "loss: 101.36758170843125\n",
      "cnt: 90800\n",
      "loss: 100.99587530136108\n",
      "cnt: 90900\n",
      "loss: 106.60288256645202\n",
      "cnt: 91000\n",
      "loss: 103.39923592567443\n",
      "cnt: 91100\n",
      "loss: 98.25423101425172\n",
      "cnt: 91200\n",
      "loss: 111.38670799255371\n",
      "cnt: 91300\n",
      "loss: 89.07156978607178\n",
      "cnt: 91400\n",
      "loss: 100.88126017570495\n",
      "cnt: 91500\n",
      "loss: 103.7532259106636\n",
      "cnt: 91600\n",
      "loss: 102.71768095016479\n",
      "cnt: 91700\n",
      "loss: 104.90030783176422\n",
      "cnt: 91800\n",
      "loss: 102.12328835487365\n",
      "cnt: 91900\n",
      "loss: 101.67581439971924\n",
      "cnt: 92000\n",
      "loss: 108.98337611198426\n",
      "cnt: 92100\n",
      "loss: 105.69175594329835\n",
      "cnt: 92200\n",
      "loss: 104.08427147865295\n",
      "cnt: 92300\n",
      "loss: 97.48916046619415\n",
      "cnt: 92400\n",
      "loss: 93.32621708393097\n",
      "cnt: 92500\n",
      "loss: 106.23688562393188\n",
      "cnt: 92600\n",
      "loss: 96.94881804943084\n",
      "cnt: 92700\n",
      "loss: 98.76555686473847\n",
      "cnt: 92800\n",
      "loss: 98.4902068734169\n",
      "cnt: 92900\n",
      "loss: 90.78183291435242\n",
      "cnt: 93000\n",
      "loss: 104.5525380897522\n",
      "cnt: 93100\n",
      "loss: 92.29461340904236\n",
      "cnt: 93200\n",
      "loss: 92.50522171735764\n",
      "cnt: 93300\n",
      "loss: 96.96536107778549\n",
      "cnt: 93400\n",
      "loss: 108.82691440582275\n",
      "cnt: 93500\n",
      "loss: 100.30333684921264\n",
      "cnt: 93600\n",
      "loss: 105.76908443927765\n",
      "cnt: 93700\n",
      "loss: 96.28479326248168\n",
      "cnt: 93800\n",
      "loss: 101.64030447006226\n",
      "cnt: 93900\n",
      "loss: 100.88052988290787\n",
      "cnt: 94000\n",
      "loss: 101.15070953845978\n",
      "cnt: 94100\n",
      "loss: 93.12202041625977\n",
      "cnt: 94200\n",
      "loss: 103.34405755996704\n",
      "cnt: 94300\n",
      "loss: 103.38432049512863\n",
      "cnt: 94400\n",
      "loss: 106.23682741165162\n",
      "cnt: 94500\n",
      "loss: 85.53184009790421\n",
      "cnt: 94600\n",
      "loss: 96.39700170516967\n",
      "cnt: 94700\n",
      "loss: 101.78205265045166\n",
      "cnt: 94800\n",
      "loss: 96.96637207508087\n",
      "cnt: 94900\n",
      "loss: 99.2700513458252\n",
      "cnt: 95000\n",
      "loss: 95.44228122711182\n",
      "cnt: 95100\n",
      "loss: 97.00677244186402\n",
      "cnt: 95200\n",
      "loss: 101.37598011016846\n",
      "cnt: 95300\n",
      "loss: 108.92019921302796\n",
      "cnt: 95400\n",
      "loss: 102.69971530914307\n",
      "cnt: 95500\n",
      "loss: 99.57668269157409\n",
      "cnt: 95600\n",
      "loss: 104.64627259254456\n",
      "cnt: 95700\n",
      "loss: 108.223648147583\n",
      "cnt: 95800\n",
      "loss: 93.47903454780578\n",
      "cnt: 95900\n",
      "loss: 91.7250248670578\n",
      "cnt: 96000\n",
      "loss: 95.4090800333023\n",
      "cnt: 96100\n",
      "loss: 100.92697089195252\n",
      "cnt: 96200\n",
      "loss: 102.60923530578613\n",
      "cnt: 96300\n",
      "loss: 91.80985226154327\n",
      "cnt: 96400\n",
      "loss: 106.43548263549805\n",
      "cnt: 96500\n",
      "loss: 105.47997303962707\n",
      "cnt: 96600\n",
      "loss: 91.83389781951904\n",
      "cnt: 96700\n",
      "loss: 96.5580417728424\n",
      "cnt: 96800\n",
      "loss: 107.13531116485596\n",
      "cnt: 96900\n",
      "loss: 95.47511493682862\n",
      "cnt: 97000\n",
      "loss: 97.5004513835907\n",
      "cnt: 97100\n",
      "loss: 106.34663985252381\n",
      "cnt: 97200\n",
      "loss: 94.86041996002197\n",
      "cnt: 97300\n",
      "loss: 106.21380508422851\n",
      "cnt: 97400\n",
      "loss: 96.47291552066802\n",
      "cnt: 97500\n",
      "loss: 86.20089967727661\n",
      "cnt: 97600\n",
      "loss: 98.42650501251221\n",
      "cnt: 97700\n",
      "loss: 105.43322267532349\n",
      "cnt: 97800\n",
      "loss: 107.58962520599366\n",
      "cnt: 97900\n",
      "loss: 103.12302905082703\n",
      "cnt: 98000\n",
      "loss: 104.091569480896\n",
      "cnt: 98100\n",
      "loss: 98.10614706516266\n",
      "cnt: 98200\n",
      "loss: 101.43681140422821\n",
      "cnt: 98300\n",
      "loss: 100.62876108169556\n",
      "cnt: 98400\n",
      "loss: 115.39555195808411\n",
      "cnt: 98500\n",
      "loss: 101.10826856613158\n",
      "cnt: 98600\n",
      "loss: 107.78256125450135\n",
      "cnt: 98700\n",
      "loss: 101.9127587223053\n",
      "cnt: 98800\n",
      "loss: 93.57715085029602\n",
      "cnt: 98900\n",
      "loss: 98.40083318710327\n",
      "cnt: 99000\n",
      "loss: 97.74570932388306\n",
      "cnt: 99100\n",
      "loss: 106.9436850452423\n",
      "cnt: 99200\n",
      "loss: 90.84702049255371\n",
      "cnt: 99300\n",
      "loss: 88.18329258918762\n",
      "cnt: 99400\n",
      "loss: 98.0312994670868\n",
      "cnt: 99500\n",
      "loss: 98.41012387275696\n",
      "cnt: 99600\n",
      "loss: 99.88326395988464\n",
      "cnt: 99700\n",
      "loss: 98.63249062538146\n",
      "cnt: 99800\n",
      "loss: 106.20003602027893\n",
      "cnt: 99900\n",
      "loss: 87.37253880739212\n",
      "cnt: 100000\n",
      "loss: 101.72660623073578\n",
      "cnt: 100100\n",
      "loss: 96.01261622428893\n",
      "cnt: 100200\n",
      "loss: 96.71913557767868\n",
      "cnt: 100300\n",
      "loss: 108.39151446342468\n",
      "cnt: 100400\n",
      "loss: 90.83024563550948\n",
      "cnt: 100500\n",
      "loss: 98.68100241184234\n",
      "cnt: 100600\n",
      "loss: 93.6797276878357\n",
      "cnt: 100700\n",
      "loss: 108.81723629951478\n",
      "cnt: 100800\n",
      "loss: 92.70765221595764\n",
      "cnt: 100900\n",
      "loss: 102.29002335548401\n",
      "cnt: 101000\n",
      "loss: 94.45369969367981\n",
      "cnt: 101100\n",
      "loss: 99.5814250946045\n",
      "cnt: 101200\n",
      "loss: 103.22267032146453\n",
      "cnt: 101300\n",
      "loss: 101.16889439582825\n",
      "cnt: 101400\n",
      "loss: 111.47652032852173\n",
      "cnt: 101500\n",
      "loss: 101.5996385383606\n",
      "cnt: 101600\n",
      "loss: 93.65731331825256\n",
      "cnt: 101700\n",
      "loss: 102.20050120592117\n",
      "cnt: 101800\n",
      "loss: 98.71775464057923\n",
      "cnt: 101900\n",
      "loss: 96.1154245185852\n",
      "cnt: 102000\n",
      "loss: 102.13423846244812\n",
      "cnt: 102100\n",
      "loss: 94.35630349159241\n",
      "cnt: 102200\n",
      "loss: 100.09724334001541\n",
      "cnt: 102300\n",
      "loss: 107.26749387741089\n",
      "cnt: 102400\n",
      "loss: 89.52433323860168\n",
      "cnt: 102500\n",
      "loss: 100.79622240066529\n",
      "cnt: 102600\n",
      "loss: 96.7660963344574\n",
      "cnt: 102700\n",
      "loss: 94.6889003944397\n",
      "cnt: 102800\n",
      "loss: 106.3339488697052\n",
      "cnt: 102900\n",
      "loss: 91.81602546691894\n",
      "cnt: 103000\n",
      "loss: 98.22055223464966\n",
      "cnt: 103100\n",
      "loss: 107.41414472579956\n",
      "cnt: 103200\n",
      "loss: 113.61684669494629\n",
      "cnt: 103300\n",
      "loss: 103.89038848400116\n",
      "cnt: 103400\n",
      "loss: 105.47735765457153\n",
      "cnt: 103500\n",
      "loss: 84.03811567544938\n",
      "cnt: 103600\n",
      "loss: 100.64686923503876\n",
      "cnt: 103700\n",
      "loss: 95.06010922908783\n",
      "cnt: 103800\n",
      "loss: 100.07119565963745\n",
      "cnt: 103900\n",
      "loss: 103.25208516120911\n",
      "cnt: 104000\n",
      "loss: 97.50867249727249\n",
      "cnt: 104100\n",
      "loss: 98.49746569633484\n",
      "cnt: 104200\n",
      "loss: 106.23900194168091\n",
      "cnt: 104300\n",
      "loss: 104.76438841819763\n",
      "cnt: 104400\n",
      "loss: 96.31325194358826\n",
      "cnt: 104500\n",
      "loss: 97.13936606407165\n",
      "cnt: 104600\n",
      "loss: 95.13680556297302\n",
      "cnt: 104700\n",
      "loss: 101.32466627120972\n",
      "cnt: 104800\n",
      "loss: 102.32172297477722\n",
      "cnt: 104900\n",
      "loss: 99.92348595619201\n",
      "cnt: 105000\n",
      "loss: 102.44545775413513\n",
      "cnt: 105100\n",
      "loss: 106.37309895515442\n",
      "cnt: 105200\n",
      "loss: 92.10336840629577\n",
      "cnt: 105300\n",
      "loss: 99.37142122507095\n",
      "cnt: 105400\n",
      "loss: 88.80833226680755\n",
      "cnt: 105500\n",
      "loss: 101.2269749736786\n",
      "cnt: 105600\n",
      "loss: 102.1746577835083\n",
      "cnt: 105700\n",
      "loss: 99.0447492480278\n",
      "cnt: 105800\n",
      "loss: 97.34416965007782\n",
      "cnt: 105900\n",
      "loss: 91.9926748919487\n",
      "cnt: 106000\n",
      "loss: 96.18892220020294\n",
      "cnt: 106100\n",
      "loss: 102.82266807556152\n",
      "cnt: 106200\n",
      "loss: 101.77994062423706\n",
      "cnt: 106300\n",
      "loss: 101.98191879272461\n",
      "cnt: 106400\n",
      "loss: 97.2440026140213\n",
      "cnt: 106500\n",
      "loss: 101.4014319896698\n",
      "cnt: 106600\n",
      "loss: 93.63153412818909\n",
      "cnt: 106700\n",
      "loss: 92.83832874774933\n",
      "cnt: 106800\n",
      "loss: 104.0330332493782\n",
      "cnt: 106900\n",
      "loss: 93.12396339416505\n",
      "cnt: 107000\n",
      "loss: 95.11250198364257\n",
      "cnt: 107100\n",
      "loss: 103.16135966300965\n",
      "cnt: 107200\n",
      "loss: 103.35144843578338\n",
      "cnt: 107300\n",
      "loss: 91.31057943344116\n",
      "cnt: 107400\n",
      "loss: 109.0355650138855\n",
      "cnt: 107500\n",
      "loss: 108.05581773281098\n",
      "cnt: 107600\n",
      "loss: 97.40307601928711\n",
      "cnt: 107700\n",
      "loss: 107.5356717824936\n",
      "cnt: 107800\n",
      "loss: 98.92424869537354\n",
      "cnt: 107900\n",
      "loss: 101.67853212594986\n",
      "cnt: 108000\n",
      "loss: 99.84495176315308\n",
      "cnt: 108100\n",
      "loss: 102.0346570622921\n",
      "cnt: 108200\n",
      "loss: 97.89648996114731\n",
      "cnt: 108300\n",
      "loss: 97.7981728363037\n",
      "cnt: 108400\n",
      "loss: 94.76725241661072\n",
      "cnt: 108500\n",
      "loss: 100.58253636360169\n",
      "cnt: 108600\n",
      "loss: 103.60392556905747\n",
      "cnt: 108700\n",
      "loss: 109.90826581001282\n",
      "cnt: 108800\n",
      "loss: 98.50396702289581\n",
      "cnt: 108900\n",
      "loss: 92.60677083969117\n",
      "cnt: 109000\n",
      "loss: 103.89378560066223\n",
      "cnt: 109100\n",
      "loss: 100.32304865837096\n",
      "cnt: 109200\n",
      "loss: 96.96881692886353\n",
      "cnt: 109300\n",
      "loss: 111.67162353038788\n",
      "cnt: 109400\n",
      "loss: 102.48340320587158\n",
      "cnt: 109500\n",
      "loss: 97.33056258201599\n",
      "cnt: 109600\n",
      "loss: 102.13795585155486\n",
      "cnt: 109700\n",
      "loss: 100.54256432533265\n",
      "cnt: 109800\n",
      "loss: 103.01483013629914\n",
      "cnt: 109900\n",
      "loss: 100.10945077896118\n",
      "cnt: 110000\n",
      "loss: 100.25433149337769\n",
      "cnt: 110100\n",
      "loss: 102.73573061943054\n",
      "cnt: 110200\n",
      "loss: 101.73822032928467\n",
      "cnt: 110300\n",
      "loss: 95.07722979545593\n",
      "cnt: 110400\n",
      "loss: 96.80114416122437\n",
      "cnt: 110500\n",
      "loss: 105.21632483482361\n",
      "cnt: 110600\n",
      "loss: 100.12176594734191\n",
      "cnt: 110700\n",
      "loss: 111.83833425998688\n",
      "cnt: 110800\n",
      "loss: 106.57846105575561\n",
      "cnt: 110900\n",
      "loss: 89.3208017206192\n",
      "cnt: 111000\n",
      "loss: 89.06080819129944\n",
      "cnt: 111100\n",
      "loss: 97.99474920272827\n",
      "cnt: 111200\n",
      "loss: 103.60309566497803\n",
      "cnt: 111300\n",
      "loss: 101.23249277830124\n",
      "cnt: 111400\n",
      "loss: 98.67495734214782\n",
      "cnt: 111500\n",
      "loss: 95.453598446846\n",
      "cnt: 111600\n",
      "loss: 93.87597135543824\n",
      "cnt: 111700\n",
      "loss: 105.11343589782715\n",
      "cnt: 111800\n",
      "loss: 103.45165335655213\n",
      "cnt: 111900\n",
      "loss: 111.24839104652405\n",
      "cnt: 112000\n",
      "loss: 102.13123908519745\n",
      "cnt: 112100\n",
      "loss: 103.34444240570069\n",
      "cnt: 112200\n",
      "loss: 101.11563618421555\n",
      "cnt: 112300\n",
      "loss: 101.38955528259277\n",
      "cnt: 112400\n",
      "loss: 100.51189283370972\n",
      "cnt: 112500\n",
      "loss: 94.49249647140503\n",
      "cnt: 112600\n",
      "loss: 96.4382901763916\n",
      "cnt: 112700\n",
      "loss: 103.51898265838624\n",
      "cnt: 112800\n",
      "loss: 94.04865903377532\n",
      "cnt: 112900\n",
      "loss: 103.08501650810241\n",
      "cnt: 113000\n",
      "loss: 89.77276352643966\n",
      "cnt: 113100\n",
      "loss: 93.04984245181083\n",
      "cnt: 113200\n",
      "loss: 106.56036897659301\n",
      "cnt: 113300\n",
      "loss: 92.95158684253693\n",
      "cnt: 113400\n",
      "loss: 98.85226362228394\n",
      "cnt: 113500\n",
      "loss: 97.63859564304352\n",
      "cnt: 113600\n",
      "loss: 94.24742221832275\n",
      "cnt: 113700\n",
      "loss: 98.82673412322998\n",
      "cnt: 113800\n",
      "loss: 99.26542136907578\n",
      "cnt: 113900\n",
      "loss: 94.07696083307266\n",
      "cnt: 114000\n",
      "loss: 98.16712542057037\n",
      "cnt: 114100\n",
      "loss: 105.96319290161132\n",
      "cnt: 114200\n",
      "loss: 103.10648639678955\n",
      "cnt: 114300\n",
      "loss: 104.04254346847534\n",
      "cnt: 114400\n",
      "loss: 102.69730872154236\n",
      "cnt: 114500\n",
      "loss: 100.97293400645256\n",
      "cnt: 114600\n",
      "loss: 106.91573591709137\n",
      "cnt: 114700\n",
      "loss: 107.06880911827088\n",
      "cnt: 114800\n",
      "loss: 94.31645022392273\n",
      "cnt: 114900\n",
      "loss: 96.56604351997376\n",
      "cnt: 115000\n",
      "loss: 99.79947771549224\n",
      "cnt: 115100\n",
      "loss: 96.97187387228013\n",
      "cnt: 115200\n",
      "loss: 103.83277598381042\n",
      "cnt: 115300\n",
      "loss: 97.36331797838211\n",
      "cnt: 115400\n",
      "loss: 93.84155023097992\n",
      "cnt: 115500\n",
      "loss: 88.23901002407074\n",
      "cnt: 115600\n",
      "loss: 102.80821458816528\n",
      "cnt: 115700\n",
      "loss: 106.55121592998505\n",
      "cnt: 115800\n",
      "loss: 100.79433832406998\n",
      "cnt: 115900\n",
      "loss: 96.64675346374511\n",
      "cnt: 116000\n",
      "loss: 92.66774570465088\n",
      "cnt: 116100\n",
      "loss: 104.5824832725525\n",
      "cnt: 116200\n",
      "loss: 99.91838207244874\n",
      "cnt: 116300\n",
      "loss: 95.66259000778199\n",
      "cnt: 116400\n",
      "loss: 102.38720276355744\n",
      "cnt: 116500\n",
      "loss: 103.72127389907837\n",
      "cnt: 116600\n",
      "loss: 89.95657108306885\n",
      "cnt: 116700\n",
      "loss: 100.54583042144776\n",
      "cnt: 116800\n",
      "loss: 103.37203083992004\n",
      "cnt: 116900\n",
      "loss: 97.26558667182923\n",
      "cnt: 117000\n",
      "loss: 105.22764029026031\n",
      "cnt: 117100\n",
      "loss: 102.83290063858033\n",
      "cnt: 117200\n",
      "loss: 86.02920411109925\n",
      "cnt: 117300\n",
      "loss: 97.70178159713745\n",
      "cnt: 117400\n",
      "loss: 103.07421664714813\n",
      "cnt: 117500\n",
      "loss: 103.0840050983429\n",
      "cnt: 117600\n",
      "loss: 101.24020714759827\n",
      "cnt: 117700\n",
      "loss: 107.19829988479614\n",
      "cnt: 117800\n",
      "loss: 107.02385894775391\n",
      "cnt: 117900\n",
      "loss: 99.12780406951904\n",
      "cnt: 118000\n",
      "loss: 94.42469187259674\n",
      "cnt: 118100\n",
      "loss: 95.51000134706497\n",
      "cnt: 118200\n",
      "loss: 99.09281871318817\n",
      "cnt: 118300\n",
      "loss: 102.73188446998596\n",
      "cnt: 118400\n",
      "loss: 95.23688071489335\n",
      "cnt: 118500\n",
      "loss: 99.03538102149963\n",
      "cnt: 118600\n",
      "loss: 96.47275289535523\n",
      "cnt: 118700\n",
      "loss: 93.91295111656189\n",
      "cnt: 118800\n",
      "loss: 92.90057862281799\n",
      "cnt: 118900\n",
      "loss: 87.84372439384461\n",
      "cnt: 119000\n",
      "loss: 92.18608613014221\n",
      "cnt: 119100\n",
      "loss: 100.9709086227417\n",
      "cnt: 119200\n",
      "loss: 99.541758518219\n",
      "cnt: 119300\n",
      "loss: 100.07162881851197\n",
      "cnt: 119400\n",
      "loss: 103.00653985977173\n",
      "cnt: 119500\n",
      "loss: 93.0286887550354\n",
      "cnt: 119600\n",
      "loss: 95.9336808514595\n",
      "cnt: 119700\n",
      "loss: 95.4085695195198\n",
      "cnt: 119800\n",
      "loss: 95.9978911781311\n",
      "cnt: 119900\n",
      "loss: 112.1674451828003\n",
      "cnt: 120000\n",
      "loss: 88.60784124374389\n",
      "cnt: 120100\n",
      "loss: 91.97213706016541\n",
      "cnt: 120200\n",
      "loss: 102.8758946800232\n",
      "cnt: 120300\n",
      "loss: 105.73631383895874\n",
      "cnt: 120400\n",
      "loss: 105.33852645397187\n",
      "cnt: 120500\n",
      "loss: 101.3072311925888\n",
      "cnt: 120600\n",
      "loss: 106.54712673425675\n",
      "cnt: 120700\n",
      "loss: 108.43525116443634\n",
      "cnt: 120800\n",
      "loss: 109.0397774887085\n",
      "cnt: 120900\n",
      "loss: 96.80392078518868\n",
      "cnt: 121000\n",
      "loss: 101.0915092587471\n",
      "cnt: 121100\n",
      "loss: 104.83276417255402\n",
      "cnt: 121200\n",
      "loss: 96.65916430473328\n",
      "cnt: 121300\n",
      "loss: 99.60797413349151\n",
      "cnt: 121400\n",
      "loss: 99.45108930587769\n",
      "cnt: 121500\n",
      "loss: 89.54055827856064\n",
      "cnt: 121600\n",
      "loss: 98.68439016342163\n",
      "cnt: 121700\n",
      "loss: 93.52283829450607\n",
      "cnt: 121800\n",
      "loss: 99.3124880027771\n",
      "cnt: 121900\n",
      "loss: 96.2980444431305\n",
      "cnt: 122000\n",
      "loss: 102.82596829414368\n",
      "cnt: 122100\n",
      "loss: 99.73085981369019\n",
      "cnt: 122200\n",
      "loss: 109.91241127967834\n",
      "cnt: 122300\n",
      "loss: 95.56273095846176\n",
      "cnt: 122400\n",
      "loss: 99.35083893299102\n",
      "cnt: 122500\n",
      "loss: 104.63881896495819\n",
      "cnt: 122600\n",
      "loss: 98.77693046569824\n",
      "cnt: 122700\n",
      "loss: 107.76504051208497\n",
      "cnt: 122800\n",
      "loss: 103.58855575561523\n",
      "cnt: 122900\n",
      "loss: 104.72269309997559\n",
      "cnt: 123000\n",
      "loss: 89.35509541034699\n",
      "cnt: 123100\n",
      "loss: 105.6967779135704\n",
      "cnt: 123200\n",
      "loss: 101.60485569953919\n",
      "cnt: 123300\n",
      "loss: 89.39025933265685\n",
      "cnt: 123400\n",
      "loss: 103.29854954719544\n",
      "cnt: 123500\n",
      "loss: 108.7840109205246\n",
      "cnt: 123600\n",
      "loss: 99.66288166999817\n",
      "cnt: 123700\n",
      "loss: 110.40118658065796\n",
      "cnt: 123800\n",
      "loss: 102.55770047187805\n",
      "cnt: 123900\n",
      "loss: 97.88868859291077\n",
      "cnt: 124000\n",
      "loss: 96.87518415451049\n",
      "cnt: 124100\n",
      "loss: 102.32280099868774\n",
      "cnt: 124200\n",
      "loss: 98.52497292518616\n",
      "cnt: 124300\n",
      "loss: 104.31438012123108\n",
      "cnt: 124400\n",
      "loss: 96.69750821828842\n",
      "cnt: 124500\n",
      "loss: 96.80357803344727\n",
      "cnt: 124600\n",
      "loss: 97.50577447891236\n",
      "cnt: 124700\n",
      "loss: 97.4984456372261\n",
      "cnt: 124800\n",
      "loss: 100.20202417135239\n",
      "cnt: 124900\n",
      "loss: 101.13642896175385\n",
      "cnt: 125000\n",
      "loss: 95.61811686992645\n",
      "cnt: 125100\n",
      "loss: 99.72428939342498\n",
      "cnt: 125200\n",
      "loss: 101.15655514717102\n",
      "cnt: 125300\n",
      "loss: 102.82733552217483\n",
      "cnt: 125400\n",
      "loss: 106.35514755249024\n",
      "cnt: 125500\n",
      "loss: 104.59807713508606\n",
      "cnt: 125600\n",
      "loss: 93.15132335662842\n",
      "cnt: 125700\n",
      "loss: 106.73986143112182\n",
      "cnt: 125800\n",
      "loss: 100.92539434432983\n",
      "cnt: 125900\n",
      "loss: 101.81951058864594\n",
      "cnt: 126000\n",
      "loss: 94.06129383087158\n",
      "cnt: 126100\n",
      "loss: 90.15209708929062\n",
      "cnt: 126200\n",
      "loss: 99.70115049839019\n",
      "cnt: 126300\n",
      "loss: 105.6874130153656\n",
      "cnt: 126400\n",
      "loss: 107.33374694347381\n",
      "cnt: 126500\n",
      "loss: 100.18864253997803\n",
      "cnt: 126600\n",
      "loss: 99.90619742870331\n",
      "cnt: 126700\n",
      "loss: 104.07560459136963\n",
      "cnt: 126800\n",
      "loss: 109.4672341632843\n",
      "cnt: 126900\n",
      "loss: 101.56283252716065\n",
      "cnt: 127000\n",
      "loss: 95.88243782997131\n",
      "cnt: 127100\n",
      "loss: 100.47770915269852\n",
      "cnt: 127200\n",
      "loss: 97.44420108318329\n",
      "cnt: 127300\n",
      "loss: 100.09026849269867\n",
      "cnt: 127400\n",
      "loss: 103.23752342224121\n",
      "cnt: 127500\n",
      "loss: 97.96982977867127\n",
      "cnt: 127600\n",
      "loss: 100.09517217636109\n",
      "cnt: 127700\n",
      "loss: 111.36125891685487\n",
      "cnt: 127800\n",
      "loss: 98.47277387619019\n",
      "cnt: 127900\n",
      "loss: 96.44855460643768\n",
      "cnt: 128000\n",
      "loss: 102.68732292175292\n",
      "cnt: 128100\n",
      "loss: 97.85844068527221\n",
      "cnt: 128200\n",
      "loss: 108.78375666856766\n",
      "cnt: 128300\n",
      "loss: 102.30671065330506\n",
      "cnt: 128400\n",
      "loss: 87.84889820098877\n",
      "cnt: 128500\n",
      "loss: 98.66809672355652\n",
      "cnt: 128600\n",
      "loss: 99.20602613449097\n",
      "cnt: 128700\n",
      "loss: 105.4837444806099\n",
      "cnt: 128800\n",
      "loss: 100.57157590866089\n",
      "cnt: 128900\n",
      "loss: 93.77259466648101\n",
      "cnt: 129000\n",
      "loss: 98.74076871871948\n",
      "cnt: 129100\n",
      "loss: 112.34626150131226\n",
      "cnt: 129200\n",
      "loss: 92.97591296195984\n",
      "cnt: 129300\n",
      "loss: 96.0919342327118\n",
      "cnt: 129400\n",
      "loss: 102.03795498847961\n",
      "cnt: 129500\n",
      "loss: 102.98899080276489\n",
      "cnt: 129600\n",
      "loss: 112.79821590900421\n",
      "cnt: 129700\n",
      "loss: 97.22593121051789\n",
      "cnt: 129800\n",
      "loss: 95.3451201057434\n",
      "cnt: 129900\n",
      "loss: 98.8763541507721\n",
      "cnt: 130000\n",
      "loss: 95.5184724855423\n",
      "cnt: 130100\n",
      "loss: 98.25490220546722\n",
      "cnt: 130200\n",
      "loss: 100.3505471611023\n",
      "cnt: 130300\n",
      "loss: 97.31660797357559\n",
      "cnt: 130400\n",
      "loss: 97.92378685951233\n",
      "cnt: 130500\n",
      "loss: 95.13353798389434\n",
      "cnt: 130600\n",
      "loss: 104.47369683265686\n",
      "cnt: 130700\n",
      "loss: 96.25007672786712\n",
      "cnt: 130800\n",
      "loss: 103.16044906139373\n",
      "cnt: 130900\n",
      "loss: 96.72559656143189\n",
      "cnt: 131000\n",
      "loss: 92.96911924600602\n",
      "cnt: 131100\n",
      "loss: 90.8460249710083\n",
      "cnt: 131200\n",
      "loss: 104.0588113307953\n",
      "cnt: 131300\n",
      "loss: 88.8293436050415\n",
      "cnt: 131400\n",
      "loss: 96.60258383274078\n",
      "cnt: 131500\n",
      "loss: 94.14620605945588\n",
      "cnt: 131600\n",
      "loss: 98.23438189506531\n",
      "cnt: 131700\n",
      "loss: 92.71089359283447\n",
      "cnt: 131800\n",
      "loss: 98.6269859790802\n",
      "cnt: 131900\n",
      "loss: 106.93164530754089\n",
      "cnt: 132000\n",
      "loss: 93.85990654706956\n",
      "cnt: 132100\n",
      "loss: 84.16954290151597\n",
      "cnt: 132200\n",
      "loss: 109.97260769844056\n",
      "cnt: 132300\n",
      "loss: 102.78872911930084\n",
      "cnt: 132400\n",
      "loss: 93.37053377628327\n",
      "cnt: 132500\n",
      "loss: 105.6637500667572\n",
      "cnt: 132600\n",
      "loss: 94.04106573104859\n",
      "cnt: 132700\n",
      "loss: 102.93465192317963\n",
      "cnt: 132800\n",
      "loss: 101.19328260421753\n",
      "cnt: 132900\n",
      "loss: 94.06370213508606\n",
      "cnt: 133000\n",
      "loss: 89.12501872539521\n",
      "cnt: 133100\n",
      "loss: 103.72082111358642\n",
      "cnt: 133200\n",
      "loss: 96.14707508087159\n",
      "cnt: 133300\n",
      "loss: 98.15862293720245\n",
      "cnt: 133400\n",
      "loss: 94.89735815525054\n",
      "cnt: 133500\n",
      "loss: 96.35239658355712\n",
      "cnt: 133600\n",
      "loss: 97.91712962150574\n",
      "cnt: 133700\n",
      "loss: 109.23803666114807\n",
      "cnt: 133800\n",
      "loss: 89.72518918991089\n",
      "cnt: 133900\n",
      "loss: 105.46961349487304\n",
      "cnt: 134000\n",
      "loss: 101.8798471069336\n",
      "cnt: 134100\n",
      "loss: 93.42675657272339\n",
      "cnt: 134200\n",
      "loss: 97.52693466186524\n",
      "cnt: 134300\n",
      "loss: 102.30016904830933\n",
      "cnt: 134400\n",
      "loss: 94.88093108177185\n",
      "cnt: 134500\n",
      "loss: 104.01432711601257\n",
      "cnt: 134600\n",
      "loss: 109.17285830497741\n",
      "cnt: 134700\n",
      "loss: 98.43079123973847\n",
      "cnt: 134800\n",
      "loss: 108.51555768966675\n",
      "cnt: 134900\n",
      "loss: 100.17693195343017\n",
      "cnt: 135000\n",
      "loss: 113.20273973703384\n",
      "cnt: 135100\n",
      "loss: 107.1288245010376\n",
      "cnt: 135200\n",
      "loss: 108.74392700195312\n",
      "cnt: 135300\n",
      "loss: 104.76348539352416\n",
      "cnt: 135400\n",
      "loss: 97.60395466804505\n",
      "cnt: 135500\n",
      "loss: 102.15596457481384\n",
      "cnt: 135600\n",
      "loss: 97.51341187000274\n",
      "cnt: 135700\n",
      "loss: 99.3889091014862\n",
      "cnt: 135800\n",
      "loss: 109.67557566165924\n",
      "cnt: 135900\n",
      "loss: 97.9570012331009\n",
      "cnt: 136000\n",
      "loss: 89.68857841968537\n",
      "cnt: 136100\n",
      "loss: 90.94932539701462\n",
      "cnt: 136200\n",
      "loss: 96.93748467445374\n",
      "cnt: 136300\n",
      "loss: 94.89127995014191\n",
      "cnt: 136400\n",
      "loss: 106.08571949958801\n",
      "cnt: 136500\n",
      "loss: 99.70791544914246\n",
      "cnt: 136600\n",
      "loss: 97.19991945505143\n",
      "cnt: 136700\n",
      "loss: 103.86621740341187\n",
      "cnt: 136800\n",
      "loss: 102.3217151069641\n",
      "cnt: 136900\n",
      "loss: 108.08885422706604\n",
      "cnt: 137000\n",
      "loss: 105.16038063049317\n",
      "cnt: 137100\n",
      "loss: 96.19805153846741\n",
      "cnt: 137200\n",
      "loss: 110.88212513923645\n",
      "cnt: 137300\n",
      "loss: 103.20409129142762\n",
      "cnt: 137400\n",
      "loss: 90.9770987033844\n",
      "cnt: 137500\n",
      "loss: 104.42949699401855\n",
      "cnt: 137600\n",
      "loss: 96.06440334796906\n",
      "cnt: 137700\n",
      "loss: 98.96566319465637\n",
      "cnt: 137800\n",
      "loss: 104.53666877746582\n",
      "cnt: 137900\n",
      "loss: 104.09992032051086\n",
      "cnt: 138000\n",
      "loss: 100.59214583396911\n",
      "cnt: 138100\n",
      "loss: 101.77246559143066\n",
      "cnt: 138200\n",
      "loss: 94.26323347568513\n",
      "cnt: 138300\n",
      "loss: 103.82406490325928\n",
      "cnt: 138400\n",
      "loss: 96.07858038902283\n",
      "cnt: 138500\n",
      "loss: 99.75861111164093\n",
      "cnt: 138600\n",
      "loss: 100.96151683092117\n",
      "cnt: 138700\n",
      "loss: 99.15371377944946\n",
      "cnt: 138800\n",
      "loss: 100.60933687210083\n",
      "cnt: 138900\n",
      "loss: 102.41747604370117\n",
      "cnt: 139000\n",
      "loss: 93.78126266002656\n",
      "cnt: 139100\n",
      "loss: 95.77412014961243\n",
      "cnt: 139200\n",
      "loss: 100.90282760620117\n",
      "cnt: 139300\n",
      "loss: 101.11845688819885\n",
      "cnt: 139400\n",
      "loss: 103.66798793554307\n",
      "cnt: 139500\n",
      "loss: 108.16578488349914\n",
      "cnt: 139600\n",
      "loss: 98.94317691802979\n",
      "cnt: 139700\n",
      "loss: 98.3682442855835\n",
      "cnt: 139800\n",
      "loss: 97.5267202758789\n",
      "cnt: 139900\n",
      "loss: 96.89419982910157\n",
      "cnt: 140000\n",
      "loss: 101.20946321487426\n",
      "cnt: 140100\n",
      "loss: 115.98907336950302\n",
      "cnt: 140200\n",
      "loss: 97.25833026885986\n",
      "cnt: 140300\n",
      "loss: 94.39645374298095\n",
      "cnt: 140400\n",
      "loss: 91.3025334262848\n",
      "cnt: 140500\n",
      "loss: 83.96951671361923\n",
      "cnt: 140600\n",
      "loss: 107.75361078739166\n",
      "cnt: 140700\n",
      "loss: 106.26364419460296\n",
      "cnt: 140800\n",
      "loss: 100.55463479995727\n",
      "cnt: 140900\n",
      "loss: 96.01116242408753\n",
      "cnt: 141000\n",
      "loss: 93.96847945213318\n",
      "cnt: 141100\n",
      "loss: 105.48344429969788\n",
      "cnt: 141200\n",
      "loss: 102.16388343811035\n",
      "cnt: 141300\n",
      "loss: 99.92301122426987\n",
      "cnt: 141400\n",
      "loss: 88.61962318658829\n",
      "cnt: 141500\n",
      "loss: 89.35554678916931\n",
      "cnt: 141600\n",
      "loss: 98.05327513933182\n",
      "cnt: 141700\n",
      "loss: 101.32729955673217\n",
      "cnt: 141800\n",
      "loss: 94.68514389514922\n",
      "cnt: 141900\n",
      "loss: 113.25243978023529\n",
      "cnt: 142000\n",
      "loss: 96.40565675735473\n",
      "cnt: 142100\n",
      "loss: 104.17716160297394\n",
      "cnt: 142200\n",
      "loss: 98.25984780550003\n",
      "cnt: 142300\n",
      "loss: 108.22913562774659\n",
      "cnt: 142400\n",
      "loss: 96.73569023609161\n",
      "cnt: 142500\n",
      "loss: 95.17105543136597\n",
      "cnt: 142600\n",
      "loss: 86.63923493385315\n",
      "cnt: 142700\n",
      "loss: 105.47983852386474\n",
      "cnt: 142800\n",
      "loss: 97.29987812042236\n",
      "cnt: 142900\n",
      "loss: 97.60722821712494\n",
      "cnt: 143000\n",
      "loss: 101.74580667495728\n",
      "cnt: 143100\n",
      "loss: 100.88175130844117\n",
      "cnt: 143200\n",
      "loss: 103.30173245429992\n",
      "cnt: 143300\n",
      "loss: 107.19227482795715\n",
      "cnt: 143400\n",
      "loss: 97.6769679403305\n",
      "cnt: 143500\n",
      "loss: 106.13037453651428\n",
      "cnt: 143600\n",
      "loss: 102.76939569473267\n",
      "cnt: 143700\n",
      "loss: 105.37210859775543\n",
      "cnt: 143800\n",
      "loss: 91.52292204856873\n",
      "cnt: 143900\n",
      "loss: 94.55108106613159\n",
      "cnt: 144000\n",
      "loss: 103.01366716384888\n",
      "cnt: 144100\n",
      "loss: 98.21123458862304\n",
      "cnt: 144200\n",
      "loss: 86.37256630420684\n",
      "cnt: 144300\n",
      "loss: 101.82149402618408\n",
      "cnt: 144400\n",
      "loss: 89.81594127655029\n",
      "cnt: 144500\n",
      "loss: 99.9517987370491\n",
      "cnt: 144600\n",
      "loss: 103.96470310211181\n",
      "cnt: 144700\n",
      "loss: 97.08570018053055\n",
      "cnt: 144800\n",
      "loss: 94.01771152496337\n",
      "cnt: 144900\n",
      "loss: 94.63410833835601\n",
      "cnt: 145000\n",
      "loss: 85.7767216205597\n",
      "cnt: 145100\n",
      "loss: 100.80424384117127\n",
      "cnt: 145200\n",
      "loss: 102.18381408691407\n",
      "cnt: 145300\n",
      "loss: 96.36424970626831\n",
      "cnt: 145400\n",
      "loss: 96.4904271888733\n",
      "cnt: 145500\n",
      "loss: 100.81932576179504\n",
      "cnt: 145600\n",
      "loss: 100.37645648956298\n",
      "cnt: 145700\n",
      "loss: 97.34591063022613\n",
      "cnt: 145800\n",
      "loss: 112.48883180618286\n",
      "cnt: 145900\n",
      "loss: 95.3312665462494\n",
      "cnt: 146000\n",
      "loss: 98.32436913728714\n",
      "cnt: 146100\n",
      "loss: 105.50008481025696\n",
      "cnt: 146200\n",
      "loss: 105.85374500513076\n",
      "cnt: 146300\n",
      "loss: 109.92052499055862\n",
      "cnt: 146400\n",
      "loss: 98.25374051094055\n",
      "cnt: 146500\n",
      "loss: 108.31566995382309\n",
      "cnt: 146600\n",
      "loss: 89.93465369224549\n",
      "cnt: 146700\n",
      "loss: 98.6466740989685\n",
      "cnt: 146800\n",
      "loss: 101.60423484802246\n",
      "cnt: 146900\n",
      "loss: 99.51462531805038\n",
      "cnt: 147000\n",
      "loss: 97.48859806060791\n",
      "cnt: 147100\n",
      "loss: 91.0897754573822\n",
      "cnt: 147200\n",
      "loss: 97.32753824234008\n",
      "cnt: 147300\n",
      "loss: 106.02589646339416\n",
      "cnt: 147400\n",
      "loss: 101.2413296532631\n",
      "cnt: 147500\n",
      "loss: 89.73174981117249\n",
      "cnt: 147600\n",
      "loss: 104.17115083694458\n",
      "cnt: 147700\n",
      "loss: 103.79724860429764\n",
      "cnt: 147800\n",
      "loss: 89.34770243644715\n",
      "cnt: 147900\n",
      "loss: 102.78565015077591\n",
      "cnt: 148000\n",
      "loss: 108.96251279830932\n",
      "cnt: 148100\n",
      "loss: 103.16489706993103\n",
      "cnt: 148200\n",
      "loss: 98.11668940544128\n",
      "cnt: 148300\n",
      "loss: 102.91191269874572\n",
      "cnt: 148400\n",
      "loss: 90.49257939338685\n",
      "cnt: 148500\n",
      "loss: 96.37338797569275\n",
      "cnt: 148600\n",
      "loss: 89.241797580719\n",
      "cnt: 148700\n",
      "loss: 106.66984875679016\n",
      "cnt: 148800\n",
      "loss: 102.50386984825134\n",
      "cnt: 148900\n",
      "loss: 89.87823240280152\n",
      "cnt: 149000\n",
      "loss: 100.16291741371155\n",
      "cnt: 149100\n",
      "loss: 96.90392191171647\n",
      "cnt: 149200\n",
      "loss: 98.76034487962723\n",
      "cnt: 149300\n",
      "loss: 98.14105711460114\n",
      "cnt: 149400\n",
      "loss: 100.4916396522522\n",
      "cnt: 149500\n",
      "loss: 111.38314723968506\n",
      "cnt: 149600\n",
      "loss: 105.5038541507721\n",
      "cnt: 149700\n",
      "loss: 89.24405284881591\n",
      "cnt: 149800\n",
      "loss: 96.85367849349976\n",
      "cnt: 149900\n",
      "loss: 81.84772378921508\n",
      "cnt: 150000\n",
      "loss: 103.01720359802246\n",
      "cnt: 150100\n",
      "loss: 100.1763291168213\n",
      "cnt: 150200\n",
      "loss: 102.95534376621247\n",
      "cnt: 150300\n",
      "loss: 90.34606004714966\n",
      "cnt: 150400\n",
      "loss: 100.09367178440094\n",
      "cnt: 150500\n",
      "loss: 109.14770018577576\n",
      "cnt: 150600\n",
      "loss: 97.53454602241516\n",
      "cnt: 150700\n",
      "loss: 86.62969514369965\n",
      "cnt: 150800\n",
      "loss: 99.01342283248901\n",
      "cnt: 150900\n",
      "loss: 108.75134517669677\n",
      "cnt: 151000\n",
      "loss: 97.62565551757812\n",
      "cnt: 151100\n",
      "loss: 106.09415281295776\n",
      "cnt: 151200\n",
      "loss: 99.82416906356812\n",
      "cnt: 151300\n",
      "loss: 99.41003391742706\n",
      "cnt: 151400\n",
      "loss: 99.75157047271729\n",
      "cnt: 151500\n",
      "loss: 100.16888483047485\n",
      "cnt: 151600\n",
      "loss: 102.00874654769898\n",
      "cnt: 151700\n",
      "loss: 98.61175631523132\n",
      "cnt: 151800\n",
      "loss: 105.43266319274902\n",
      "cnt: 151900\n",
      "loss: 101.88742444992066\n",
      "cnt: 152000\n",
      "loss: 108.63506528615952\n",
      "cnt: 152100\n",
      "loss: 96.95533915281295\n",
      "cnt: 152200\n",
      "loss: 95.6675528049469\n",
      "cnt: 152300\n",
      "loss: 109.04267863035201\n",
      "cnt: 152400\n",
      "loss: 92.26888406515121\n",
      "cnt: 152500\n",
      "loss: 101.06454595565796\n",
      "cnt: 152600\n",
      "loss: 104.45699115991593\n",
      "cnt: 152700\n",
      "loss: 94.98661782264709\n",
      "cnt: 152800\n",
      "loss: 101.94504374742507\n",
      "cnt: 152900\n",
      "loss: 102.74370024681092\n",
      "cnt: 153000\n",
      "loss: 106.29159297704696\n",
      "cnt: 153100\n",
      "loss: 95.21647569656372\n",
      "cnt: 153200\n",
      "loss: 89.90218289375305\n",
      "cnt: 153300\n",
      "loss: 95.97797064781189\n",
      "cnt: 153400\n",
      "loss: 100.21926614761352\n",
      "cnt: 153500\n",
      "loss: 97.20743442058563\n",
      "cnt: 153600\n",
      "loss: 103.20721882820129\n",
      "cnt: 153700\n",
      "loss: 101.90235807418823\n",
      "cnt: 153800\n",
      "loss: 89.01929658412934\n",
      "cnt: 153900\n",
      "loss: 101.86780185699463\n",
      "cnt: 154000\n",
      "loss: 98.30640162467957\n",
      "cnt: 154100\n",
      "loss: 106.36727278709412\n",
      "cnt: 154200\n",
      "loss: 92.41511907577515\n",
      "cnt: 154300\n",
      "loss: 104.71335968017578\n",
      "cnt: 154400\n",
      "loss: 106.60201093673706\n",
      "cnt: 154500\n",
      "loss: 99.36919918298722\n",
      "cnt: 154600\n",
      "loss: 104.64153517961502\n",
      "cnt: 154700\n",
      "loss: 100.29349102020264\n",
      "cnt: 154800\n",
      "loss: 105.91899220466614\n",
      "cnt: 154900\n",
      "loss: 100.99078093528748\n",
      "cnt: 155000\n",
      "loss: 106.51707174777985\n",
      "cnt: 155100\n",
      "loss: 97.65813950061798\n",
      "cnt: 155200\n",
      "loss: 93.66881043672562\n",
      "cnt: 155300\n",
      "loss: 95.21579637289047\n",
      "cnt: 155400\n",
      "loss: 92.75181246757508\n",
      "cnt: 155500\n",
      "loss: 97.83157141685486\n",
      "cnt: 155600\n",
      "loss: 102.75557043075561\n",
      "cnt: 155700\n",
      "loss: 106.60758056640626\n",
      "cnt: 155800\n",
      "loss: 94.54577477931976\n",
      "cnt: 155900\n",
      "loss: 95.52517524242401\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13824\\2230433162.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/gpu:0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m   \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13824\\2420077682.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     53\u001b[0m       \u001b[1;31m# loss = self.train_step()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m       \u001b[1;31m# cnt += self.dataset_extractor.step_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m       \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m       \u001b[0mcnt\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_extractor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mcnt\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13824\\2420077682.py\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     29\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransducer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mspectrograms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         loss = rnnt_loss(\n\u001b[0m\u001b[0;32m     32\u001b[0m           \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m           \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13824\\2529736850.py\u001b[0m in \u001b[0;36mrnnt_loss\u001b[1;34m(logits, labels, time_lengths, label_lengths)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mrnnt_loss\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m   \u001b[0mlog_pr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m   \u001b[0mpr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpr_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_pr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m   \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\custom_gradient.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *a, **k)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\custom_gradient.py\u001b[0m in \u001b[0;36mdecorated\u001b[1;34m(wrapped, args, kwargs)\u001b[0m\n\u001b[0;32m    292\u001b[0m     \u001b[1;34m\"\"\"Decorated function with custom gradient.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0m_eager_mode_decorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_graph_mode_decorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\custom_gradient.py\u001b[0m in \u001b[0;36m_eager_mode_decorator\u001b[1;34m(f, args, kwargs)\u001b[0m\n\u001b[0;32m    529\u001b[0m   \u001b[1;34m\"\"\"Implement custom gradient decorator for eager mode.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0mtape_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariableWatcher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mvariable_watcher\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m     \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m   \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m   \u001b[0mall_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13824\\2529736850.py\u001b[0m in \u001b[0;36mpr_loss\u001b[1;34m(log_pr, labels, time_lengths, label_lengths)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m   \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_alpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruth_log_pr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblank_log_pr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m   \u001b[0mbeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_beta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruth_log_pr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblank_log_pr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m   \u001b[0mtotal_log_pr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13824\\2529736850.py\u001b[0m in \u001b[0;36mget_beta\u001b[1;34m(truth_log_pr, blank_log_pr)\u001b[0m\n\u001b[0;32m    145\u001b[0m     beta_diag = tf.concat([\n\u001b[0;32m    146\u001b[0m       \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial_diag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m       \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtruth_diag\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblank_diag\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_diag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m     ], axis=0)\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1176\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1177\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1178\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    627\u001b[0m                   \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'in a future version'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m                   if date is None else ('after %s' % date), instructions)\n\u001b[1;32m--> 629\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    630\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m     doc = _add_deprecated_arg_value_notice_to_docstring(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\functional_ops.py\u001b[0m in \u001b[0;36mscan_v2\u001b[1;34m(fn, elems, initializer, parallel_iterations, back_prop, swap_memory, infer_shape, reverse, name)\u001b[0m\n\u001b[0;32m    803\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    804\u001b[0m   \"\"\"\n\u001b[1;32m--> 805\u001b[1;33m   return scan(\n\u001b[0m\u001b[0;32m    806\u001b[0m       \u001b[0mfn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    807\u001b[0m       \u001b[0melems\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0melems\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1176\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1177\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1178\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\functional_ops.py\u001b[0m in \u001b[0;36mscan\u001b[1;34m(fn, elems, initializer, parallel_iterations, back_prop, swap_memory, infer_shape, reverse, name)\u001b[0m\n\u001b[0;32m    661\u001b[0m       \u001b[0minitial_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    662\u001b[0m       \u001b[0mcondition\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_2\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 663\u001b[1;33m     _, _, r_a = control_flow_ops.while_loop(\n\u001b[0m\u001b[0;32m    664\u001b[0m         \u001b[0mcondition\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m         \u001b[0mcompute\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minitial_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_flat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccs_ta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[1;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[0;32m   2759\u001b[0m       loop_var_structure = nest.map_structure(type_spec.type_spec_from_value,\n\u001b[0;32m   2760\u001b[0m                                               list(loop_vars))\n\u001b[1;32m-> 2761\u001b[1;33m       \u001b[1;32mwhile\u001b[0m \u001b[0mcond\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2762\u001b[0m         \u001b[0mloop_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2763\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtry_to_pack\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_basetuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(i, lv)\u001b[0m\n\u001b[0;32m   2750\u001b[0m         \u001b[0mloop_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloop_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2751\u001b[0m         cond = lambda i, lv: (  # pylint: disable=g-long-lambda\n\u001b[1;32m-> 2752\u001b[1;33m             math_ops.logical_and(i < maximum_iterations, orig_cond(*lv)))\n\u001b[0m\u001b[0;32m   2753\u001b[0m         \u001b[0mbody\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlv\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morig_body\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2754\u001b[0m       \u001b[0mtry_to_pack\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mlogical_and\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   5691\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5692\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5693\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   5694\u001b[0m         _ctx, \"LogicalAnd\", name, x, y)\n\u001b[0;32m   5695\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# encoder = Encoder(UNITS, CODER_OUTPUT_DIM)\n",
    "# decoder = Decoder(dataset.phoneme.VOCAB_SIZE, EMBEDDING_DIM, UNITS, CODER_OUTPUT_DIM)\n",
    "# joint_net = JointNet(1024, dataset.phoneme.VOCAB_SIZE)\n",
    "# transducer = Transducer(encoder, decoder, joint_net)\n",
    "\n",
    "transducer = Transducer(EMBEDDING_DIM, UNITS, CODER_OUTPUT_DIM, JOINT_NET_INNER_DIM, dataset.phoneme.VOCAB_SIZE)\n",
    "\n",
    "transducer([\n",
    "  tf.keras.Input(shape=(None, 128), dtype=tf.float32),\n",
    "  tf.keras.Input(shape=(None, ), dtype=tf.int32)\n",
    "])\n",
    "\n",
    "transducer.summary()\n",
    "\n",
    "# transducer = Transducer(EMBEDDING_DIM, UNITS, CODER_OUTPUT_DIM, JOINT_NET_INNER_DIM, VOCAB_SIZE)\n",
    "# transducer.load_weights('./checkpoints/no5000/ckpt')\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "trainer = Trainer(transducer, optimizer, BATCH_SIZE, extractor, SAVE_TIME_DELTA)\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "  trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram, labels = extractor.get_value()[0]\n",
    "\n",
    "logits = transducer((tf.convert_to_tensor([spectrogram]), tf.convert_to_tensor([labels])))\n",
    "print(tf.argmax(logits, axis=-1)[:, :, : ][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDecoder:\n",
    "  def __init__ (self, transducer):\n",
    "    self.transducer = transducer\n",
    "  \n",
    "  def decode (self, spectrogram):\n",
    "    label = [0]\n",
    "    for i in range(spectrogram.shape[0]):\n",
    "      sub_spectrogram = spectrogram[0: i + 1]\n",
    "      sub_spectrogram = tf.convert_to_tensor(sub_spectrogram)\n",
    "      sub_spectrogram = tf.expand_dims(sub_spectrogram, axis=0)\n",
    "      sub_label = tf.constant([label], dtype=tf.int32)\n",
    "      sub_rst = self.transducer((sub_spectrogram, sub_label))\n",
    "      sub_rst = tf.argmax(sub_rst[0][-1][-1], axis=-1).numpy()\n",
    "      label.append(sub_rst)\n",
    "    return label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = AudioDecoder(transducer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(filter(lambda x: x != 0, decoder.decode(spectrogram)))\n",
    "print(moasseugi(vector_to_sentence([0] + a)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_to_sentence (x):\n",
    "  return ''.join([dataset.phoneme.phonemes[idx] for idx in x][1: ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(moasseugi(vector_to_sentence(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rst = []\n",
    "x, y = 0, 0\n",
    "while x < logits.shape[2] and y < logits.shape[1]:\n",
    "  sel = tf.argmax(logits[0][y][x], axis=-1).numpy()\n",
    "  rst.append(sel)\n",
    "  if sel == 0:\n",
    "    y += 1\n",
    "  else:\n",
    "    x += 1\n",
    "\n",
    "print(moasseugi(vector_to_sentence([0] + list(filter(lambda x: x != 0, rst)))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec62f2ec7c7d49224ec335e3ef472b8ef537dc7949933ed74784e8710c2a92fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
